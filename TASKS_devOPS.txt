Task 1 - Task 36 : Kodekloud Cheatsheat SysAdmin Task Commands.txt
--------------------------------------------------------------------------------------------------------------------------------------------
Task 37 : 14/Feb/2022
Create Replicaset in Kubernetes Cluster

The Nautilus DevOps team is going to deploy some applications on kubernetes cluster as they are planning to migrate some of their existing applications there. Recently one of the team members has been assigned a task to write a template as per details mentioned below:

    Create a ReplicaSet using httpd image with latest tag only and remember to mention tag i.e httpd:latest and name it as httpd-replicaset.

    Labels app should be httpd_app, labels type should be front-end. The container should be named as httpd-container; also make sure replicas counts are 4.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

		1.
		thor@jump_host ~$ kubectl get deploy
			No resources found in default namespace.
		 
		2.
		thor@jump_host ~$ kubectl get services
			NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
			kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   57m
		
		3. 
		thor@jump_host ~$ vi /tmp/httpd.yaml
			apiVersion: apps/v1
			kind: ReplicaSet
			metadata:
			  name: httpd-replicaset
			  labels:
			    app: httpd_app
			    type: front-end
			spec:
			  replicas: 4
			  selector:
			    matchLabels:
			      app: httpd_app
			  template:
			    metadata:
			      labels:
			        app: httpd_app
			        type: front-end
			    spec:
			      containers:
			        - name: httpd-container
			          image: httpd:latest

		4.	          
		thor@jump_host ~$ cat /tmp/httpd.yaml
			apiVersion: apps/v1
				kind: ReplicaSet
				metadata:
				  name: httpd-replicaset
				  labels:
				    app: httpd_app
				    type: front-end
				spec:
				  replicas: 4
				  selector:
				    matchLabels:
				      app: httpd_app
				  template:
				    metadata:
				      labels:
				        app: httpd_app
				        type: front-end
				    spec:
				      containers:
				        - name: httpd-container
				          image: httpd:latest

		5.
		thor@jump_host ~$ kubectl create -f /tmp/httpd.yaml 
			replicaset.apps/httpd-replicaset created
		 
		6.
		thor@jump_host ~$ kubectl get pods
			NAME                     READY   STATUS              RESTARTS   AGE
			httpd-replicaset-9ddvb   0/1     ContainerCreating   0          13s
			httpd-replicaset-ndtzd   0/1     ContainerCreating   0          13s
			httpd-replicaset-p8vdv   0/1     ContainerCreating   0          13s
			httpd-replicaset-w4nmd   0/1     ContainerCreating   0          13s

--------------------------------------------------------------------------------------------------------------------------------------------
Task 38: 15/Feb/2022
Git Clone Repositories

DevOps team created a new Git repository last week; however, as of now no team is using it. The Nautilus application development team recently asked for a copy of that repo on Storage server in Stratos DC. Please clone the repo as per details shared below:

    The repo that needs to be cloned is /opt/news.git

    Clone this git repository under /usr/src/kodekloudrepos directory. Please do not try to make any changes in repo.

		1. Login to Storage server an switch to root
		ssh natasha@ststor01

		sudo su -

		2. Go to location where you want to clone the repository / destination and verify its empty
		# cd /usr/src/kodekloudrepos/
		# ll
			total 0

		3. Clone the repository using given location/source and verify the contents
		# git clone /opt/news.git/
			Cloning into 'news'...
			warning: You appear to have cloned an empty repository.
			done.

		# ll
			total 4
			drwxr-xr-x 3 root root 4096 Feb 15 17:32 news

		# cd news

		# ll
			total 0

--------------------------------------------------------------------------------------------------------------------------------------------
Task 39 : 17/Feb/2022
Setup Puppet Certs

The Nautilus DevOps team has set up a puppet master and an agent node in Stratos Datacenter. Puppet master is running on jump host itself (also note that Puppet master node is also running as Puppet CA server) and Puppet agent is running on App Server 2. Since it is a fresh set up, the team wants to sign certificates for puppet master as well as puppet agent nodes so that they can proceed with the next steps of set up. You can find more details about the task below:

Puppet server and agent nodes are already have required packages, but you may need to start puppetserver (on master) and puppet service on both nodes.

    Assign and sign certificates for both master and agent node.


	On Jump Host/PuppetMaster server :

		1. Switch to root user 
		sudo su -

		2. Make changes in /etc/hosts to include alias for puppet master
		# vi /etc/hosts
			172.16.238.3    jump_host.stratos.xfusioncorp.com jump_host puppet
			172.16.239.5    jump_host.stratos.xfusioncorp.com jump_host puppet
			172.17.0.4      jump_host.stratos.xfusioncorp.com jump_host puppet

		# cat /etc/hosts
			127.0.0.1       localhost
			::1     localhost ip6-localhost ip6-loopback
			fe00::0 ip6-localnet
			ff00::0 ip6-mcastprefix
			ff02::1 ip6-allnodes
			ff02::2 ip6-allrouters
			172.16.238.10   stapp01.stratos.xfusioncorp.com
			172.16.238.11   stapp02.stratos.xfusioncorp.com
			172.16.238.12   stapp03.stratos.xfusioncorp.com
			172.16.238.3    jump_host.stratos.xfusioncorp.com jump_host puppet
			172.16.239.5    jump_host.stratos.xfusioncorp.com jump_host puppet
			172.17.0.4      jump_host.stratos.xfusioncorp.com jump_host puppet

		3.Check puppetserver status
		# systemctl status puppetserver
		● puppetserver.service - puppetserver Service
		   Loaded: loaded (/usr/lib/systemd/system/puppetserver.service; disabled; vendor preset: disabled)
		   Active: active (running) since Thu 2022-02-17 04:34:20 UTC; 1min 11s ago
		  Process: 13195 ExecStart=/opt/puppetlabs/server/apps/puppetserver/bin/puppetserver start (code=exited, status=0/SUCCESS)
		 Main PID: 13258 (java)
		    Tasks: 90 (limit: 4915)
		   CGroup: /docker/9463a0d98965db0a54bc2af297eb82a9ef1abcbe227e26805fd5cf3cd02bdb53/system.slice/puppetserver.service
		           └─13258 /usr/bin/java -Xms512m -Xmx512m -Djruby.logger.class=com.puppetlabs.jruby_utils.jruby.Slf4jLogger -XX:OnOutOfMemoryErro...

		4. Check certificates on puppetserver
		# puppetserver ca  list --all
		Signed Certificates:
		    964369dd618d.c.argo-prod-us-east1.internal       (SHA256)  8C:19:47:FD:59:97:CE:0C:1D:DF:52:92:A3:BA:41:22:F2:3D:95:D4:38:D9:EF:85:65:9A:7B:B5:59:D5:CA:E6       alt names: ["DNS:puppet", "DNS:964369dd618d.c.argo-prod-us-east1.internal"]     authorization extensions: [pp_cli_auth: true]
		    jump_host.stratos.xfusioncorp.com                (SHA256)  F1:1F:B0:FE:F6:4A:20:52:C4:C7:D6:9E:1B:FC:2E:32:21:8C:53:5E:E3:61:9F:A1:47:B9:A9:78:2F:E5:3D:66       alt names: ["DNS:puppet", "DNS:jump_host.stratos.xfusioncorp.com"]      authorization extensions: [pp_cli_auth: true]

	On App server/PuppeetClient :

		5. Login to App server and switch to root
		ssh steve@stapp02

		sudo su -

		6. Make the PuppetServer Master entries in /etc/hosts file
		# vi /etc/hosts
		 	172.16.238.3    jump_host.stratos.xfusioncorp.com jump_host puppet
			172.16.239.5    jump_host.stratos.xfusioncorp.com jump_host puppet
			172.17.0.4      jump_host.stratos.xfusioncorp.com jump_host puppet

		7. Verify hosts entries via ping 
		# ping puppet
		PING jump_host.stratos.xfusioncorp.com (172.16.238.3) 56(84) bytes of data.
		64 bytes from jump_host.stratos.xfusioncorp.com (172.16.238.3): icmp_seq=1 ttl=64 time=0.068 ms
		64 bytes from jump_host.stratos.xfusioncorp.com (172.16.238.3): icmp_seq=2 ttl=64 time=0.076 ms
		64 bytes from jump_host.stratos.xfusioncorp.com (172.16.238.3): icmp_seq=3 ttl=64 time=0.071 ms
		64 bytes from jump_host.stratos.xfusioncorp.com (172.16.238.3): icmp_seq=4 ttl=64 time=0.072 ms
		64 bytes from jump_host.stratos.xfusioncorp.com (172.16.238.3): icmp_seq=5 ttl=64 time=0.090 ms
		^C
		--- jump_host.stratos.xfusioncorp.com ping statistics ---
		5 packets transmitted, 5 received, 0% packet loss, time 4081ms
		rtt min/avg/max/mdev = 0.068/0.075/0.090/0.011 ms

		8. Check puppet client status and check if errors
		# systemctl status puppet
		● puppet.service - Puppet agent
		   Loaded: loaded (/usr/lib/systemd/system/puppet.service; disabled; vendor preset: disabled)
		   Active: active (running) since Thu 2022-02-17 04:31:58 UTC; 6min ago
		 Main PID: 487 (puppet)
		   CGroup: /docker/a819efbba5ee93fe68e7e03589eeef739764d9d47a85c8db98909ab600c248bc/system.slice/puppet.service
		           └─487 /opt/puppetlabs/puppet/bin/ruby /opt/puppetlabs/puppet/bin/puppet agent --no-daemonize

		Feb 17 04:31:58 stapp02.stratos.xfusioncorp.com systemd[1]: About to execute: /opt/puppetlabs/puppet/bin/puppet agent $PUPPET_EXTRA_...monize
		Feb 17 04:31:58 stapp02.stratos.xfusioncorp.com systemd[1]: Forked /opt/puppetlabs/puppet/bin/puppet as 487
		Feb 17 04:31:58 stapp02.stratos.xfusioncorp.com systemd[1]: puppet.service changed dead -> running
		Feb 17 04:31:58 stapp02.stratos.xfusioncorp.com systemd[1]: Job puppet.service/start finished, result=done
		Feb 17 04:31:58 stapp02.stratos.xfusioncorp.com systemd[1]: Started Puppet agent.
		Feb 17 04:31:58 stapp02.stratos.xfusioncorp.com systemd[487]: Executing: /opt/puppetlabs/puppet/bin/puppet agent --no-daemonize
		Feb 17 04:34:20 stapp02.stratos.xfusioncorp.com puppet-agent[487]: Request to https://puppet:8140/puppet-ca/v1 timed out connect oper...conds
		Feb 17 04:34:20 stapp02.stratos.xfusioncorp.com puppet-agent[487]: Wrapped exception:
		Feb 17 04:34:20 stapp02.stratos.xfusioncorp.com puppet-agent[487]: execution expired
		Feb 17 04:34:20 stapp02.stratos.xfusioncorp.com puppet-agent[487]: No more routes to ca
		Hint: Some lines were ellipsized, use -l to show in full.

		9. Restart puppet client and check status
		# systemctl restart puppet
			
		# systemctl status puppet
		● puppet.service - Puppet agent
		   Loaded: loaded (/usr/lib/systemd/system/puppet.service; disabled; vendor preset: disabled)
		   Active: active (running) since Thu 2022-02-17 04:38:12 UTC; 3s ago
		 Main PID: 596 (puppet)
		   CGroup: /docker/a819efbba5ee93fe68e7e03589eeef739764d9d47a85c8db98909ab600c248bc/system.slice/puppet.service
		           └─596 /opt/puppetlabs/puppet/bin/ruby /opt/puppetlabs/puppet/bin/puppet agent --no-daemonize

		Feb 17 04:38:12 stapp02.stratos.xfusioncorp.com systemd[1]: About to execute: /opt/puppetlabs/puppet/bin/puppet agent $PUPPET_EXTRA_...monize
		Feb 17 04:38:12 stapp02.stratos.xfusioncorp.com systemd[1]: Forked /opt/puppetlabs/puppet/bin/puppet as 596
		Feb 17 04:38:12 stapp02.stratos.xfusioncorp.com systemd[1]: puppet.service changed dead -> running
		Feb 17 04:38:12 stapp02.stratos.xfusioncorp.com systemd[1]: Job puppet.service/start finished, result=done
		Feb 17 04:38:12 stapp02.stratos.xfusioncorp.com systemd[1]: Started Puppet agent.
		Feb 17 04:38:12 stapp02.stratos.xfusioncorp.com systemd[596]: Executing: /opt/puppetlabs/puppet/bin/puppet agent --no-daemonize
		Hint: Some lines were ellipsized, use -l to show in full.

	On Jump Host/PuppetMaster Server :

		10. Check certificates on puppetserver and look for stapp02 certificate request 
		# puppetserver ca  list --all
		Requested Certificates:
		    stapp02.stratos.xfusioncorp.com       (SHA256)  8F:E9:B3:29:D6:8C:83:7D:47:3F:03:0C:F0:F2:06:18:E1:60:00:47:5F:C3:D0:00:68:4B:B9:6A:BC:99:5F:DB
		Signed Certificates:
		    964369dd618d.c.argo-prod-us-east1.internal       (SHA256)  8C:19:47:FD:59:97:CE:0C:1D:DF:52:92:A3:BA:41:22:F2:3D:95:D4:38:D9:EF:85:65:9A:7B:B5:59:D5:CA:E6       alt names: ["DNS:puppet", "DNS:964369dd618d.c.argo-prod-us-east1.internal"]     authorization extensions: [pp_cli_auth: true]
		    jump_host.stratos.xfusioncorp.com                (SHA256)  F1:1F:B0:FE:F6:4A:20:52:C4:C7:D6:9E:1B:FC:2E:32:21:8C:53:5E:E3:61:9F:A1:47:B9:A9:78:2F:E5:3D:66       alt names: ["DNS:puppet", "DNS:jump_host.stratos.xfusioncorp.com"]      authorization extensions: [pp_cli_auth: true]

		11. Sign all certificates
		# puppetserver ca sign --all
		Successfully signed certificate request for stapp02.stratos.xfusioncorp.com	

	On App server/PuppetClient :

		12. Validate puppet agent sign on
		# puppet agent -t
		Info: csr_attributes file loading from /etc/puppetlabs/puppet/csr_attributes.yaml
		Info: Creating a new SSL certificate request for stapp02.stratos.xfusioncorp.com
		Info: Certificate Request fingerprint (SHA256): 8F:E9:B3:29:D6:8C:83:7D:47:3F:03:0C:F0:F2:06:18:E1:60:00:47:5F:C3:D0:00:68:4B:B9:6A:BC:99:5F:DB
		Info: Downloaded certificate for stapp02.stratos.xfusioncorp.com from https://puppet:8140/puppet-ca/v1
		Info: Using configured environment 'production'
		Info: Retrieving pluginfacts
		Info: Retrieving plugin
		Info: Retrieving locales
		Info: Caching catalog for stapp02.stratos.xfusioncorp.com
		Info: Applying configuration version '1645072754'
		Info: Creating state file /opt/puppetlabs/puppet/cache/state/state.yaml
		Notice: Applied catalog in 0.01 seconds

--------------------------------------------------------------------------------------------------------------------------------------------
Task 40 : 24/Feb/2022
Ansible Setup Httpd and PHP
Nautilus Application development team wants to test the Apache and PHP setup on one of the app servers in Stratos Datacenter. They want the DevOps team to prepare an Ansible playbook to accomplish this task. Below you can find more details about the task.

There is an inventory file ~/playbooks/inventory on jump host.

Create a playbook ~/playbooks/httpd.yml on jump host and perform the following tasks on App Server 1.

a. Install httpd and php packages (whatever default version is available in yum repo).

b. Change default document root of Apache to /var/www/html/myroot in default Apache config /etc/httpd/conf/httpd.conf. Make sure /var/www/html/myroot path exists (if not please create the same).

c. There is a template ~/playbooks/templates/phpinfo.php.j2 on jump host. Copy this template to the Apache document root you created as phpinfo.php file and make sure user owner and the group owner for this file is apache user.

d. Start and enable httpd service.

Note: Validation will try to run the playbook using command ansible-playbook -i inventory httpd.yml, so please make sure the playbook works this way without passing any extra arguments.


thor@jump_host ~$ cd playbooks/
thor@jump_host ~/playbooks$ ll -a
total 20
drwxr-xr-x 3 thor thor 4096 Feb 24 05:10 .
drwxr----- 1 thor thor 4096 Feb 24 05:10 ..
-rw-r--r-- 1 thor thor   36 Feb 24 05:10 ansible.cfg
-rw-r--r-- 1 thor thor  237 Feb 24 05:10 inventory
drwxr-xr-x 2 thor thor 4096 Feb 24 05:10 templates
thor@jump_host ~/playbooks$ cd inventory 
bash: cd: inventory: Not a directory
thor@jump_host ~/playbooks$ ll -a
total 20
drwxr-xr-x 3 thor thor 4096 Feb 24 05:10 .
drwxr----- 1 thor thor 4096 Feb 24 05:10 ..
-rw-r--r-- 1 thor thor   36 Feb 24 05:10 ansible.cfg
-rw-r--r-- 1 thor thor  237 Feb 24 05:10 inventory
drwxr-xr-x 2 thor thor 4096 Feb 24 05:10 templates
thor@jump_host ~/playbooks$ vi httpd.yml
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ cat httpd.yml 
- name: Setup Httpd and PHP

  hosts: stapp01

  become: yes

  tasks:

    - name: Install latest version of httpd and php

      package:

        name:

          - httpd

          - php

        state: latest

    - name: Replace default DocumentRoot in httpd.conf

      replace:

        path: /etc/httpd/conf/httpd.conf

        regexp: DocumentRoot \"\/var\/www\/html\"

        replace: DocumentRoot "/var/www/html/myroot"

    - name: Create the new DocumentRoot directory if it does not exist

      file:

        path: /var/www/html/myroot

        state: directory

        owner: apache

        group: apache

    - name: Use Jinja2 template to generate phpinfo.php

      template:

        src: /home/thor/playbooks/templates/phpinfo.php.j2

        dest: /var/www/html/myroot/phpinfo.php

        owner: apache

        group: apache

    - name: Start and enable service httpd

      service:

        name: httpd

        state: started

        enabled: yes 
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ cd templates/
thor@jump_host ~/playbooks/templates$  ll-a
bash: ll-a: command not found
thor@jump_host ~/playbooks/templates$  ll -a
total 12
drwxr-xr-x 2 thor thor 4096 Feb 24 05:10 .
drwxr-xr-x 3 thor thor 4096 Feb 24 05:13 ..
-rw-r--r-- 1 thor thor   19 Feb 24 05:10 phpinfo.php.j2
thor@jump_host ~/playbooks/templates$ cd ../
thor@jump_host ~/playbooks$ ansible-playbook -i inventory httpd.yml 

PLAY [Setup Httpd and PHP] ******************************************************************************************************************

TASK [Gathering Facts] **********************************************************************************************************************
ok: [stapp01]

TASK [Install latest version of httpd and php] **********************************************************************************************
changed: [stapp01]

TASK [Replace default DocumentRoot in httpd.conf] *******************************************************************************************
changed: [stapp01]

TASK [Create the new DocumentRoot directory if it does not exist] ***************************************************************************
changed: [stapp01]

TASK [Use Jinja2 template to generate phpinfo.php] ******************************************************************************************
changed: [stapp01]

TASK [Start and enable service httpd] *******************************************************************************************************
changed: [stapp01]

PLAY RECAP **********************************************************************************************************************************
stapp01                    : ok=6    changed=5    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

thor@jump_host ~/playbooks$ 

Last login: Thu Feb 24 05:10:23 UTC 2022 on pts/0
thor@jump_host ~$ ssh tony@stapp01
The authenticity of host 'stapp01 (172.16.238.10)' can't be established.
ECDSA key fingerprint is SHA256:HRK4JT6fwc3O4QN4Kd4nKBfOFgvwjBs/XBj+WP8m56Y.
ECDSA key fingerprint is MD5:46:21:7e:34:9a:50:9c:45:12:9f:ba:d6:de:ac:ed:db.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'stapp01' (ECDSA) to the list of known hosts.
tony@stapp01's password: 
Last login: Thu Feb 24 05:15:26 2022 from jump_host.devops-ansible-httpd-php-v2_app_net
[tony@stapp01 ~]$ 
[tony@stapp01 ~]$ 
[tony@stapp01 ~]$ sudo su -
[root@stapp01 ~]# Ir0nM@n
-bash: Ir0nM@n: command not found
[root@stapp01 ~]# 
[root@stapp01 ~]# 
[root@stapp01 ~]# 
[root@stapp01 ~]# 
[root@stapp01 ~]# 
[root@stapp01 ~]# 
[root@stapp01 ~]# rpm -qa |grep httpd
httpd-tools-2.4.6-97.el7.centos.4.x86_64
httpd-2.4.6-97.el7.centos.4.x86_64
[root@stapp01 ~]# rpm -qa |grep php
php-cli-5.4.16-48.el7.x86_64
php-common-5.4.16-48.el7.x86_64
php-5.4.16-48.el7.x86_64
[root@stapp01 ~]# 
[root@stapp01 ~]# 
[root@stapp01 ~]# cd /var/www/html/
[root@stapp01 html]# ls -ahl
total 12K
drwxr-xr-x 3 root   root   4.0K Feb 24 05:15 .
drwxr-xr-x 4 root   root   4.0K Feb 24 05:15 ..
drwxr-xr-x 2 apache apache 4.0K Feb 24 05:15 myroot
[root@stapp01 html]# 

--------------------------------------------------------------------------------------------------------------------------------------------

Task 41 : 25/Feb/2022
Ansible File Module
The Nautilus DevOps team is working to test several Ansible modules on servers in Stratos DC. Recently they wanted to test the file creation on remote hosts using Ansible. Find below more details about the task:

a. Create an inventory file ~/playbook/inventory on jump host and add all app servers in it.

b. Create a playbook ~/playbook/playbook.yml to create a blank file /opt/nfsshare.txt on all app servers.

c. The /opt/nfsshare.txt file permission must be 0755.

d. The user/group owner of file /opt/nfsshare.txt must be tony on app server 1, steve on app server 2 and banner on app server 3.

Note: Validation will try to run the playbook using command ansible-playbook -i inventory playbook.yml, so please make sure the playbook works this way without passing any extra arguments.


		1. Got to playbook directory 
		cd playbook/

		pwd
			/home/thor/playbook

		2. Create the inventory file as per given requirements and verify
		ll -a
			total 8
			drwxr-xr-x 2 thor thor 4096 Feb 25 09:14 .
			drwxr----- 1 thor thor 4096 Feb 25 09:14 ..

		vi inventory
			stapp01 ansible_host=172.16.238.10 ansible_ssh_pass=Ir0nM@n  ansible_user=tony
			stapp02 ansible_host=172.16.238.11 ansible_ssh_pass=Am3ric@  ansible_user=steve
			stapp03 ansible_host=172.16.238.12 ansible_ssh_pass=BigGr33n  ansible_user=banner

		cat inventory 
			stapp01 ansible_host=172.16.238.10 ansible_ssh_pass=Ir0nM@n  ansible_user=tony
			stapp02 ansible_host=172.16.238.11 ansible_ssh_pass=Am3ric@  ansible_user=steve
			stapp03 ansible_host=172.16.238.12 ansible_ssh_pass=BigGr33n  ansible_user=banner

		3. Verify if ansible is able to connect as per inventory file
		ansible all -a "ls -ahl /opt/nfsshare.txt" -i inventory
			stapp03 | FAILED | rc=2 >>
			ls: cannot access /opt/nfsshare.txt: No such file or directorynon-zero return code
			stapp02 | FAILED | rc=2 >>
			ls: cannot access /opt/nfsshare.txt: No such file or directorynon-zero return code
			stapp01 | FAILED | rc=2 >>
			ls: cannot access /opt/nfsshare.txt: No such file or directorynon-zero return code

		4. Create playbook.yml file as per requirements and verify
		vi playbook.yml
			- name: Create file in appservers
		  	  hosts: stapp01, stapp02, stapp03
		  	  become: yes
		      tasks:
			    - name: Create the file and set properties
			      file:
			        path: /opt/nfsshare.txt
			        owner: "{{ ansible_user }}"
			        group: "{{ ansible_user }}"
			        mode: "0755"
			        state: touch

		cat playbook.yml 
			- name: Create file in appservers

			  hosts: stapp01, stapp02, stapp03

			  become: yes

			  tasks:

			    - name: Create the file and set properties

			      file:

			        path: /opt/nfsshare.txt

			        owner: "{{ ansible_user }}"

			        group: "{{ ansible_user }}"

			        mode: "0755"

			        state: touch

		5. Run the playbook
		ansible-playbook -i inventory playbook.yml 

			PLAY [Create file in appservers] ************************************************************************************************************

			TASK [Gathering Facts] **********************************************************************************************************************
			ok: [stapp03]
			ok: [stapp02]
			ok: [stapp01]

			TASK [Create the file and set properties] ***************************************************************************************************
			changed: [stapp01]
			changed: [stapp03]
			changed: [stapp02]

			PLAY RECAP **********************************************************************************************************************************
			stapp01                    : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
			stapp02                    : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
			stapp03                    : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

		6. Verify the files created on App servers
		ansible all -a "ls -ahl /opt/nfsshare.txt" -i inventory
			stapp01 | CHANGED | rc=0 >>
			-rwxr-xr-x 1 tony tony 0 Feb 25 09:20 /opt/nfsshare.txt
			stapp03 | CHANGED | rc=0 >>
			-rwxr-xr-x 1 banner banner 0 Feb 25 09:20 /opt/nfsshare.txt
			stapp02 | CHANGED | rc=0 >>
			-rwxr-xr-x 1 steve steve 0 Feb 25 09:20 /opt/nfsshare.txt

--------------------------------------------------------------------------------------------------------------------------------------------

Task 42: 4/Mar/2022
Troubleshoot Deployment issues in Kubernetes

Last week, the Nautilus DevOps team deployed a redis app on Kubernetes cluster, which was working fine so far. This morning one of the team members was making some changes in this existing setup, but he made some mistakes and the app went down. We need to fix this as soon as possible. Please take a look.

The deployment name is redis-deployment. The pods are not in running state right now, so please look into the issue and fix the same.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

thor@jump_host ~$ kubectl get-deployment
Error: unknown command "get-deployment" for "kubectl"
Run 'kubectl --help' for usage.
thor@jump_host ~$ kubectl get deployment
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
redis-deployment   0/1     1            0           71s
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get pods
NAME                               READY   STATUS              RESTARTS   AGE
redis-deployment-8bdf985f7-65g4j   0/1     ContainerCreating   0          106s
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get configmap
NAME               DATA   AGE
kube-root-ca.crt   1      20m
redis-config       2      2m4s
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl describe deployment
Name:                   redis-deployment
Namespace:              default
CreationTimestamp:      Fri, 04 Mar 2022 07:59:44 +0000
Labels:                 app=redis
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=redis
Replicas:               1 desired | 1 updated | 1 total | 0 available | 1 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=redis
  Containers:
   redis-container:
    Image:      redis:alpin
    Port:       6379/TCP
    Host Port:  0/TCP
    Requests:
      cpu:        300m
    Environment:  <none>
    Mounts:
      /redis-master from config (rw)
      /redis-master-data from data (rw)
  Volumes:
   data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
   config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      redis-conig
    Optional:  false
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      False   MinimumReplicasUnavailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:  redis-deployment-8bdf985f7 (1/1 replicas created)
NewReplicaSet:   <none>
Events:
  Type    Reason             Age    From                   Message
  ----    ------             ----   ----                   -------
  Normal  ScalingReplicaSet  2m45s  deployment-controller  Scaled up replica set redis-deployment-8bdf985f7 to 1
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl describe pods
[OName:           redis-deployment-8bdf985f7-65g4j
Namespace:      default
Priority:       0
Node:           kodekloud-control-plane/172.17.0.2
Start Time:     Fri, 04 Mar 2022 07:59:44 +0000
Labels:         app=redis
                pod-template-hash=8bdf985f7
Annotations:    <none>
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/redis-deployment-8bdf985f7
Containers:
  redis-container:
    Container ID:   
    Image:          redis:alpin
    Image ID:       
    Port:           6379/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        300m
    Environment:  <none>
    Mounts:
      /redis-master from config (rw)
      /redis-master-data from data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-mktdc (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      redis-conig
    Optional:  false
  default-token-mktdc:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-mktdc
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason       Age                  From               Message
  ----     ------       ----                 ----               -------
  Normal   Scheduled    3m38s                default-scheduler  Successfully assigned default/redis-deployment-8bdf985f7-65g4j to kodekloud-control-plane
  Warning  FailedMount  95s                  kubelet            Unable to attach or mount volumes: unmounted volumes=[config], unattached volumes=[data config default-token-mktdc]: timed out waiting for the condition
  Warning  FailedMount  90s (x9 over 3m37s)  kubelet            MountVolume.SetUp failed for volume "config" : configmap "redis-conig" not found
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl describe configmap
Name:         kube-root-ca.crt
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
ca.crt:
----
-----BEGIN CERTIFICATE-----
MIIC5zCCAc+gAwIBAgIBADANBgkqhkiG9w0BAQsFADAVMRMwEQYDVQQDEwprdWJl
cm5ldGVzMB4XDTIyMDMwNDA3MzkxMVoXDTMyMDMwMTA3MzkxMVowFTETMBEGA1UE
AxMKa3ViZXJuZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBALme
RU36Jbasf+ky0j2DuqdVD+2Vw1trKQnA4N5n/gFzvKFUu4d6ZW0lWNyd73f/0W4U
y9joi+Pp2WZX5wEbwyTbJZ9RYopm8rgRRg1V1+GU4mC5aBUrat4pjQZX4OQNyBMd
ZBz/Cll/c/S6Y1vTDeqOm4SYz+2YSyVOtdLtBnUDGxH6m13gfqrGN/n/RhlDJO9T
722YgASeOFvPXpJMzStK/lZjUcsW4EYNoQgKewn+yB5k2LTvhs3ovj5Mc0gbWMBn
azJgCen+EJAV2yFxuFf41ppi1MvY7CeytGuOnv3uNi609TJrlp7NEuSoMCeIt1gT
IcaNpsFGFJ8CiOFWjgsCAwEAAaNCMEAwDgYDVR0PAQH/BAQDAgKkMA8GA1UdEwEB
/wQFMAMBAf8wHQYDVR0OBBYEFJ6FKF3kj6rXegz1wkfzx09gxROjMA0GCSqGSIb3
DQEBCwUAA4IBAQCuOE/yWOW7NAZAgYCavM8gd+px/onSM9o43wcl7YSvFM6/TVDq
ksbpbdG60GmgV5Fggui6p+zaaAYrtY33qRIYKwvQ0rpEx04llQRETrvXyXCtAe83
zYteCjlbY0rlpjWhpxxXS0RJLilMojtbi4Vr05zLEyYa5cQ1NN5YNgzGdFBHYY+D
bPvGVXFyVrzDdf8iUt9VcMOsrD03s7sUitnoIJSE+PkaYvdxcTuwENRccQTZoj6E
vfWAmEqPTVTYnkv6Z1lHejwXUoWd/DaVnmajUKB8YS2TgoUjwpEJbS0lRAikHeSs
zZiAqUXdZmZQXDWK6mlQELlg0u48IhGrHrF6
-----END CERTIFICATE-----

Events:  <none>


Name:         redis-config
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
maxmemory:
----
2mb
maxmemory-policy:
----
allkeys-lru
Events:  <none>
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl edit deployment
deployment.apps/redis-deployment edited
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get deploy
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
redis-deployment   1/1     1            1           9m18s
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get pods
NAME                                READY   STATUS    RESTARTS   AGE
redis-deployment-5bb6dd57fd-l4l56   1/1     Running   0          46s
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get deploy
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
redis-deployment   1/1     1            1           9m33s
thor@jump_host ~$ 


--------------------------------------------------------------------------------------------------------------------------------------------
Task 43 : 12/Mar/2022
Deploy Grafana on Kubernetes Cluster

The Nautilus DevOps teams is planning to set up a Grafana tool to collect and analyze analytics from some applications. They are planning to deploy it on Kubernetes cluster. Below you can find more details.

1.) Create a deployment named grafana-deployment-nautilus using any grafana image for Grafana app. Set other parameters as per your choice.

2.) Create NodePort type service with nodePort 32000 to expose the app.

You need not to make any configuration changes inside the Grafana app once deployed, just make sure you are able to access the Grafana login page.

Note: The kubeclt on jump_host has been configured to work with kubernetes cluster.


thor@jump_host ~$ kubectl get pods
No resources found in default namespace.
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get services
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3h16m
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ vi /tmp/grafana.yaml
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ cat /tmp/grafana.yaml 
apiVersion: v1

kind: Service

metadata:

  name: grafana-service-nautilus

spec:

  type: NodePort

  selector:

    app: grafana

  ports:

    - port: 3000

      targetPort: 3000

      nodePort: 32000

---

apiVersion: apps/v1

kind: Deployment

metadata:

  name: grafana-deployment-nautilus

spec:

  selector:

    matchLabels:

      app: grafana

  template:

    metadata:

      labels:

        app: grafana

    spec:

      containers:

        - name: grafana-container-nautilus

          image: grafana/grafana:latest

          ports:

            - containerPort: 3000
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl create -f /tmp/grafana.yaml 
service/grafana-service-nautilus created
deployment.apps/grafana-deployment-nautilus created
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get services
NAME                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
grafana-service-nautilus   NodePort    10.96.233.148   <none>        3000:32000/TCP   15s
kubernetes                 ClusterIP   10.96.0.1       <none>        443/TCP          3h18m
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get pods
NAME                                           READY   STATUS    RESTARTS   AGE
grafana-deployment-nautilus-66bb8cdd8b-xkrwz   1/1     Running   0          22s

--------------------------------------------------------------------------------------------------------------------------------------------
Task 44 : 16/Mar/2022

Create Cronjobs in Kubernetes
There are some jobs/tasks that need to be run regularly on different schedules. Currently the Nautilus DevOps team is working on developing some scripts that will be executed on different schedules, but for the time being the team is creating some cron jobs in Kubernetes cluster with some dummy commands (which will be replaced by original scripts later). Create a cronjob as per details given below:

    Create a cronjob named datacenter.

    Set schedule to */8 * * * *.

    Container name should be cron-datacenter.

    Use nginx image with latest tag only and remember to mention the tag i.e nginx:latest.

    Run a dummy command echo Welcome to xfusioncorp!.

    Ensure restart policy is OnFailure.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

thor@jump_host ~$ kubectl get pods
No resources found in default namespace.
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get cronjobs
No resources found in default namespace.
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get cronjob
No resources found in default namespace.
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ vi /tmp/cron.yml
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ cat /tmp/cron.yml 
apiVersion: batch/v1beta1

kind: CronJob

metadata:

  name: devops

spec:

  schedule: "*/5 * * * *"

  jobTemplate:

    spec:

      template:

        spec:

          containers:

            - name: cron-devops

              image: nginx:latest

              command:

                - /bin/sh

                - -c

                - echo Welcome to xfusioncorp!

          restartPolicy: OnFailure
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl create -f /tmp/cron.yml 
cronjob.batch/devops created
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get pods
No resources found in default namespace.
thor@jump_host ~$ kubectl get pod
No resources found in default namespace.
thor@jump_host ~$ kubectl get cronjob
NAME     SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
devops   */5 * * * *   False     0        <none>          35s
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get pod
No resources found in default namespace.
thor@jump_host ~$ kubectl get pod
No resources found in default namespace.
thor@jump_host ~$ kubectl get pod
No resources found in default namespace.
thor@jump_host ~$ kubectl get cronjob
NAME     SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
devops   */5 * * * *   False     0        <none>          61s
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get cronjob
NAME     SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
devops   */5 * * * *   False     0        <none>          67s
thor@jump_host ~$ kubectl get cronjob
NAME     SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
devops   */5 * * * *   False     0        <none>          70s
thor@jump_host ~$ kubectl get cronjob
NAME     SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
devops   */5 * * * *   False     1        15s             101s
thor@jump_host ~$ kubectl get pod
NAME                      READY   STATUS              RESTARTS   AGE
devops-1647415500-7jhh7   0/1     ContainerCreating   0          16s
thor@jump_host ~$ kubectl logs devops-1647415500-7jhh7

--------------------------------------------------------------------------------------------------------------------------------------------

Task 45 : 18/Mar/2022
Git Manage Remotes

The xFusionCorp development team added updates to the project that is maintained under /opt/beta.git repo and cloned under /usr/src/kodekloudrepos/beta. Recently some changes were made on Git server that is hosted on Storage server in Stratos DC. The DevOps team added some new Git remotes, so we need to update remote on /usr/src/kodekloudrepos/beta repository as per details mentioned below:

a. In /usr/src/kodekloudrepos/beta repo add a new remote dev_beta and point it to /opt/xfusioncorp_beta.git repository.

b. There is a file /tmp/index.html on same server; copy this file to the repo and add/commit to master branch.

c. Finally push master branch to this new remote origin.

thor@jump_host ~$ ssh natasha@ststor01
The authenticity of host 'ststor01 (172.16.238.15)' can't be established.
ECDSA key fingerprint is SHA256:ZYks87RZDpXlv4gMTVaz+GZFGeGKd3ziPSVrZQSr8vI.
ECDSA key fingerprint is MD5:bf:52:2a:cd:74:f5:9f:ad:8a:28:a4:d7:09:3e:e9:d1.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'ststor01,172.16.238.15' (ECDSA) to the list of known hosts.
natasha@ststor01's password: 
[natasha@ststor01 ~]$ 
[natasha@ststor01 ~]$ 
[natasha@ststor01 ~]$ 
[natasha@ststor01 ~]$ sudo su -

We trust you have received the usual lecture from the local System
Administrator. It usually boils down to these three things:

    #1) Respect the privacy of others.
    #2) Think before you type.
    #3) With great power comes great responsibility.

[sudo] password for natasha: 
[root@ststor01 ~]# 
[root@ststor01 ~]# 
[root@ststor01 ~]# 
[root@ststor01 ~]# cd /usr/src/kodekloudrepos/
[root@ststor01 kodekloudrepos]# ls -ahl
total 12K
drwxr-xr-x 3 root root 4.0K Mar 17 18:35 .
drwxr-xr-x 1 root root 4.0K Mar 17 18:35 ..
drwxr-xr-x 3 root root 4.0K Mar 17 18:35 beta
[root@ststor01 kodekloudrepos]# cd beta/
[root@ststor01 beta]# 
[root@ststor01 beta]# ls -ahl
total 16K
drwxr-xr-x 3 root root 4.0K Mar 17 18:35 .
drwxr-xr-x 3 root root 4.0K Mar 17 18:35 ..
drwxr-xr-x 8 root root 4.0K Mar 17 18:35 .git
-rw-r--r-- 1 root root   34 Mar 17 18:35 info.txt
[root@ststor01 beta]# git remote add dev_beta /opt/xfusioncorp_beta.git
[root@ststor01 beta]# 
[root@ststor01 beta]# 
[root@ststor01 beta]# 
[root@ststor01 beta]# cp /tmp/index.html ./
[root@ststor01 beta]# ls -ahl
total 20K
drwxr-xr-x 3 root root 4.0K Mar 17 18:39 .
drwxr-xr-x 3 root root 4.0K Mar 17 18:35 ..
drwxr-xr-x 8 root root 4.0K Mar 17 18:38 .git
-rw-r--r-- 1 root root  120 Mar 17 18:39 index.html
-rw-r--r-- 1 root root   34 Mar 17 18:35 info.txt
[root@ststor01 beta]# 
[root@ststor01 beta]# 
[root@ststor01 beta]# git init
Reinitialized existing Git repository in /usr/src/kodekloudrepos/beta/.git/
[root@ststor01 beta]# 
[root@ststor01 beta]# 
[root@ststor01 beta]# git add index.html 
[root@ststor01 beta]# 
[root@ststor01 beta]# 
[root@ststor01 beta]# git commit -m "Added index.html"
[master 36e84d4] Added index.html
 1 file changed, 10 insertions(+)
 create mode 100644 index.html
[root@ststor01 beta]# 
[root@ststor01 beta]# 
[root@ststor01 beta]# 
[root@ststor01 beta]# git push -u dev_beta master
Counting objects: 6, done.
Delta compression using up to 36 threads.
Compressing objects: 100% (4/4), done.
Writing objects: 100% (6/6), 584 bytes | 0 bytes/s, done.
Total 6 (delta 0), reused 0 (delta 0)
To /opt/xfusioncorp_beta.git
 * [new branch]      master -> master
Branch master set up to track remote branch master from dev_beta.
[root@ststor01 beta]# 
[root@ststor01 beta]# 
[root@ststor01 beta]# 

--------------------------------------------------------------------------------------------------------------------------------------------
Task 46: 19/Mar/2022
Docker EXEC Operations

One of the Nautilus DevOps team members was working to configure services on a kkloud container that is running on App Server 1 in Stratos Datacenter. Due to some personal work he is on PTO for the rest of the week, but we need to finish his pending work ASAP. Please complete the remaining work as per details given below:

a. Install apache2 in kkloud container using apt that is running on App Server 1 in Stratos Datacenter.

b. Configure Apache to listen on port 5001 instead of default http port. Do not bind it to listen on specific IP or hostname only, i.e it should listen on localhost, 127.0.0.1, container ip, etc.

c. Make sure Apache service is up and running inside the container. Keep the container in running state at the end.

thor@jump_host ~$ ssh tony@stapp01
The authenticity of host 'stapp01 (172.16.238.10)' can't be established.
ECDSA key fingerprint is SHA256:4yH914g0tXmYd6hoQ6Lf3EOXLvLr0mE6XM97FGBFex0.
ECDSA key fingerprint is MD5:98:c0:60:13:6b:23:d6:a3:d9:02:dc:28:f6:2b:14:d5.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'stapp01,172.16.238.10' (ECDSA) to the list of known hosts.
tony@stapp01's password: 
[tony@stapp01 ~]$ 
[tony@stapp01 ~]$ 
[tony@stapp01 ~]$ 
[tony@stapp01 ~]$ sudo su - 

We trust you have received the usual lecture from the local System
Administrator. It usually boils down to these three things:

    #1) Respect the privacy of others.
    #2) Think before you type.
    #3) With great power comes great responsibility.

[sudo] password for tony: 
[root@stapp01 ~]# 
[root@stapp01 ~]# 
[root@stapp01 ~]# 
[root@stapp01 ~]# docker ps
CONTAINER ID   IMAGE           COMMAND   CREATED              STATUS              PORTS     NAMES
b71b6813474d   ubuntu:latest   "bash"    About a minute ago   Up About a minute             kkloud
[root@stapp01 ~]# 
[root@stapp01 ~]# 
[root@stapp01 ~]# docker exec -it kkloud /bin/sh
# 
# 
# apt install apache2 -y
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following additional packages will be installed:
  apache2-bin apache2-data apache2-utils file libapr1 libaprutil1 libaprutil1-dbd-sqlite3 libaprutil1-ldap libexpat1 libgdbm-compat4
  libgdbm6 libicu66 libjansson4 liblua5.2-0 libmagic-mgc libmagic1 libperl5.30 libxml2 mime-support netbase perl perl-modules-5.30 ssl-cert
  tzdata xz-utils
Suggested packages:
  apache2-doc apache2-suexec-pristine | apache2-suexec-custom www-browser ufw gdbm-l10n perl-doc libterm-readline-gnu-perl
  | libterm-readline-perl-perl make libb-debug-perl liblocale-codes-perl openssl-blacklist
The following NEW packages will be installed:
  apache2 apache2-bin apache2-data apache2-utils file libapr1 libaprutil1 libaprutil1-dbd-sqlite3 libaprutil1-ldap libexpat1
  libgdbm-compat4 libgdbm6 libicu66 libjansson4 liblua5.2-0 libmagic-mgc libmagic1 libperl5.30 libxml2 mime-support netbase perl
  perl-modules-5.30 ssl-cert tzdata xz-utils
0 upgraded, 26 newly installed, 0 to remove and 0 not upgraded.
Need to get 18.8 MB of archives.
After this operation, 101 MB of additional disk space will be used.
Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 perl-modules-5.30 all 5.30.0-9ubuntu0.2 [2738 kB]
Get:2 http://archive.ubuntu.com/ubuntu focal/main amd64 libgdbm6 amd64 1.18.1-5 [27.4 kB]
Get:3 http://archive.ubuntu.com/ubuntu focal/main amd64 libgdbm-compat4 amd64 1.18.1-5 [6244 B]
Get:4 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libperl5.30 amd64 5.30.0-9ubuntu0.2 [3952 kB]
Get:5 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 perl amd64 5.30.0-9ubuntu0.2 [224 kB]
Get:6 http://archive.ubuntu.com/ubuntu focal/main amd64 libapr1 amd64 1.6.5-1ubuntu1 [91.4 kB]
Get:7 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libexpat1 amd64 2.2.9-1ubuntu0.4 [74.4 kB]
Get:8 http://archive.ubuntu.com/ubuntu focal/main amd64 libaprutil1 amd64 1.6.1-4ubuntu2 [84.7 kB]
Get:9 http://archive.ubuntu.com/ubuntu focal/main amd64 libaprutil1-dbd-sqlite3 amd64 1.6.1-4ubuntu2 [10.5 kB]
Get:10 http://archive.ubuntu.com/ubuntu focal/main amd64 libaprutil1-ldap amd64 1.6.1-4ubuntu2 [8736 B]
Get:11 http://archive.ubuntu.com/ubuntu focal/main amd64 libjansson4 amd64 2.12-1build1 [28.9 kB]
Get:12 http://archive.ubuntu.com/ubuntu focal/main amd64 liblua5.2-0 amd64 5.2.4-1.1build3 [106 kB]
Get:13 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 tzdata all 2021e-0ubuntu0.20.04 [295 kB]
Get:14 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libicu66 amd64 66.1-2ubuntu2.1 [8515 kB]
Get:15 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libxml2 amd64 2.9.10+dfsg-5ubuntu0.20.04.2 [640 kB]
Get:16 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 apache2-bin amd64 2.4.41-4ubuntu3.10 [1181 kB]
Get:17 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 apache2-data all 2.4.41-4ubuntu3.10 [158 kB]
Get:18 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 apache2-utils amd64 2.4.41-4ubuntu3.10 [84.5 kB]
Get:19 http://archive.ubuntu.com/ubuntu focal/main amd64 mime-support all 3.64ubuntu1 [30.6 kB]
Get:20 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 apache2 amd64 2.4.41-4ubuntu3.10 [95.5 kB]
Get:21 http://archive.ubuntu.com/ubuntu focal/main amd64 libmagic-mgc amd64 1:5.38-4 [218 kB]
Get:22 http://archive.ubuntu.com/ubuntu focal/main amd64 libmagic1 amd64 1:5.38-4 [75.9 kB]
Get:23 http://archive.ubuntu.com/ubuntu focal/main amd64 file amd64 1:5.38-4 [23.3 kB]
Get:24 http://archive.ubuntu.com/ubuntu focal/main amd64 netbase all 6.1 [13.1 kB]
Get:25 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 xz-utils amd64 5.2.4-1ubuntu1 [82.5 kB]
Get:26 http://archive.ubuntu.com/ubuntu focal/main amd64 ssl-cert all 1.0.39 [17.0 kB]
Fetched 18.8 MB in 1s (30.2 MB/s)     
debconf: delaying package configuration, since apt-utils is not installed
Selecting previously unselected package perl-modules-5.30.
(Reading database ... 4660 files and directories currently installed.)
Preparing to unpack .../00-perl-modules-5.30_5.30.0-9ubuntu0.2_all.deb ...
Unpacking perl-modules-5.30 (5.30.0-9ubuntu0.2) ...
Selecting previously unselected package libgdbm6:amd64.
Preparing to unpack .../01-libgdbm6_1.18.1-5_amd64.deb ...
Unpacking libgdbm6:amd64 (1.18.1-5) ...
Selecting previously unselected package libgdbm-compat4:amd64.
Preparing to unpack .../02-libgdbm-compat4_1.18.1-5_amd64.deb ...
Unpacking libgdbm-compat4:amd64 (1.18.1-5) ...
Selecting previously unselected package libperl5.30:amd64.
Preparing to unpack .../03-libperl5.30_5.30.0-9ubuntu0.2_amd64.deb ...
Unpacking libperl5.30:amd64 (5.30.0-9ubuntu0.2) ...
Selecting previously unselected package perl.
Preparing to unpack .../04-perl_5.30.0-9ubuntu0.2_amd64.deb ...
Unpacking perl (5.30.0-9ubuntu0.2) ...
Selecting previously unselected package libapr1:amd64.
Preparing to unpack .../05-libapr1_1.6.5-1ubuntu1_amd64.deb ...
Unpacking libapr1:amd64 (1.6.5-1ubuntu1) ...
Selecting previously unselected package libexpat1:amd64.
Preparing to unpack .../06-libexpat1_2.2.9-1ubuntu0.4_amd64.deb ...
Unpacking libexpat1:amd64 (2.2.9-1ubuntu0.4) ...
Selecting previously unselected package libaprutil1:amd64.
Preparing to unpack .../07-libaprutil1_1.6.1-4ubuntu2_amd64.deb ...
Unpacking libaprutil1:amd64 (1.6.1-4ubuntu2) ...
Selecting previously unselected package libaprutil1-dbd-sqlite3:amd64.
Preparing to unpack .../08-libaprutil1-dbd-sqlite3_1.6.1-4ubuntu2_amd64.deb ...
Unpacking libaprutil1-dbd-sqlite3:amd64 (1.6.1-4ubuntu2) ...
Selecting previously unselected package libaprutil1-ldap:amd64.
Preparing to unpack .../09-libaprutil1-ldap_1.6.1-4ubuntu2_amd64.deb ...
Unpacking libaprutil1-ldap:amd64 (1.6.1-4ubuntu2) ...
Selecting previously unselected package libjansson4:amd64.
Preparing to unpack .../10-libjansson4_2.12-1build1_amd64.deb ...
Unpacking libjansson4:amd64 (2.12-1build1) ...
Selecting previously unselected package liblua5.2-0:amd64.
Preparing to unpack .../11-liblua5.2-0_5.2.4-1.1build3_amd64.deb ...
Unpacking liblua5.2-0:amd64 (5.2.4-1.1build3) ...
Selecting previously unselected package tzdata.
Preparing to unpack .../12-tzdata_2021e-0ubuntu0.20.04_all.deb ...
Unpacking tzdata (2021e-0ubuntu0.20.04) ...
Selecting previously unselected package libicu66:amd64.
Preparing to unpack .../13-libicu66_66.1-2ubuntu2.1_amd64.deb ...
Unpacking libicu66:amd64 (66.1-2ubuntu2.1) ...
Selecting previously unselected package libxml2:amd64.
Preparing to unpack .../14-libxml2_2.9.10+dfsg-5ubuntu0.20.04.2_amd64.deb ...
Unpacking libxml2:amd64 (2.9.10+dfsg-5ubuntu0.20.04.2) ...
Selecting previously unselected package apache2-bin.
Preparing to unpack .../15-apache2-bin_2.4.41-4ubuntu3.10_amd64.deb ...
Unpacking apache2-bin (2.4.41-4ubuntu3.10) ...
Selecting previously unselected package apache2-data.
Preparing to unpack .../16-apache2-data_2.4.41-4ubuntu3.10_all.deb ...
Unpacking apache2-data (2.4.41-4ubuntu3.10) ...
Selecting previously unselected package apache2-utils.
Preparing to unpack .../17-apache2-utils_2.4.41-4ubuntu3.10_amd64.deb ...
Unpacking apache2-utils (2.4.41-4ubuntu3.10) ...
Selecting previously unselected package mime-support.
Preparing to unpack .../18-mime-support_3.64ubuntu1_all.deb ...
Unpacking mime-support (3.64ubuntu1) ...
Selecting previously unselected package apache2.
Preparing to unpack .../19-apache2_2.4.41-4ubuntu3.10_amd64.deb ...
Unpacking apache2 (2.4.41-4ubuntu3.10) ...
Selecting previously unselected package libmagic-mgc.
Preparing to unpack .../20-libmagic-mgc_1%3a5.38-4_amd64.deb ...
Unpacking libmagic-mgc (1:5.38-4) ...
Selecting previously unselected package libmagic1:amd64.
Preparing to unpack .../21-libmagic1_1%3a5.38-4_amd64.deb ...
Unpacking libmagic1:amd64 (1:5.38-4) ...
Selecting previously unselected package file.
Preparing to unpack .../22-file_1%3a5.38-4_amd64.deb ...
Unpacking file (1:5.38-4) ...
Selecting previously unselected package netbase.
Preparing to unpack .../23-netbase_6.1_all.deb ...
Unpacking netbase (6.1) ...
Selecting previously unselected package xz-utils.
Preparing to unpack .../24-xz-utils_5.2.4-1ubuntu1_amd64.deb ...
Unpacking xz-utils (5.2.4-1ubuntu1) ...
Selecting previously unselected package ssl-cert.
Preparing to unpack .../25-ssl-cert_1.0.39_all.deb ...
Unpacking ssl-cert (1.0.39) ...
Setting up libexpat1:amd64 (2.2.9-1ubuntu0.4) ...
Setting up perl-modules-5.30 (5.30.0-9ubuntu0.2) ...
Setting up mime-support (3.64ubuntu1) ...
Setting up libmagic-mgc (1:5.38-4) ...
Setting up libmagic1:amd64 (1:5.38-4) ...
Setting up libapr1:amd64 (1.6.5-1ubuntu1) ...
Setting up file (1:5.38-4) ...
Setting up libjansson4:amd64 (2.12-1build1) ...
Setting up tzdata (2021e-0ubuntu0.20.04) ...
debconf: unable to initialize frontend: Dialog
debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)
debconf: falling back to frontend: Readline
Configuring tzdata
------------------

Please select the geographic area in which you live. Subsequent configuration questions will narrow this down by presenting a list of
cities, representing the time zones in which they are located.

  1. Africa   3. Antarctica  5. Arctic  7. Atlantic  9. Indian    11. SystemV  13. Etc
  2. America  4. Australia   6. Asia    8. Europe    10. Pacific  12. US
Geographic area: 12

Please select the city or region corresponding to your time zone.

  1. Alaska    3. Arizona  5. Eastern  7. Indiana-Starke  9. Mountain  11. Samoa
  2. Aleutian  4. Central  6. Hawaii   8. Michigan        10. Pacific
Time zone: 1


Current default time zone: 'US/Alaska'
Local time is now:      Fri Mar 18 20:46:13 AKDT 2022.
Universal Time is now:  Sat Mar 19 04:46:13 UTC 2022.
Run 'dpkg-reconfigure tzdata' if you wish to change it.

Setting up ssl-cert (1.0.39) ...
debconf: unable to initialize frontend: Dialog
debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)
debconf: falling back to frontend: Readline
Setting up xz-utils (5.2.4-1ubuntu1) ...
update-alternatives: using /usr/bin/xz to provide /usr/bin/lzma (lzma) in auto mode
update-alternatives: warning: skip creation of /usr/share/man/man1/lzma.1.gz because associated file /usr/share/man/man1/xz.1.gz (of link group lzma) doesn't exist
update-alternatives: warning: skip creation of /usr/share/man/man1/unlzma.1.gz because associated file /usr/share/man/man1/unxz.1.gz (of link group lzma) doesn't exist
update-alternatives: warning: skip creation of /usr/share/man/man1/lzcat.1.gz because associated file /usr/share/man/man1/xzcat.1.gz (of link group lzma) doesn't exist
update-alternatives: warning: skip creation of /usr/share/man/man1/lzmore.1.gz because associated file /usr/share/man/man1/xzmore.1.gz (of link group lzma) doesn't exist
update-alternatives: warning: skip creation of /usr/share/man/man1/lzless.1.gz because associated file /usr/share/man/man1/xzless.1.gz (of link group lzma) doesn't exist
update-alternatives: warning: skip creation of /usr/share/man/man1/lzdiff.1.gz because associated file /usr/share/man/man1/xzdiff.1.gz (of link group lzma) doesn't exist
update-alternatives: warning: skip creation of /usr/share/man/man1/lzcmp.1.gz because associated file /usr/share/man/man1/xzcmp.1.gz (of link group lzma) doesn't exist
update-alternatives: warning: skip creation of /usr/share/man/man1/lzgrep.1.gz because associated file /usr/share/man/man1/xzgrep.1.gz (of link group lzma) doesn't exist
update-alternatives: warning: skip creation of /usr/share/man/man1/lzegrep.1.gz because associated file /usr/share/man/man1/xzegrep.1.gz (of link group lzma) doesn't exist
update-alternatives: warning: skip creation of /usr/share/man/man1/lzfgrep.1.gz because associated file /usr/share/man/man1/xzfgrep.1.gz (of link group lzma) doesn't exist
Setting up liblua5.2-0:amd64 (5.2.4-1.1build3) ...
Setting up netbase (6.1) ...
Setting up apache2-data (2.4.41-4ubuntu3.10) ...
Setting up libgdbm6:amd64 (1.18.1-5) ...
Setting up libaprutil1:amd64 (1.6.1-4ubuntu2) ...
Setting up libicu66:amd64 (66.1-2ubuntu2.1) ...
Setting up libaprutil1-ldap:amd64 (1.6.1-4ubuntu2) ...
Setting up libaprutil1-dbd-sqlite3:amd64 (1.6.1-4ubuntu2) ...
Setting up libgdbm-compat4:amd64 (1.18.1-5) ...
Setting up libperl5.30:amd64 (5.30.0-9ubuntu0.2) ...
Setting up libxml2:amd64 (2.9.10+dfsg-5ubuntu0.20.04.2) ...
Setting up apache2-utils (2.4.41-4ubuntu3.10) ...
Setting up perl (5.30.0-9ubuntu0.2) ...
Setting up apache2-bin (2.4.41-4ubuntu3.10) ...
Setting up apache2 (2.4.41-4ubuntu3.10) ...
Enabling module mpm_event.
Enabling module authz_core.
Enabling module authz_host.
Enabling module authn_core.
Enabling module auth_basic.
Enabling module access_compat.
Enabling module authn_file.
Enabling module authz_user.
Enabling module alias.
Enabling module dir.
Enabling module autoindex.
Enabling module env.
Enabling module mime.
Enabling module negotiation.
Enabling module setenvif.
Enabling module filter.
Enabling module deflate.
Enabling module status.
Enabling module reqtimeout.
Enabling conf charset.
Enabling conf localized-error-pages.
Enabling conf other-vhosts-access-log.
Enabling conf security.
Enabling conf serve-cgi-bin.
Enabling site 000-default.
invoke-rc.d: could not determine current runlevel
invoke-rc.d: policy-rc.d denied execution of start.
Processing triggers for libc-bin (2.31-0ubuntu9.7) ...
# 
# 
# 
# cd /etc/apache2
# 
# ls -ahl
total 88K
drwxr-xr-x  8 root root 4.0K Mar 18 20:46 .
drwxr-xr-x 38 root root 4.0K Mar 18 20:46 ..
-rw-r--r--  1 root root 7.1K Mar 16 08:52 apache2.conf
drwxr-xr-x  2 root root 4.0K Mar 18 20:46 conf-available
drwxr-xr-x  2 root root 4.0K Mar 18 20:46 conf-enabled
-rw-r--r--  1 root root 1.8K Sep 30  2020 envvars
-rw-r--r--  1 root root  31K Sep 30  2020 magic
drwxr-xr-x  2 root root  12K Mar 18 20:46 mods-available
drwxr-xr-x  2 root root 4.0K Mar 18 20:46 mods-enabled
-rw-r--r--  1 root root  320 Sep 30  2020 ports.conf
drwxr-xr-x  2 root root 4.0K Mar 18 20:46 sites-available
drwxr-xr-x  2 root root 4.0K Mar 18 20:46 sites-enabled
# vi ports.conf
/bin/sh: 10: vi: not found
# 
# which sed
/usr/bin/sed
# 
# 
# sed -i 's/Listen 80/Listen 5001/g' ports.conf
# 
# sed -i 's/:80/:5001/g' apache2.conf
# 
# 
#  sed -i 's/#ServerName www.example.com/ServerName localhost/g' apache2.conf
# 
# 
# cat ports.conf
# If you just change the port or add more ports here, you will likely also
# have to change the VirtualHost statement in
# /etc/apache2/sites-enabled/000-default.conf

Listen 5001

<IfModule ssl_module>
        Listen 443
</IfModule>

<IfModule mod_gnutls.c>
        Listen 443
</IfModule>

# vim: syntax=apache ts=4 sw=4 sts=4 sr noet
# 
# 
# 
# cat apache2.conf
# This is the main Apache server configuration file.  It contains the
# configuration directives that give the server its instructions.
# See http://httpd.apache.org/docs/2.4/ for detailed information about
# the directives and /usr/share/doc/apache2/README.Debian about Debian specific
# hints.
#
#
# Summary of how the Apache 2 configuration works in Debian:
# The Apache 2 web server configuration in Debian is quite different to
# upstream's suggested way to configure the web server. This is because Debian's
# default Apache2 installation attempts to make adding and removing modules,
# virtual hosts, and extra configuration directives as flexible as possible, in
# order to make automating the changes and administering the server as easy as
# possible.

# It is split into several files forming the configuration hierarchy outlined
# below, all located in the /etc/apache2/ directory:
#
#       /etc/apache2/
#       |-- apache2.conf
#       |       `--  ports.conf
#       |-- mods-enabled
#       |       |-- *.load
#       |       `-- *.conf
#       |-- conf-enabled
#       |       `-- *.conf
#       `-- sites-enabled
#               `-- *.conf
#
#
# * apache2.conf is the main configuration file (this file). It puts the pieces
#   together by including all remaining configuration files when starting up the
#   web server.
#
# * ports.conf is always included from the main configuration file. It is
#   supposed to determine listening ports for incoming connections which can be
#   customized anytime.
#
# * Configuration files in the mods-enabled/, conf-enabled/ and sites-enabled/
#   directories contain particular configuration snippets which manage modules,
#   global configuration fragments, or virtual host configurations,
#   respectively.
#
#   They are activated by symlinking available configuration files from their
#   respective *-available/ counterparts. These should be managed by using our
#   helpers a2enmod/a2dismod, a2ensite/a2dissite and a2enconf/a2disconf. See
#   their respective man pages for detailed information.
#
# * The binary is called apache2. Due to the use of environment variables, in
#   the default configuration, apache2 needs to be started/stopped with
#   /etc/init.d/apache2 or apache2ctl. Calling /usr/bin/apache2 directly will not
#   work with the default configuration.


# Global configuration
#

#
# ServerRoot: The top of the directory tree under which the server's
# configuration, error, and log files are kept.
#
# NOTE!  If you intend to place this on an NFS (or otherwise network)
# mounted filesystem then please read the Mutex documentation (available
# at <URL:http://httpd.apache.org/docs/2.4/mod/core.html#mutex>);
# you will save yourself a lot of trouble.
#
# Do NOT add a slash at the end of the directory path.
#
#ServerRoot "/etc/apache2"

#
# The accept serialization lock file MUST BE STORED ON A LOCAL DISK.
#
#Mutex file:${APACHE_LOCK_DIR} default

#
# The directory where shm and other runtime files will be stored.
#

DefaultRuntimeDir ${APACHE_RUN_DIR}

#
# PidFile: The file in which the server should record its process
# identification number when it starts.
# This needs to be set in /etc/apache2/envvars
#
PidFile ${APACHE_PID_FILE}

#
# Timeout: The number of seconds before receives and sends time out.
#
Timeout 300

#
# KeepAlive: Whether or not to allow persistent connections (more than
# one request per connection). Set to "Off" to deactivate.
#
KeepAlive On

#
# MaxKeepAliveRequests: The maximum number of requests to allow
# during a persistent connection. Set to 0 to allow an unlimited amount.
# We recommend you leave this number high, for maximum performance.
#
MaxKeepAliveRequests 100

#
# KeepAliveTimeout: Number of seconds to wait for the next request from the
# same client on the same connection.
#
KeepAliveTimeout 5


# These need to be set in /etc/apache2/envvars
User ${APACHE_RUN_USER}
Group ${APACHE_RUN_GROUP}

#
# HostnameLookups: Log the names of clients or just their IP addresses
# e.g., www.apache.org (on) or 204.62.129.132 (off).
# The default is off because it'd be overall better for the net if people
# had to knowingly turn this feature on, since enabling it means that
# each client request will result in AT LEAST one lookup request to the
# nameserver.
#
HostnameLookups Off

# ErrorLog: The location of the error log file.
# If you do not specify an ErrorLog directive within a <VirtualHost>
# container, error messages relating to that virtual host will be
# logged here.  If you *do* define an error logfile for a <VirtualHost>
# container, that host's errors will be logged there and not here.
#
ErrorLog ${APACHE_LOG_DIR}/error.log

#
# LogLevel: Control the severity of messages logged to the error_log.
# Available values: trace8, ..., trace1, debug, info, notice, warn,
# error, crit, alert, emerg.
# It is also possible to configure the log level for particular modules, e.g.
# "LogLevel info ssl:warn"
#
LogLevel warn

# Include module configuration:
IncludeOptional mods-enabled/*.load
IncludeOptional mods-enabled/*.conf

# Include list of ports to listen on
Include ports.conf


# Sets the default security model of the Apache2 HTTPD server. It does
# not allow access to the root filesystem outside of /usr/share and /var/www.
# The former is used by web applications packaged in Debian,
# the latter may be used for local directories served by the web server. If
# your system is serving content from a sub-directory in /srv you must allow
# access here, or in any related virtual host.
<Directory />
        Options FollowSymLinks
        AllowOverride None
        Require all denied
</Directory>

<Directory /usr/share>
        AllowOverride None
        Require all granted
</Directory>

<Directory /var/www/>
        Options Indexes FollowSymLinks
        AllowOverride None
        Require all granted
</Directory>

#<Directory /srv/>
#       Options Indexes FollowSymLinks
#       AllowOverride None
#       Require all granted
#</Directory>




# AccessFileName: The name of the file to look for in each directory
# for additional configuration directives.  See also the AllowOverride
# directive.
#
AccessFileName .htaccess

#
# The following lines prevent .htaccess and .htpasswd files from being
# viewed by Web clients.
#
<FilesMatch "^\.ht">
        Require all denied
</FilesMatch>


#
# The following directives define some format nicknames for use with
# a CustomLog directive.
#
# These deviate from the Common Log Format definitions in that they use %O
# (the actual bytes sent including headers) instead of %b (the size of the
# requested file), because the latter makes it impossible to detect partial
# requests.
#
# Note that the use of %{X-Forwarded-For}i instead of %h is not recommended.
# Use mod_remoteip instead.
#
LogFormat "%v:%p %h %l %u %t \"%r\" %>s %O \"%{Referer}i\" \"%{User-Agent}i\"" vhost_combined
LogFormat "%h %l %u %t \"%r\" %>s %O \"%{Referer}i\" \"%{User-Agent}i\"" combined
LogFormat "%h %l %u %t \"%r\" %>s %O" common
LogFormat "%{Referer}i -> %U" referer
LogFormat "%{User-agent}i" agent

# Include of directories ignores editors' and dpkg's backup files,
# see README.Debian for details.

# Include generic snippets of statements
IncludeOptional conf-enabled/*.conf

# Include the virtual host configurations:
IncludeOptional sites-enabled/*.conf

# vim: syntax=apache ts=4 sw=4 sts=4 sr noet
# 
# 
# 
# service apache2 start
 * Starting Apache httpd web server apache2                                                                                                  AH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 192.168.3.2. Set the 'ServerName' directive globally to suppress this message
 * 
# 
# 
# 
# service apache2 status
 * apache2 is running
# 
# 
# 
# curl -Ik localhost:5001
HTTP/1.1 200 OK
Date: Sat, 19 Mar 2022 04:50:56 GMT
Server: Apache/2.4.41 (Ubuntu)
Last-Modified: Sat, 19 Mar 2022 04:46:14 GMT
ETag: "2aa6-5da8af170b5f3"
Accept-Ranges: bytes
Content-Length: 10918
Vary: Accept-Encoding
Content-Type: text/html

--------------------------------------------------------------------------------------------------------------------------------------------
Task 47 : 20/Mar/2022
Kubernetes Shared Volumes

We are working on an application that will be deployed on multiple containers within a pod on Kubernetes cluster. There is a requirement to share a volume among the containers to save some temporary data. The Nautilus DevOps team is developing a similar template to replicate the scenario. Below you can find more details about it.

    Create a pod named volume-share-devops.

    For the first container, use image centos with latest tag only and remember to mention the tag i.e centos:latest, container should be named as volume-container-devops-1, and run a sleep command for it so that it remains in running state. Volume volume-share should be mounted at path /tmp/media.

    For the second container, use image centos with the latest tag only and remember to mention the tag i.e centos:latest, container should be named as volume-container-devops-2, and again run a sleep command for it so that it remains in running state. Volume volume-share should be mounted at path /tmp/cluster.

    Volume name should be volume-share of type emptyDir.

    After creating the pod, exec into the first container i.e volume-container-devops-1, and just for testing create a file media.txt with any content under the mounted path of first container i.e /tmp/media.

    The file media.txt should be present under the mounted path /tmp/cluster on the second container volume-container-devops-2 as well, since they are using a shared volume.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

1.
kubectl get services
	NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
	kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   13m

2.
kubectl get pods
	No resources found in default namespace.

3.
vi /tmp/volume.yaml
		apiVersion: v1
		kind: Pod
		metadata:
		  name: volume-share-devops
		  labels:
		    name: myapp
		spec:
		  volumes:
		    - name: volume-share
		      emptyDir: {}
		  containers:
		    - name: volume-container-devops-1
		      image: centos:latest
		      command: ["/bin/bash", "-c", "sleep 10000"]
		      volumeMounts:
		        - name: volume-share
		          mountPath: /tmp/media
		    - name: volume-container-devops-2
		      image: centos:latest
		      command: ["/bin/bash", "-c", "sleep 10000"]
		      volumeMounts:
		        - name: volume-share
		          mountPath: /tmp/cluster

4.
kubectl create -f /tmp/volume.yaml 
	pod/volume-share-devops created

5.
kubectl get pods
	NAME                  READY   STATUS              RESTARTS   AGE
	volume-share-devops   0/2     ContainerCreating   0          7s
6.
kubectl get pods
	NAME                  READY   STATUS    RESTARTS   AGE
	volume-share-devops   2/2     Running   0          26s

7.
kubectl get pods -o wide
	NAME                  READY   STATUS    RESTARTS   AGE   IP           NODE                      NOMINATED NODE   READINESS GATES
	volume-share-devops   2/2     Running   0          40s   10.244.0.5   kodekloud-control-plane   <none>           <none>

8.
kubectl exec -it volume-share-devops -c volume-container-devops-1 -- /bin/bash
	[root@volume-share-devops /]# 
	[root@volume-share-devops /]# echo "Welcome to xFusionCorps!" > /tmp/media/media.txt
	[root@volume-share-devops /]# 
	[root@volume-share-devops /]# cat /tmp/media/media.txt 
		Welcome to xFusionCorps!
	[root@volume-share-devops /]# 
	[root@volume-share-devops /]# exit
exit

9.
kubectl exec -it volume-share-devops -c volume-container-devops-2 -- ls /tmp/cluster
	media.txt

--------------------------------------------------------------------------------------------------------------------------------------------
Task 48 : 22/Mar/2022
ReplicationController in Kubernetes

The Nautilus DevOps team wants to create a ReplicationController to deploy several pods. They are going to deploy applications on these pods, these applications need highly available infrastructure. Below you can find exact details, create the ReplicationController accordingly.

    Create a ReplicationController using nginx image, preferably with latest tag, and name it as nginx-replicationcontroller.

    Labels app should be nginx_app, and labels type should be front-end. The container should be named as nginx-container and also make sure replica counts are 3.

All pods should be running state after deployment.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

thor@jump_host ~$ kubectl get namespace
NAME                 STATUS   AGE
default              Active   62m
kube-node-lease      Active   62m
kube-public          Active   62m
kube-system          Active   62m
local-path-storage   Active   62m
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get pods
No resources found in default namespace.
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ vi /tmp/replica.yml
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ cat /tmp/replica.yml 
apiVersion: v1

kind: ReplicationController

metadata:

  name: nginx-replicationcontroller

  labels:

    app: nginx_app

    type: front-end

spec:

  replicas: 3

  selector:

    app: nginx_app

  template:

    metadata:

      name: nginx_pod

      labels:

        app: nginx_app

        type: front-end

    spec:

      containers:

        - name: nginx-container

          image: nginx:latest

          ports:

            - containerPort: 80 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl create -f /tmp/replica.yml 
replicationcontroller/nginx-replicationcontroller created
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get pods
NAME                                READY   STATUS              RESTARTS   AGE
nginx-replicationcontroller-6tk8s   0/1     ContainerCreating   0          17s
nginx-replicationcontroller-8sfw5   1/1     Running             0          17s
nginx-replicationcontroller-959vx   1/1     Running             0          17s
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get pods
NAME                                READY   STATUS    RESTARTS   AGE
nginx-replicationcontroller-6tk8s   1/1     Running   0          22s
nginx-replicationcontroller-8sfw5   1/1     Running   0          22s
nginx-replicationcontroller-959vx   1/1     Running   0          22s
thor@jump_host ~$ kubectl get pods
NAME                                READY   STATUS    RESTARTS   AGE
nginx-replicationcontroller-6tk8s   1/1     Running   0          24s
nginx-replicationcontroller-8sfw5   1/1     Running   0          24s
nginx-replicationcontroller-959vx   1/1     Running   0          24s
thor@jump_host ~$ 
thor@jump_host ~$ kubectl exec nginx-replicationcontroller-6tk8s -- curl http://localhost
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
100   615  100   615    0     0   600k      0 --:--:-- --:--:-- --:--:--  600k

--------------------------------------------------------------------------------------------------------------------------------------------
Task 49 : 24/Mar/2022
Git Revert Some Changes

The Nautilus application development team was working on a git repository /usr/src/kodekloudrepos/beta present on Storage server in Stratos DC. However, they reported an issue with the recent commits being pushed to this repo. They have asked the DevOps team to revert repo HEAD to last commit. Below are more details about the task:

    In /usr/src/kodekloudrepos/beta git repository, revert the latest commit ( HEAD ) to the previous commit (JFYI the previous commit hash should be with initial commit message ).

    Use revert beta message (please use all small letters for commit message) for the new revert commit.


thor@jump_host ~$ ssh natasha@ststor01
The authenticity of host 'ststor01 (172.16.238.15)' can't be established.
ECDSA key fingerprint is SHA256:T7UXzHW+LeEeNkv73OTPOhek8Of2LkVCFEhN9lBPkfE.
ECDSA key fingerprint is MD5:b5:54:33:9f:35:0d:a8:92:25:d0:bb:db:e0:28:da:62.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'ststor01,172.16.238.15' (ECDSA) to the list of known hosts.
natasha@ststor01's password: 
[natasha@ststor01 ~]$ sudo su - 

We trust you have received the usual lecture from the local System
Administrator. It usually boils down to these three things:

    #1) Respect the privacy of others.
    #2) Think before you type.
    #3) With great power comes great responsibility.

[sudo] password for natasha: 
[root@ststor01 ~]# 
[root@ststor01 ~]# 
[root@ststor01 ~]# 
[root@ststor01 ~]# cd /usr/src/kodekloudrepos/beta/
[root@ststor01 beta]# ls -ahl
total 16K
drwxr-xr-x 3 root root 4.0K Mar 24 07:33 .
drwxr-xr-x 3 root root 4.0K Mar 24 07:33 ..
-rw-r--r-- 1 root root   33 Mar 24 07:33 beta.txt
drwxr-xr-x 8 root root 4.0K Mar 24 07:33 .git
[root@ststor01 beta]# git status
# On branch master
# Untracked files:
#   (use "git add <file>..." to include in what will be committed)
#
#       beta.txt
nothing added to commit but untracked files present (use "git add" to track)
[root@ststor01 beta]# 
[root@ststor01 beta]# 
[root@ststor01 beta]# git log
commit cd69d88c6e045317a72558dbe8d0317cbd746a3f
Author: Admin <admin@kodekloud.com>
Date:   Thu Mar 24 07:33:32 2022 +0000

    add data.txt file

commit 5081b7a926a8f8eac484fe8a1ebe38dda72c0aec
Author: Admin <admin@kodekloud.com>
Date:   Thu Mar 24 07:33:32 2022 +0000

    initial commit
[root@ststor01 beta]# git revert HEAD
[master 5043e35] Revert "add data.txt file"
 1 file changed, 1 insertion(+)
 create mode 100644 info.txt
[root@ststor01 beta]# git add .
[root@ststor01 beta]# git commit -m "revert beta"
[master 5a846d4] revert beta
 1 file changed, 1 insertion(+)
 create mode 100644 beta.txt
[root@ststor01 beta]# 
[root@ststor01 beta]# git log
commit 5a846d477761f6238c832d9c7fc3f033c9802c7b
Author: Admin <admin@kodekloud.com>
Date:   Thu Mar 24 07:38:00 2022 +0000

    revert beta

commit 5043e350cffee736b097bd2bbbbf4423b1658221
Author: Admin <admin@kodekloud.com>
Date:   Thu Mar 24 07:35:52 2022 +0000

    Revert "add data.txt file"
    
    This reverts commit cd69d88c6e045317a72558dbe8d0317cbd746a3f.

commit cd69d88c6e045317a72558dbe8d0317cbd746a3f
Author: Admin <admin@kodekloud.com>
Date:   Thu Mar 24 07:33:32 2022 +0000

    add data.txt file

commit 5081b7a926a8f8eac484fe8a1ebe38dda72c0aec
Author: Admin <admin@kodekloud.com>
Date:   Thu Mar 24 07:33:32 2022 +0000

    initial commit
[root@ststor01 beta]# 

--------------------------------------------------------------------------------------------------------------------------------------------
Task 50 : 25/Mar/2022

Ansible Facts Gathering
The Nautilus DevOps team is trying to setup a simple Apache web server on all app servers in Stratos DC using Ansible. They also want to create a sample html page for now with some app specific data on it. Below you can find more details about the task.

You will find a valid inventory file /home/thor/playbooks/inventory on jump host (which we are using as an Ansible controller).

    Create a playbook index.yml under /home/thor/playbooks directory on jump host. Using blockinfile Ansible module create a file facts.txt under /root directory on all app servers and add the following given block in it. You will need to enable facts gathering for this task.

Ansible managed node IP is <default ipv4 address>

(You can obtain default ipv4 address from Ansible's gathered facts by using the correct Ansible variable while taking into account Jinja2 syntax)

    Install httpd server on all apps. After that make a copy of facts.txt file as index.html under /var/www/html directory. Make sure to start httpd service after that.

Note: Do not create a separate role for this task, just add all of the changes in index.yml playbook.

thor@jump_host ~$ cd /home/thor/playbooks/
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ ls -ahl
total 16K
drwxr-xr-x 2 thor thor 4.0K Mar 25 11:16 .
drwxr----- 1 thor thor 4.0K Mar 25 11:16 ..
-rw-r--r-- 1 thor thor   36 Mar 25 11:16 ansible.cfg
-rw-r--r-- 1 thor thor  237 Mar 25 11:16 inventory
thor@jump_host ~/playbooks$ cat inventory 
stapp01 ansible_host=172.16.238.10 ansible_ssh_pass=Ir0nM@n ansible_user=tony
stapp02 ansible_host=172.16.238.11 ansible_ssh_pass=Am3ric@ ansible_user=steve
stapp03 ansible_host=172.16.238.12 ansible_ssh_pass=BigGr33n ansible_user=bannerthor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ vi index.yml
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ cat index.yml 
---
-
  hosts: stapp01, stapp02, stapp03
  gather_facts: true
  become: yes
  become_method: sudo
  tasks:
    - name: create a  file using blockinfile
      blockinfile:
       create: yes
       path: /root/facts.txt
       block: |
         Ansible managed node IP is {{ ansible_default_ipv4.address }}

    - name: Install apache packages
      package:
       name: httpd

    - name: file copy
      shell: cp /root/facts.txt /var/www/html/index.html

    - name: ensure httpd is running
      systemd:
       name: httpd
       state: restarted
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ ansible-playbook -i inventory index.yml 

PLAY [stapp01, stapp02, stapp03] ************************************************************************************************************

TASK [Gathering Facts] **********************************************************************************************************************
ok: [stapp02]
ok: [stapp01]
ok: [stapp03]

TASK [create a  file using blockinfile] *****************************************************************************************************
changed: [stapp02]
changed: [stapp01]
changed: [stapp03]

TASK [Install apache packages] **************************************************************************************************************
changed: [stapp01]
changed: [stapp03]
changed: [stapp02]

TASK [file copy] ****************************************************************************************************************************
changed: [stapp02]
changed: [stapp03]
changed: [stapp01]

TASK [ensure httpd is running] **************************************************************************************************************
changed: [stapp01]
changed: [stapp02]
changed: [stapp03]

PLAY RECAP **********************************************************************************************************************************
stapp01                    : ok=5    changed=4    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
stapp02                    : ok=5    changed=4    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
stapp03                    : ok=5    changed=4    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ curl -i http://stapp01
HTTP/1.1 200 OK
Date: Fri, 25 Mar 2022 11:22:09 GMT
Server: Apache/2.4.6 (CentOS)
Last-Modified: Fri, 25 Mar 2022 11:21:53 GMT
ETag: "63-5db092b69f1bb"
Accept-Ranges: bytes
Content-Length: 99
Content-Type: text/html; charset=UTF-8

# BEGIN ANSIBLE MANAGED BLOCK
Ansible managed node IP is 172.16.238.10
# END ANSIBLE MANAGED BLOCK
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ curl http://stapp01
# BEGIN ANSIBLE MANAGED BLOCK
Ansible managed node IP is 172.16.238.10
# END ANSIBLE MANAGED BLOCK
thor@jump_host ~/playbooks$ curl http://stapp02
# BEGIN ANSIBLE MANAGED BLOCK
Ansible managed node IP is 172.16.238.11
# END ANSIBLE MANAGED BLOCK
thor@jump_host ~/playbooks$ curl http://stapp03
# BEGIN ANSIBLE MANAGED BLOCK
Ansible managed node IP is 172.16.238.12
# END ANSIBLE MANAGED BLOCK
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ 

--------------------------------------------------------------------------------------------------------------------------------------------
Task 51 : 9/Apr/2022
Deploy Guest Book App on Kubernetes

The Nautilus Application development team has finished development of one of the applications and it is ready for deployment. It is a guestbook application that will be used to manage entries for guests/visitors. As per discussion with the DevOps team, they have finalized the infrastructure that will be deployed on Kubernetes cluster. Below you can find more details about it.

BACK-END TIER

    Create a deployment named redis-master for Redis master.

    a.) Replicas count should be 1.

    b.) Container name should be master-redis-xfusion and it should use image redis.

    c.) Request resources as CPU should be 100m and Memory should be 100Mi.

    d.) Container port should be redis default port i.e 6379.

    Create a service named redis-master for Redis master. Port and targetPort should be Redis default port i.e 6379.

    Create another deployment named redis-slave for Redis slave.

    a.) Replicas count should be 2.

    b.) Container name should be slave-redis-xfusion and it should use gcr.io/google_samples/gb-redisslave:v3 image.

    c.) Requests resources as CPU should be 100m and Memory should be 100Mi.

    d.) Define an environment variable named GET_HOSTS_FROM and its value should be dns.

    e.) Container port should be Redis default port i.e 6379.

    Create another service named redis-slave. It should use Redis default port i.e 6379.

FRONT END TIER

    Create a deployment named frontend.

    a.) Replicas count should be 3.

    b.) Container name should be php-redis-xfusion and it should use gcr.io/google-samples/gb-frontend:v4 image.

    c.) Request resources as CPU should be 100m and Memory should be 100Mi.

    d.) Define an environment variable named as GET_HOSTS_FROM and its value should be dns.

    e.) Container port should be 80.

    Create a service named frontend. Its type should be NodePort, port should be 80 and its nodePort should be 30009.

Finally, you can check the guestbook app by clicking on + button in the top left corner and Select port to view on Host 1 then enter your nodePort.

You can use any labels as per your choice.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

1.
kubectl get deploy
	No resources found in default namespace.

2.
kubectl get pods
	No resources found in default namespace.

3.
kubectl get service
	NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
	kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4h13m

4.
cd /tmp

5.
vi Back-end-Deploy-Redis-Master-Guest-Book-App.yaml
		---
		apiVersion: apps/v1
		kind: Deployment
		metadata:
		  name: redis-master
		spec:
		  replicas: 1
		  selector:
		    matchLabels:
		      app: redis-master
		      tier: back-end
		  template:
		    metadata:
		      labels:
		        app: redis-master
		        tier: back-end
		    spec:
		      containers:
		        - name: master-redis-xfusion
		          image: redis
		          resources:
		            requests:
		              memory: "100Mi"
		              cpu: "100m"
		          ports:
		            - containerPort: 6379


6.
vi Back-end-Deploy-Redis-slave-Guest-Book-App.yaml
		---
		apiVersion: apps/v1
		kind: Deployment
		metadata:
		  name: redis-slave
		spec:
		  replicas: 2
		  selector:
		    matchLabels:
		      app: redis-slave
		      tier: back-end
		  template:
		    metadata:
		      labels:
		        app: redis-slave
		        tier: back-end
		    spec:
		      containers:
		        - name: slave-redis-xfusion
		          image: gcr.io/google_samples/gb-redisslave:v3
		          resources:
		            requests:
		              memory: "100Mi"
		              cpu: "100m"
		          env:
		            - name: GET_HOSTS_FROM
		              value: dns
		          ports:
		            - containerPort: 6379

7.
vi Back-end-service-Redis-Master-Guest-Book-App.yaml
		---
		apiVersion: v1
		kind: Service
		metadata:
		  name: redis-master
		spec:
		  type: ClusterIP
		  selector:
		    app: redis-master
		    tier: back-end
		  ports:
		    - port: 6379
		      targetPort: 6379

8.
vi Back-end-service-Redis-slave-Guest-Book-App.yaml
		---
		apiVersion: v1
		kind: Service
		metadata:
		  name: redis-slave
		spec:
		  type: ClusterIP
		  selector:
		    app: redis-slave
		    tier: back-end
		  ports:
		    - port: 6379
		      targetPort: 6379

9.
vi Front-end-Deploy-Redis-php-Guest-Book-App.yaml
		---
		apiVersion: apps/v1
		kind: Deployment
		metadata:
		  name: frontend
		spec:
		  replicas: 3
		  selector:
		    matchLabels:
		      app: guestbook
		      tier: front-end
		  template:
		    metadata:
		      labels:
		        app: guestbook
		        tier: front-end
		    spec:
		      containers:
		        - name: php-redis-xfusion
		          image: gcr.io/google-samples/gb-frontend:v4
		          resources:
		            requests:
		              memory: "100Mi"
		              cpu: "100m"
		          env:
		            - name: GET_HOSTS_FROM
		              value: dns
		          ports:
		            - containerPort: 80

10.
vi Front-end-service-Redis-php-Guest-Book-App.yaml
		---
		apiVersion: v1
		kind: Service
		metadata:
		  name: frontend
		spec:
		  type: NodePort
		  selector:
		    app: guestbook
		    tier: front-end
		  ports:
		    - port: 80
		      targetPort: 80
		      nodePort: 30009

11.Create Backend Master deployment

kubectl apply -f Back-end-Deploy-Redis-Master-Guest-Book-App.yaml
	deployment.apps/redis-master created

12.Create Backend  Master service 

kubectl apply -f Back-end-service-Redis-Master-Guest-Book-App.yaml
	service/redis-master created

13.Create Backend slave deployment

kubectl apply -f Back-end-Deploy-Redis-slave-Guest-Book-App.yaml
	deployment.apps/redis-slave created

14.Create Backend slave service 

kubectl apply -f Back-end-service-Redis-slave-Guest-Book-App.yaml
	service/redis-slave created

15.Create frontend deployment

kubectl apply -f Front-end-Deploy-Redis-php-Guest-Book-App.yaml
	deployment.apps/frontend created	 

16.Create front-end-service 

kubectl apply -f Front-end-service-Redis-php-Guest-Book-App.yaml
	service/frontend created


17.
 kubectl get deploy
	NAME           READY   UP-TO-DATE   AVAILABLE   AGE
	frontend       0/3     3            0           30s
	redis-master   1/1     1            1           88s
	redis-slave    2/2     2            2           59s

18.
kubectl get pods
	NAME                           READY   STATUS    RESTARTS   AGE
	frontend-586bdcd8bb-hkqm6      1/1     Running   0          118s
	frontend-586bdcd8bb-nts2c      1/1     Running   0          118s
	frontend-586bdcd8bb-skpw2      1/1     Running   0          118s
	redis-master-fd5fb5746-7qmw4   1/1     Running   0          2m55s
	redis-slave-8b57b5779-7pgc4    1/1     Running   0          2m27s
	redis-slave-8b57b5779-92szx    1/1     Running   0          2m27s

19.
 kubectl get service
	NAME           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
	frontend       NodePort    10.96.79.21     <none>        80:30009/TCP   30s
	kubernetes     ClusterIP   10.96.0.1       <none>        443/TCP        4h20m
	redis-master   ClusterIP   10.96.104.169   <none>        6379/TCP       92s
	redis-slave    ClusterIP   10.96.203.200   <none>        6379/TCP       65s

20.
 kubectl exec frontend-586bdcd8bb-hkqm6 -- curl -I http://localhost/
	  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
	                                 Dload  Upload   Total   Spent    Left  Speed
	  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0HTTP/1.1 200 OK
	Date: Sat, 09 Apr 2022 10:36:14 GMT
	Server: Apache/2.4.10 (Debian) PHP/5.6.20
	Last-Modified: Wed, 09 Sep 2015 18:35:04 GMT
	ETag: "399-51f54bdb4a600"
	Accept-Ranges: bytes
	Content-Length: 921
	Vary: Accept-Encoding
	Content-Type: text/html

	  0   921    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0

21. View Port <Advanced setting> : 30009
--------------------------------------------------------------------------------------------------------------------------------------------
Task 52 : 15/Apr/2022
Puppet Setup Database

The Nautilus DevOps team had a meeting with development team last week to discuss about some new requirements for an application deployment. Team is working on to setup a mariadb database server on Nautilus DB Server in Stratos Datacenter. They want to setup the same using Puppet. Below you can find more details about the requirements:

Create a puppet programming file news.pp under /etc/puppetlabs/code/environments/production/manifests directory on puppet master node i.e on Jump Server. Define a class mysql_database in puppet programming code and perform below mentioned tasks:

    Install package mariadb-server (whichever version is available by default in yum repo) on puppet agent node i.e on DB Server also start its service.

    Create a database kodekloud_db1 , a database userkodekloud_tim and set passwordTmPcZjtRQx for this new user also remember host should be localhost. Finally grant some usual permissions like select, update (or full) ect to this newly created user on newly created database.

Notes: :- Please make sure to run the puppet agent test using sudo on agent nodes, otherwise you can face certificate issues. In that case you will have to clean the certificates first and then you will be able to run the puppet agent test.

:- Before clicking on the Check button please make sure to verify puppet server and puppet agent services are up and running on the respective servers, also please make sure to run puppet agent test to apply/test the changes manually first.

:- Please note that once lab is loaded, the puppet server service should start automatically on puppet master server, however it can take upto 2-3 minutes to start.


1. Login to root user on jump host and go to mention manifest directory
	root@jump_host ~# cd /etc/puppetlabs/code/environments/production/manifests/
	root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
	root@jump_host /etc/puppetlabs/code/environments/production/manifests# ls -ahl
		total 8.0K
		drwxr-xr-x 1 puppet puppet 4.0K Jul 13  2021 .
		drwxr-xr-x 1 puppet puppet 4.0K Aug  9  2021 ..

2. Create the puppet manifest file and verify 
	root@jump_host /etc/puppetlabs/code/environments/production/manifests# vi news.pp
	 
	root@jump_host /etc/puppetlabs/code/environments/production/manifests# cat news.pp 
		class mysql_database {
		    package {'mariadb-server':
		      ensure => installed
		    }

		    service {'mariadb':
			ensure    => running,
			enable    => true,
		    }    

		    mysql::db { 'kodekloud_db1':
		      user     => 'kodekloud_tim',
		      password => 'TmPcZjtRQx',
		      host     => 'localhost',
		      grant    => ['ALL'],
		    }
		}

		node 'stdb01.stratos.xfusioncorp.com' {
		  include mysql_database
		}

3. Validate the news.pp file
	root@jump_host /etc/puppetlabs/code/environments/production/manifests# puppet parser validate news.pp
	root@jump_host /etc/puppetlabs/code/environments/production/manifests# 

4. Login to DB server , switch to root and puppet run test agent
	root@jump_host /etc/puppetlabs/code/environments/production/manifests# ssh peter@stdb01
	The authenticity of host 'stdb01 (172.16.239.10)' can't be established.
	ECDSA key fingerprint is SHA256:XEEWbBLqOUWSgyQ/M3FI1iWhqdu3LZCE0A3b5AdgHVc.
	ECDSA key fingerprint is MD5:42:dc:82:00:61:4f:8f:33:ff:b5:d1:cd:0a:da:1c:b7.
	Are you sure you want to continue connecting (yes/no)? yes
	Warning: Permanently added 'stdb01,172.16.239.10' (ECDSA) to the list of known hosts.
	peter@stdb01's password: 
	
	[peter@stdb01 ~]$ sudo su -

	We trust you have received the usual lecture from the local System
	Administrator. It usually boils down to these three things:

	    #1) Respect the privacy of others.
	    #2) Think before you type.
	    #3) With great power comes great responsibility.

	[sudo] password for peter: 
	[root@stdb01 ~]# 

	[root@stdb01 ~]# puppet agent -tv
		Info: Using configured environment 'production'
		Info: Retrieving pluginfacts
		Info: Retrieving plugin
		Info: Retrieving locales
		Info: Loading facts
		Info: Caching catalog for stdb01.stratos.xfusioncorp.com
		Info: Applying configuration version '1650080109'
		Notice: Applied catalog in 0.57 seconds

5. Check mariadb running status 
	[root@stdb01 ~]# systemctl status mariadb
	● mariadb.service - MariaDB database server
	   Loaded: loaded (/usr/lib/systemd/system/mariadb.service; enabled; vendor preset: disabled)
	   Active: active (running) since Sat 2022-04-16 03:34:02 UTC; 1min 29s ago
	 Main PID: 1393 (mysqld_safe)
	   CGroup: /docker/1671009373eeb7a708dd181a7f2be1c35d33621d36c3858b5f7f9ac27f62e2c3/system.slice/mariadb.service
		   ├─1393 /bin/sh /usr/bin/mysqld_safe --basedir=/usr
		   └─1557 /usr/libexec/mysqld --basedir=/usr --datadir=/var/lib/mysql --plugin-dir=/usr/lib64/mysql/plugin --log-error=/var/log/ma...

	Apr 16 03:34:00 stdb01.stratos.xfusioncorp.com systemd[1393]: Executing: /usr/bin/mysqld_safe --basedir=/usr
	Apr 16 03:34:00 stdb01.stratos.xfusioncorp.com systemd[1394]: Executing: /usr/libexec/mariadb-wait-ready 1393
	Apr 16 03:34:00 stdb01.stratos.xfusioncorp.com mysqld_safe[1393]: 220416 03:34:00 mysqld_safe Logging to '/var/log/mariadb/mariadb.log'.
	Apr 16 03:34:00 stdb01.stratos.xfusioncorp.com mysqld_safe[1393]: 220416 03:34:00 mysqld_safe Starting mysqld daemon with databases f...mysql
	Apr 16 03:34:02 stdb01.stratos.xfusioncorp.com systemd[1]: Child 1394 belongs to mariadb.service
	Apr 16 03:34:02 stdb01.stratos.xfusioncorp.com systemd[1]: mariadb.service: control process exited, code=exited status=0
	Apr 16 03:34:02 stdb01.stratos.xfusioncorp.com systemd[1]: mariadb.service got final SIGCHLD for state start-post
	Apr 16 03:34:02 stdb01.stratos.xfusioncorp.com systemd[1]: mariadb.service changed start-post -> running
	Apr 16 03:34:02 stdb01.stratos.xfusioncorp.com systemd[1]: Job mariadb.service/start finished, result=done
	Apr 16 03:34:02 stdb01.stratos.xfusioncorp.com systemd[1]: Started MariaDB database server.
	Hint: Some lines were ellipsized, use -l to show in full.

6. Login to DB server using give n user credentials
	[root@stdb01 ~]# mysql -u kodekloud_tim -p kodekloud_db1 -h localhost
		Enter password: 
		Welcome to the MariaDB monitor.  Commands end with ; or \g.
		Your MariaDB connection id is 50
		Server version: 5.5.68-MariaDB MariaDB Server

		Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

		Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

		MariaDB [kodekloud_db1]> 
		MariaDB [kodekloud_db1]> 
		MariaDB [kodekloud_db1]> :
		    -> ;
		ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near ':' at line 1
		MariaDB [kodekloud_db1]> ;
		ERROR: No query specified

		MariaDB [kodekloud_db1]> select * from kodekloud_db1;
		ERROR 1146 (42S02): Table 'kodekloud_db1.kodekloud_db1' doesn't exist
		MariaDB [kodekloud_db1]> exit
		Bye
	[root@stdb01 ~]# 

--------------------------------------------------------------------------------------------------------------------------------
Task 53 : 21/Apr/2022
Puppet Create Symlinks
Some directory structure in the Stratos Datacenter needs to be changed, there is a directory that needs to be linked to the default Apache document root. We need to accomplish this task using Puppet, as per the instructions given below:

Create a puppet programming file apps.pp under /etc/puppetlabs/code/environments/production/manifests directory on puppet master node i.e on Jump Server. Within that define a class symlink and perform below mentioned tasks:

    Create a symbolic link through puppet programming code. The source path should be /opt/security and destination path should be /var/www/html on Puppet agents 3 i.e on App Servers 3.

    Create a blank file blog.txt under /opt/security directory on puppet agent 3 nodes i.e on App Servers 3.

Notes: :- Please make sure to run the puppet agent test using sudo on agent nodes, otherwise you can face certificate issues. In that case you will have to clean the certificates first and then you will be able to run the puppet agent test.

:- Before clicking on the Check button please make sure to verify puppet server and puppet agent services are up and running on the respective servers, also please make sure to run puppet agent test to apply/test the changes manually first.

:- Please note that once lab is loaded, the puppet server service should start automatically on puppet master server, however it can take upto 2-3 minutes to start.

thor@jump_host ~$ sudo su -

We trust you have received the usual lecture from the local System
Administrator. It usually boils down to these three things:

    #1) Respect the privacy of others.
    #2) Think before you type.
    #3) With great power comes great responsibility.

[sudo] password for thor: 
root@jump_host ~# 
root@jump_host ~# 
root@jump_host ~# 
root@jump_host ~# cd /etc/puppetlabs/code/environments/production/manifests
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# ls -ahl
total 8.0K
drwxr-xr-x 1 puppet puppet 4.0K Jul 13  2021 .
drwxr-xr-x 1 puppet puppet 4.0K Aug  9  2021 ..
root@jump_host /etc/puppetlabs/code/environments/production/manifests# vi apps.pp
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# cat apps.pp
class symlink {

  # First create a symlink to /var/www/html

  file { '/opt/security':

    ensure => 'link',

    target => '/var/www/html',

  }

   # Now create blog.txt under /opt/security

  file { '/opt/security/blog.txt':

    ensure => 'present',

  }

}

node 'stapp03.stratos.xfusioncorp.com' {

  include symlink

}
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# puppet parser validate apps.pp
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# ssh banner@stapp03
The authenticity of host 'stapp03 (172.16.238.12)' can't be established.
ECDSA key fingerprint is SHA256:0Nt2RVU4t8V7/rRWUyB+8tN7VvdIhGY2Gk4HUPWTQfM.
ECDSA key fingerprint is MD5:a9:29:ee:b1:f2:85:dd:f2:6e:1f:d2:bd:7d:81:da:8c.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'stapp03,172.16.238.12' (ECDSA) to the list of known hosts.
banner@stapp03's password: 
[banner@stapp03 ~]$ 
[banner@stapp03 ~]$ 
[banner@stapp03 ~]$ sudo su -

We trust you have received the usual lecture from the local System
Administrator. It usually boils down to these three things:

    #1) Respect the privacy of others.
    #2) Think before you type.
    #3) With great power comes great responsibility.

[sudo] password for banner: 
[root@stapp03 ~]# 
[root@stapp03 ~]# 
[root@stapp03 ~]# 
[root@stapp03 ~]# puppet agent -tv
Info: Using configured environment 'production'
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Retrieving locales
Info: Caching catalog for stapp03.stratos.xfusioncorp.com
Info: Applying configuration version '1650526391'
Notice: /Stage[main]/Symlink/File[/opt/security]/ensure: created
Notice: /Stage[main]/Symlink/File[/opt/security/blog.txt]/ensure: created
Notice: Applied catalog in 0.04 seconds
[root@stapp03 ~]# ls -ahl /var/www/html
total 8.0K
drwxr-xr-x 2 root root 4.0K Apr 21 07:33 .
drwxr-xr-x 3 root root 4.0K Apr 21 07:25 ..
-rw-r--r-- 1 root root    0 Apr 21 07:33 blog.txt
[root@stapp03 ~]# 
[root@stapp03 ~]# ls -ahl /opt/security/
total 8.0K
drwxr-xr-x 2 root root 4.0K Apr 21 07:33 .
drwxr-xr-x 3 root root 4.0K Apr 21 07:25 ..
-rw-r--r-- 1 root root    0 Apr 21 07:33 blog.txt
[root@stapp03 ~]# ll /opt/security/
total 0
-rw-r--r-- 1 root root 0 Apr 21 07:33 blog.txt
[root@stapp03 ~]# 
[root@stapp03 ~]# 
--------------------------------------------------------------------------------------------------------------------------------
Task 54: 24/Apr/2022
Install Docker Package
Last week the Nautilus DevOps team met with the application development team and decided to containerize several of their applications. The DevOps team wants to do some testing per the following:

    Install docker-ce and docker-compose packages on App Server 3.

    Start docker service.

1. Login to App Server 3 and switch to root
 ssh banner@stapp03

 sudo su -

2. Fetch docker compose executable
# curl -L "https://github.com/docker/compose/releases/download/1.28.6/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
	  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
		                         Dload  Upload   Total   Spent    Left  Speed
	100   664  100   664    0     0   1646      0 --:--:-- --:--:-- --:--:--  1643
	100 11.6M  100 11.6M    0     0  12.2M      0 --:--:-- --:--:-- --:--:-- 12.2M

3. Provide executable permission to docker-compose 
# ls -ahl /usr/local/bin/docker-compose
	-rw-r--r-- 1 root root 12M Apr 24 05:37 /usr/local/bin/docker-compose

# chmod +x /usr/local/bin/docker-compose 

# ls -ahl /usr/local/bin/docker-compose
	-rwxr-xr-x 1 root root 12M Apr 24 05:37 /usr/local/bin/docker-compose

4. Validate instllation
# docker-compose --version
	docker-compose version 1.28.6, build 5db8d86f

5. Add docker repo for installation:
# yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
	Loaded plugins: fastestmirror, ovl
	adding repo from: https://download.docker.com/linux/centos/docker-ce.repo
	grabbing file https://download.docker.com/linux/centos/docker-ce.repo to /etc/yum.repos.d/docker-ce.repo
	repo saved to /etc/yum.repos.d/docker-ce.repo

6. Install docker
# yum install docker-ce docker-ce-cli containerd-io
	Loaded plugins: fastestmirror, ovl
	Determining fastest mirrors
	 * base: mirrors.gigenet.com
	 * extras: mirror.vacares.com
	 * updates: bay.uchicago.edu
	base                                                                                                                  | 3.6 kB  00:00:00     
	docker-ce-stable                                                                                                      | 3.5 kB  00:00:00     
	extras                                                                                                                | 2.9 kB  00:00:00     
	updates                                                                                                               | 2.9 kB  00:00:00     
	(1/6): base/7/x86_64/group_gz                                                                                         | 153 kB  00:00:00     
	(2/6): extras/7/x86_64/primary_db                                                                                     | 246 kB  00:00:00     
	(3/6): docker-ce-stable/7/x86_64/primary_db                                                                           |  75 kB  00:00:00     
	(4/6): docker-ce-stable/7/x86_64/updateinfo                                                                           |   55 B  00:00:00     
	(5/6): base/7/x86_64/primary_db                                                                                       | 6.1 MB  00:00:02     
	(6/6): updates/7/x86_64/primary_db                                                                                    |  15 MB  00:00:04     
	No package containerd-io available.
	Resolving Dependencies
	--> Running transaction check
	---> Package docker-ce.x86_64 3:20.10.14-3.el7 will be installed
	--> Processing Dependency: container-selinux >= 2:2.74 for package: 3:docker-ce-20.10.14-3.el7.x86_64
	--> Processing Dependency: containerd.io >= 1.4.1 for package: 3:docker-ce-20.10.14-3.el7.x86_64
	--> Processing Dependency: libseccomp >= 2.3 for package: 3:docker-ce-20.10.14-3.el7.x86_64
	--> Processing Dependency: docker-ce-rootless-extras for package: 3:docker-ce-20.10.14-3.el7.x86_64
	--> Processing Dependency: libcgroup for package: 3:docker-ce-20.10.14-3.el7.x86_64
	---> Package docker-ce-cli.x86_64 1:20.10.14-3.el7 will be installed
	--> Processing Dependency: docker-scan-plugin(x86-64) for package: 1:docker-ce-cli-20.10.14-3.el7.x86_64
	--> Running transaction check
	---> Package container-selinux.noarch 2:2.119.2-1.911c772.el7_8 will be installed
	--> Processing Dependency: selinux-policy-targeted >= 3.13.1-216.el7 for package: 2:container-selinux-2.119.2-1.911c772.el7_8.noarch
	--> Processing Dependency: selinux-policy-base >= 3.13.1-216.el7 for package: 2:container-selinux-2.119.2-1.911c772.el7_8.noarch
	--> Processing Dependency: selinux-policy >= 3.13.1-216.el7 for package: 2:container-selinux-2.119.2-1.911c772.el7_8.noarch
	--> Processing Dependency: policycoreutils >= 2.5-11 for package: 2:container-selinux-2.119.2-1.911c772.el7_8.noarch
	--> Processing Dependency: policycoreutils-python for package: 2:container-selinux-2.119.2-1.911c772.el7_8.noarch
	--> Processing Dependency: libselinux-utils for package: 2:container-selinux-2.119.2-1.911c772.el7_8.noarch
	---> Package containerd.io.x86_64 0:1.5.11-3.1.el7 will be installed
	---> Package docker-ce-rootless-extras.x86_64 0:20.10.14-3.el7 will be installed
	--> Processing Dependency: fuse-overlayfs >= 0.7 for package: docker-ce-rootless-extras-20.10.14-3.el7.x86_64
	--> Processing Dependency: slirp4netns >= 0.4 for package: docker-ce-rootless-extras-20.10.14-3.el7.x86_64
	---> Package docker-scan-plugin.x86_64 0:0.17.0-3.el7 will be installed
	---> Package libcgroup.x86_64 0:0.41-21.el7 will be installed
	---> Package libseccomp.x86_64 0:2.3.1-4.el7 will be installed
	--> Running transaction check
	---> Package fuse-overlayfs.x86_64 0:0.7.2-6.el7_8 will be installed
	--> Processing Dependency: libfuse3.so.3(FUSE_3.2)(64bit) for package: fuse-overlayfs-0.7.2-6.el7_8.x86_64
	--> Processing Dependency: libfuse3.so.3(FUSE_3.0)(64bit) for package: fuse-overlayfs-0.7.2-6.el7_8.x86_64
	--> Processing Dependency: libfuse3.so.3()(64bit) for package: fuse-overlayfs-0.7.2-6.el7_8.x86_64
	---> Package libselinux-utils.x86_64 0:2.5-15.el7 will be installed
	--> Processing Dependency: libselinux(x86-64) = 2.5-15.el7 for package: libselinux-utils-2.5-15.el7.x86_64
	---> Package policycoreutils.x86_64 0:2.5-34.el7 will be installed
	---> Package policycoreutils-python.x86_64 0:2.5-34.el7 will be installed
	--> Processing Dependency: setools-libs >= 3.3.8-4 for package: policycoreutils-python-2.5-34.el7.x86_64
	--> Processing Dependency: libsemanage-python >= 2.5-14 for package: policycoreutils-python-2.5-34.el7.x86_64
	--> Processing Dependency: audit-libs-python >= 2.1.3-4 for package: policycoreutils-python-2.5-34.el7.x86_64
	--> Processing Dependency: python-IPy for package: policycoreutils-python-2.5-34.el7.x86_64
	--> Processing Dependency: libselinux-python for package: policycoreutils-python-2.5-34.el7.x86_64
	--> Processing Dependency: libqpol.so.1(VERS_1.4)(64bit) for package: policycoreutils-python-2.5-34.el7.x86_64
	--> Processing Dependency: libqpol.so.1(VERS_1.2)(64bit) for package: policycoreutils-python-2.5-34.el7.x86_64
	--> Processing Dependency: libapol.so.4(VERS_4.0)(64bit) for package: policycoreutils-python-2.5-34.el7.x86_64
	--> Processing Dependency: checkpolicy for package: policycoreutils-python-2.5-34.el7.x86_64
	--> Processing Dependency: libqpol.so.1()(64bit) for package: policycoreutils-python-2.5-34.el7.x86_64
	--> Processing Dependency: libapol.so.4()(64bit) for package: policycoreutils-python-2.5-34.el7.x86_64
	---> Package selinux-policy.noarch 0:3.13.1-268.el7_9.2 will be installed
	---> Package selinux-policy-targeted.noarch 0:3.13.1-268.el7_9.2 will be installed
	---> Package slirp4netns.x86_64 0:0.4.3-4.el7_8 will be installed
	--> Running transaction check
	---> Package audit-libs-python.x86_64 0:2.8.5-4.el7 will be installed
	--> Processing Dependency: audit-libs(x86-64) = 2.8.5-4.el7 for package: audit-libs-python-2.8.5-4.el7.x86_64
	---> Package checkpolicy.x86_64 0:2.5-8.el7 will be installed
	---> Package fuse3-libs.x86_64 0:3.6.1-4.el7 will be installed
	---> Package libselinux.x86_64 0:2.5-14.1.el7 will be updated
	---> Package libselinux.x86_64 0:2.5-15.el7 will be an update
	---> Package libselinux-python.x86_64 0:2.5-15.el7 will be installed
	---> Package libsemanage-python.x86_64 0:2.5-14.el7 will be installed
	---> Package python-IPy.noarch 0:0.75-6.el7 will be installed
	---> Package setools-libs.x86_64 0:3.3.8-4.el7 will be installed
	--> Running transaction check
	---> Package audit-libs.x86_64 0:2.8.4-4.el7 will be updated
	---> Package audit-libs.x86_64 0:2.8.5-4.el7 will be an update
	--> Finished Dependency Resolution

	Dependencies Resolved

	=============================================================================================================================================
	 Package                                 Arch                 Version                                   Repository                      Size
	=============================================================================================================================================
	Installing:
	 docker-ce                               x86_64               3:20.10.14-3.el7                          docker-ce-stable                22 M
	 docker-ce-cli                           x86_64               1:20.10.14-3.el7                          docker-ce-stable                30 M
	Installing for dependencies:
	 audit-libs-python                       x86_64               2.8.5-4.el7                               base                            76 k
	 checkpolicy                             x86_64               2.5-8.el7                                 base                           295 k
	 container-selinux                       noarch               2:2.119.2-1.911c772.el7_8                 extras                          40 k
	 containerd.io                           x86_64               1.5.11-3.1.el7                            docker-ce-stable                29 M
	 docker-ce-rootless-extras               x86_64               20.10.14-3.el7                            docker-ce-stable               8.1 M
	 docker-scan-plugin                      x86_64               0.17.0-3.el7                              docker-ce-stable               3.7 M
	 fuse-overlayfs                          x86_64               0.7.2-6.el7_8                             extras                          54 k
	 fuse3-libs                              x86_64               3.6.1-4.el7                               extras                          82 k
	 libcgroup                               x86_64               0.41-21.el7                               base                            66 k
	 libseccomp                              x86_64               2.3.1-4.el7                               base                            56 k
	 libselinux-python                       x86_64               2.5-15.el7                                base                           236 k
	 libselinux-utils                        x86_64               2.5-15.el7                                base                           151 k
	 libsemanage-python                      x86_64               2.5-14.el7                                base                           113 k
	 policycoreutils                         x86_64               2.5-34.el7                                base                           917 k
	 policycoreutils-python                  x86_64               2.5-34.el7                                base                           457 k
	 python-IPy                              noarch               0.75-6.el7                                base                            32 k
	 selinux-policy                          noarch               3.13.1-268.el7_9.2                        updates                        498 k
	 selinux-policy-targeted                 noarch               3.13.1-268.el7_9.2                        updates                        7.0 M
	 setools-libs                            x86_64               3.3.8-4.el7                               base                           620 k
	 slirp4netns                             x86_64               0.4.3-4.el7_8                             extras                          81 k
	Updating for dependencies:
	 audit-libs                              x86_64               2.8.5-4.el7                               base                           102 k
	 libselinux                              x86_64               2.5-15.el7                                base                           162 k

	Transaction Summary
	=============================================================================================================================================
	Install  2 Packages (+20 Dependent packages)
	Upgrade             (  2 Dependent packages)

	Total download size: 104 M
	Is this ok [y/d/N]: y
	Downloading packages:
	Delta RPMs disabled because /usr/bin/applydeltarpm not installed.
	(1/24): audit-libs-python-2.8.5-4.el7.x86_64.rpm                                                                      |  76 kB  00:00:00     
	(2/24): audit-libs-2.8.5-4.el7.x86_64.rpm                                                                             | 102 kB  00:00:00     
	(3/24): checkpolicy-2.5-8.el7.x86_64.rpm                                                                              | 295 kB  00:00:00     
	(4/24): container-selinux-2.119.2-1.911c772.el7_8.noarch.rpm                                                          |  40 kB  00:00:00     
	warning: /var/cache/yum/x86_64/7/docker-ce-stable/packages/docker-ce-20.10.14-3.el7.x86_64.rpm: Header V4 RSA/SHA512 Signature, key ID 621e9f35: NOKEY
	Public key for docker-ce-20.10.14-3.el7.x86_64.rpm is not installed
	(5/24): docker-ce-20.10.14-3.el7.x86_64.rpm                                                                           |  22 MB  00:00:00     
	(6/24): containerd.io-1.5.11-3.1.el7.x86_64.rpm                                                                       |  29 MB  00:00:00     
	(7/24): docker-ce-rootless-extras-20.10.14-3.el7.x86_64.rpm                                                           | 8.1 MB  00:00:00     
	(8/24): docker-ce-cli-20.10.14-3.el7.x86_64.rpm                                                                       |  30 MB  00:00:00     
	(9/24): docker-scan-plugin-0.17.0-3.el7.x86_64.rpm                                                                    | 3.7 MB  00:00:00     
	(10/24): fuse-overlayfs-0.7.2-6.el7_8.x86_64.rpm                                                                      |  54 kB  00:00:00     
	(11/24): libcgroup-0.41-21.el7.x86_64.rpm                                                                             |  66 kB  00:00:00     
	(12/24): libselinux-2.5-15.el7.x86_64.rpm                                                                             | 162 kB  00:00:00     
	(13/24): fuse3-libs-3.6.1-4.el7.x86_64.rpm                                                                            |  82 kB  00:00:00     
	(14/24): libselinux-python-2.5-15.el7.x86_64.rpm                                                                      | 236 kB  00:00:00     
	(15/24): libselinux-utils-2.5-15.el7.x86_64.rpm                                                                       | 151 kB  00:00:00     
	(16/24): libseccomp-2.3.1-4.el7.x86_64.rpm                                                                            |  56 kB  00:00:00     
	(17/24): libsemanage-python-2.5-14.el7.x86_64.rpm                                                                     | 113 kB  00:00:00     
	(18/24): policycoreutils-python-2.5-34.el7.x86_64.rpm                                                                 | 457 kB  00:00:00     
	(19/24): python-IPy-0.75-6.el7.noarch.rpm                                                                             |  32 kB  00:00:00     
	(20/24): setools-libs-3.3.8-4.el7.x86_64.rpm                                                                          | 620 kB  00:00:00     
	(21/24): policycoreutils-2.5-34.el7.x86_64.rpm                                                                        | 917 kB  00:00:00     
	(22/24): slirp4netns-0.4.3-4.el7_8.x86_64.rpm                                                                         |  81 kB  00:00:00     
	(23/24): selinux-policy-3.13.1-268.el7_9.2.noarch.rpm                                                                 | 498 kB  00:00:00     
	(24/24): selinux-policy-targeted-3.13.1-268.el7_9.2.noarch.rpm                                                        | 7.0 MB  00:00:02     
	---------------------------------------------------------------------------------------------------------------------------------------------
	Total                                                                                                         23 MB/s | 104 MB  00:00:04     
	Retrieving key from https://download.docker.com/linux/centos/gpg
	Importing GPG key 0x621E9F35:
	 Userid     : "Docker Release (CE rpm) <docker@docker.com>"
	 Fingerprint: 060a 61c5 1b55 8a7f 742b 77aa c52f eb6b 621e 9f35
	 From       : https://download.docker.com/linux/centos/gpg
	Is this ok [y/N]: y
	Running transaction check
	Running transaction test
	Transaction test succeeded
	Running transaction
	  Updating   : libselinux-2.5-15.el7.x86_64                                                                                             1/26 
	  Installing : libseccomp-2.3.1-4.el7.x86_64                                                                                            2/26 
	  Installing : libselinux-utils-2.5-15.el7.x86_64                                                                                       3/26 
	  Installing : libcgroup-0.41-21.el7.x86_64                                                                                             4/26 
	  Updating   : audit-libs-2.8.5-4.el7.x86_64                                                                                            5/26 
	  Installing : policycoreutils-2.5-34.el7.x86_64                                                                                        6/26 
	  Installing : selinux-policy-3.13.1-268.el7_9.2.noarch                                                                                 7/26 
	  Installing : selinux-policy-targeted-3.13.1-268.el7_9.2.noarch                                                                        8/26 
	  Installing : audit-libs-python-2.8.5-4.el7.x86_64                                                                                     9/26 
	  Installing : slirp4netns-0.4.3-4.el7_8.x86_64                                                                                        10/26 
	  Installing : setools-libs-3.3.8-4.el7.x86_64                                                                                         11/26 
	  Installing : libselinux-python-2.5-15.el7.x86_64                                                                                     12/26 
	  Installing : 1:docker-ce-cli-20.10.14-3.el7.x86_64                                                                                   13/26 
	  Installing : docker-scan-plugin-0.17.0-3.el7.x86_64                                                                                  14/26 
	  Installing : libsemanage-python-2.5-14.el7.x86_64                                                                                    15/26 
	  Installing : fuse3-libs-3.6.1-4.el7.x86_64                                                                                           16/26 
	  Installing : fuse-overlayfs-0.7.2-6.el7_8.x86_64                                                                                     17/26 
	  Installing : python-IPy-0.75-6.el7.noarch                                                                                            18/26 
	  Installing : checkpolicy-2.5-8.el7.x86_64                                                                                            19/26 
	  Installing : policycoreutils-python-2.5-34.el7.x86_64                                                                                20/26 
	  Installing : 2:container-selinux-2.119.2-1.911c772.el7_8.noarch                                                                      21/26 
	setsebool:  SELinux is disabled.
	  Installing : containerd.io-1.5.11-3.1.el7.x86_64                                                                                     22/26 
	  Installing : docker-ce-rootless-extras-20.10.14-3.el7.x86_64                                                                         23/26 
	  Installing : 3:docker-ce-20.10.14-3.el7.x86_64                                                                                       24/26 
	  Cleanup    : audit-libs-2.8.4-4.el7.x86_64                                                                                           25/26 
	  Cleanup    : libselinux-2.5-14.1.el7.x86_64                                                                                          26/26 
	  Verifying  : containerd.io-1.5.11-3.1.el7.x86_64                                                                                      1/26 
	  Verifying  : fuse-overlayfs-0.7.2-6.el7_8.x86_64                                                                                      2/26 
	  Verifying  : libselinux-2.5-15.el7.x86_64                                                                                             3/26 
	  Verifying  : docker-ce-rootless-extras-20.10.14-3.el7.x86_64                                                                          4/26 
	  Verifying  : 2:container-selinux-2.119.2-1.911c772.el7_8.noarch                                                                       5/26 
	  Verifying  : selinux-policy-targeted-3.13.1-268.el7_9.2.noarch                                                                        6/26 
	  Verifying  : audit-libs-2.8.5-4.el7.x86_64                                                                                            7/26 
	  Verifying  : checkpolicy-2.5-8.el7.x86_64                                                                                             8/26 
	  Verifying  : policycoreutils-2.5-34.el7.x86_64                                                                                        9/26 
	  Verifying  : python-IPy-0.75-6.el7.noarch                                                                                            10/26 
	  Verifying  : libseccomp-2.3.1-4.el7.x86_64                                                                                           11/26 
	  Verifying  : libselinux-utils-2.5-15.el7.x86_64                                                                                      12/26 
	  Verifying  : policycoreutils-python-2.5-34.el7.x86_64                                                                                13/26 
	  Verifying  : docker-scan-plugin-0.17.0-3.el7.x86_64                                                                                  14/26 
	  Verifying  : setools-libs-3.3.8-4.el7.x86_64                                                                                         15/26 
	  Verifying  : 3:docker-ce-20.10.14-3.el7.x86_64                                                                                       16/26 
	  Verifying  : fuse3-libs-3.6.1-4.el7.x86_64                                                                                           17/26 
	  Verifying  : libsemanage-python-2.5-14.el7.x86_64                                                                                    18/26 
	  Verifying  : slirp4netns-0.4.3-4.el7_8.x86_64                                                                                        19/26 
	  Verifying  : libselinux-python-2.5-15.el7.x86_64                                                                                     20/26 
	  Verifying  : selinux-policy-3.13.1-268.el7_9.2.noarch                                                                                21/26 
	  Verifying  : audit-libs-python-2.8.5-4.el7.x86_64                                                                                    22/26 
[I	  Verifying  : 1:docker-ce-cli-20.10.14-3.el7.x86_64                                                                                   23/26 
	  Verifying  : libcgroup-0.41-21.el7.x86_64                                                                                            24/26 
	  Verifying  : audit-libs-2.8.4-4.el7.x86_64                                                                                           25/26 
	  Verifying  : libselinux-2.5-14.1.el7.x86_64                                                                                          26/26 

	Installed:
	  docker-ce.x86_64 3:20.10.14-3.el7                                   docker-ce-cli.x86_64 1:20.10.14-3.el7                                  

	Dependency Installed:
	  audit-libs-python.x86_64 0:2.8.5-4.el7                               checkpolicy.x86_64 0:2.5-8.el7                                       
	  container-selinux.noarch 2:2.119.2-1.911c772.el7_8                   containerd.io.x86_64 0:1.5.11-3.1.el7                                
	  docker-ce-rootless-extras.x86_64 0:20.10.14-3.el7                    docker-scan-plugin.x86_64 0:0.17.0-3.el7                             
	  fuse-overlayfs.x86_64 0:0.7.2-6.el7_8                                fuse3-libs.x86_64 0:3.6.1-4.el7                                      
	  libcgroup.x86_64 0:0.41-21.el7                                       libseccomp.x86_64 0:2.3.1-4.el7                                      
	  libselinux-python.x86_64 0:2.5-15.el7                                libselinux-utils.x86_64 0:2.5-15.el7                                 
	  libsemanage-python.x86_64 0:2.5-14.el7                               policycoreutils.x86_64 0:2.5-34.el7                                  
	  policycoreutils-python.x86_64 0:2.5-34.el7                           python-IPy.noarch 0:0.75-6.el7                                       
	  selinux-policy.noarch 0:3.13.1-268.el7_9.2                           selinux-policy-targeted.noarch 0:3.13.1-268.el7_9.2                  
	  setools-libs.x86_64 0:3.3.8-4.el7                                    slirp4netns.x86_64 0:0.4.3-4.el7_8                                   

	Dependency Updated:
	  audit-libs.x86_64 0:2.8.5-4.el7                                       libselinux.x86_64 0:2.5-15.el7                                      

	Complete!

7. Validate installation
# rpm -qa |grep docker
	docker-scan-plugin-0.17.0-3.el7.x86_64
	docker-ce-20.10.14-3.el7.x86_64
	docker-ce-cli-20.10.14-3.el7.x86_64
	docker-ce-rootless-extras-20.10.14-3.el7.x86_64

8. Enable and run docker and check status
# systemctl enable docker
	Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.

# systemctl start docker

# systemctl status docker
	● docker.service - Docker Application Container Engine
	   Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled)
	   Active: active (running) since Sun 2022-04-24 05:42:51 UTC; 7s ago
	     Docs: https://docs.docker.com
	 Main PID: 1608 (dockerd)
	    Tasks: 28
	   Memory: 39.1M
	   CGroup: /docker/14a2eb37eda22233817fc68b10c29135f57b0a84c4d97c43bfbbef29352d2dc9/system.slice/docker.service
		   └─1608 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock

	Apr 24 05:42:50 stapp03.stratos.xfusioncorp.com dockerd[1608]: time="2022-04-24T05:42:50.860791328Z" level=info msg="Loading containe...one."
	Apr 24 05:42:50 stapp03.stratos.xfusioncorp.com dockerd[1608]: time="2022-04-24T05:42:50.936515661Z" level=info msg="Docker daemon" c...10.14
	Apr 24 05:42:50 stapp03.stratos.xfusioncorp.com dockerd[1608]: time="2022-04-24T05:42:50.936725295Z" level=info msg="Daemon has compl...tion"
	Apr 24 05:42:51 stapp03.stratos.xfusioncorp.com systemd[1]: Got notification message for unit docker.service
	Apr 24 05:42:51 stapp03.stratos.xfusioncorp.com systemd[1]: docker.service: Got notification message from PID 1608 (READY=1)
	Apr 24 05:42:51 stapp03.stratos.xfusioncorp.com systemd[1]: docker.service: got READY=1
	Apr 24 05:42:51 stapp03.stratos.xfusioncorp.com systemd[1]: docker.service changed start -> running
	Apr 24 05:42:51 stapp03.stratos.xfusioncorp.com systemd[1]: Job docker.service/start finished, result=done
	Apr 24 05:42:51 stapp03.stratos.xfusioncorp.com systemd[1]: Started Docker Application Container Engine.
	Apr 24 05:42:51 stapp03.stratos.xfusioncorp.com dockerd[1608]: time="2022-04-24T05:42:51.144159968Z" level=info msg="API listen on /v...sock"
	Hint: Some lines were ellipsized, use -l to show in full.

9. Validate the installations  
# docker --version
	Docker version 20.10.14, build a224086

# docker ps
	CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES

# docker-compose --version
	docker-compose version 1.28.6, build 5db8d86f

--------------------------------------------------------------------------------------------------------------------------------
Task 55 : 1/May/2022
Deploy Apache Web Server on Kubernetes CLuster

There is an application that needs to be deployed on Kubernetes cluster under Apache web server. The Nautilus application development team has asked the DevOps team to deploy it. We need to develop a template as per requirements mentioned below:

    Create a namespace named as httpd-namespace-devops.

    Create a deployment named as httpd-deployment-devops under newly created namespace. For the deployment use httpd image with latest tag only and remember to mention the tag i.e httpd:latest, and make sure replica counts are 2.

    Create a service named as httpd-service-devops under same namespace to expose the deployment, nodePort should be 30004.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.


thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get namespace
NAME                 STATUS   AGE
default              Active   61m
kube-node-lease      Active   61m
kube-public          Active   62m
kube-system          Active   62m
local-path-storage   Active   61m
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl create namespace httpd-namespace-devops
namespace/httpd-namespace-devops created
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get namespace
NAME                     STATUS   AGE
default                  Active   62m
httpd-namespace-devops   Active   4s
kube-node-lease          Active   62m
kube-public              Active   62m
kube-system              Active   62m
local-path-storage       Active   62m
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get pods -n  httpd-namespace-devops
No resources found in httpd-namespace-devops namespace.
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get service  -n  httpd-namespace-devops
No resources found in httpd-namespace-devops namespace.
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ vi /tmp/httpd.yaml
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ cat /tmp/httpd.yaml 
apiVersion: v1
kind: Service
metadata:
  name: httpd-service-devops
  namespace: httpd-namespace-devops
spec:
  type: NodePort
  selector:
    app: httpd_app_devops
  ports:
    - port: 80
      targetPort: 80
      nodePort: 30004
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: httpd-deployment-devops
  namespace: httpd-namespace-devops
  labels:
    app: httpd_app_devops
spec:
  replicas: 2
  selector:
    matchLabels:
      app: httpd_app_devops
  template:
    metadata:
      labels:
        app: httpd_app_devops
    spec:
      containers:
        - name: httpd-container-devops
          image: httpd:latest
          ports:
            - containerPort: 80
thor@jump_host ~$ kubectl create -f /tmp/httpd.yaml
service/httpd-service-devops created
deployment.apps/httpd-deployment-devops created
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get service  -n  httpd-namespace-devops
NAME                   TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
httpd-service-devops   NodePort   10.96.72.224   <none>        80:30004/TCP   40s
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get pods -n  httpd-namespace-devops
NAME                                      READY   STATUS    RESTARTS   AGE
httpd-deployment-devops-867b499f4-qdx2m   1/1     Running   0          47s
httpd-deployment-devops-867b499f4-sw7qc   1/1     Running   0          47s
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
--------------------------------------------------------------------------------------------------------------------------------
Task 56: 3/May/2022
Git Merge Branches
The Nautilus application development team has been working on a project repository /opt/beta.git. This repo is cloned at /usr/src/kodekloudrepos on storage server in Stratos DC. They recently shared the following requirements with DevOps team:

a. Create a new branch nautilus in /usr/src/kodekloudrepos/beta repo from master and copy the /tmp/index.html file (on storage server itself). Add/commit this file in the new branch and merge back that branch to the master branch. Finally, push the changes to origin for both of the branches.

thor@jump_host ~$ ssh natasha@ststor01
The authenticity of host 'ststor01 (172.16.238.15)' can't be established.
ECDSA key fingerprint is SHA256:AMgD8K2XIX3sOAvahEWXDs51RbPFOGSsgp8+UWIfXNc.
ECDSA key fingerprint is MD5:9b:83:09:6b:3b:a8:af:dc:8b:6a:b8:99:35:a0:8d:c0.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'ststor01,172.16.238.15' (ECDSA) to the list of known hosts.
natasha@ststor01's password: 
[natasha@ststor01 ~]$ 
[natasha@ststor01 ~]$ 
[natasha@ststor01 ~]$ sudo su - 

We trust you have received the usual lecture from the local System
Administrator. It usually boils down to these three things:

    #1) Respect the privacy of others.
    #2) Think before you type.
    #3) With great power comes great responsibility.

[sudo] password for natasha: 
[root@ststor01 ~]# 
[root@ststor01 ~]# 
[root@ststor01 ~]# 
[root@ststor01 ~]# cd /usr/src/kodekloudrepos/beta/
[root@ststor01 beta]# ls -ahl
total 20K
drwxr-xr-x 3 root root 4.0K May  3 03:44 .
drwxr-xr-x 3 root root 4.0K May  3 03:44 ..
drwxr-xr-x 8 root root 4.0K May  3 03:44 .git
-rw-r--r-- 1 root root   34 May  3 03:44 info.txt
-rw-r--r-- 1 root root   26 May  3 03:44 welcome.txt
[root@ststor01 beta]# git status
# On branch master
nothing to commit, working directory clean
[root@ststor01 beta]# 
[root@ststor01 beta]# git checkout -b nautilus
Switched to a new branch 'nautilus'
[root@ststor01 beta]# 
[root@ststor01 beta]# git status
# On branch nautilus
nothing to commit, working directory clean
[root@ststor01 beta]# 
[root@ststor01 beta]# 
[root@ststor01 beta]# git branch
  master
* nautilus
[root@ststor01 beta]# 
[root@ststor01 beta]# cp /tmp/index.html /usr/src/kodekloudrepos/beta/
[root@ststor01 beta]# 
[root@ststor01 beta]# 
[root@ststor01 beta]# ls -ahl
total 24K
drwxr-xr-x 3 root root 4.0K May  3 03:48 .
drwxr-xr-x 3 root root 4.0K May  3 03:44 ..
drwxr-xr-x 8 root root 4.0K May  3 03:47 .git
-rw-r--r-- 1 root root   27 May  3 03:48 index.html
-rw-r--r-- 1 root root   34 May  3 03:44 info.txt
-rw-r--r-- 1 root root   26 May  3 03:44 welcome.txt
[root@ststor01 beta]# git add index.html 
[root@ststor01 beta]# git commit -m "add beta"
[nautilus 71d74be] add beta
 1 file changed, 1 insertion(+)
 create mode 100644 index.html
[root@ststor01 beta]# 
[root@ststor01 beta]# git checkout master
Switched to branch 'master'
[root@ststor01 beta]# 
[root@ststor01 beta]# 
[root@ststor01 beta]# git merge nautilus
Updating 440bde1..71d74be
Fast-forward
 index.html | 1 +
 1 file changed, 1 insertion(+)
 create mode 100644 index.html
[root@ststor01 beta]# 
[root@ststor01 beta]# 
[root@ststor01 beta]# git push -u origin nautilus
Counting objects: 4, done.
Delta compression using up to 36 threads.
Compressing objects: 100% (2/2), done.
Writing objects: 100% (3/3), 325 bytes | 0 bytes/s, done.
Total 3 (delta 0), reused 0 (delta 0)
To /opt/beta.git
 * [new branch]      nautilus -> nautilus
Branch nautilus set up to track remote branch nautilus from origin.
[root@ststor01 beta]# 
[root@ststor01 beta]# git push -u origin master
Total 0 (delta 0), reused 0 (delta 0)
To /opt/beta.git
   440bde1..71d74be  master -> master
Branch master set up to track remote branch master from origin.
[root@ststor01 beta]# 
[root@ststor01 beta]# 
[root@ststor01 beta]# git status
# On branch master
nothing to commit, working directory clean

--------------------------------------------------------------------------------------------------------------------------------
Task 57: 17/May/2022

Countdown job in Kubernetes
The Nautilus DevOps team is working on to create few jobs in Kubernetes cluster. They might come up with some real scripts/commands to use, but for now they are preparing the templates and testing the jobs with dummy commands. Please create a job template as per details given below:

    Create a job countdown-xfusion.

    The spec template should be named as countdown-xfusion (under metadata), and the container should be named as container-countdown-xfusion

    Use image fedora with latest tag only and remember to mention tag i.e fedora:latest, and restart policy should be Never.

    Use command for i in 10 9 8 7 6 5 4 3 2 1 ; do echo $i ; done

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.



thor@jump_host ~$ kubectl get services
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   87m
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get pods
No resources found in default namespace.

 
thor@jump_host ~$ vi /tmp/countdown.yaml
		apiVersion: batch/v1
		kind: Job
		metadata:
		  name: countdown-xfusion
		spec:
		  template:
		    metadata:
		      name: countdown-xfusion
		    spec:
		      containers:
		        - name: container-countdown-xfusion
		          image: fedora:latest
		          command: ["/bin/sh", "-c"]
		          args:
		            [
		              "for i in 10 9 8 7 6 5 4 3 2 1 ; do echo $i ; done",
		            ]
		      restartPolicy: Never 


thor@jump_host ~$ kubectl create -f /tmp/countdown.yaml 
job.batch/countdown-xfusion created
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get pods
NAME                      READY   STATUS      RESTARTS   AGE
countdown-xfusion-xtgxv   0/1     Completed   0          14s
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get pods
NAME                      READY   STATUS      RESTARTS   AGE
countdown-xfusion-xtgxv   0/1     Completed   0          20s
thor@jump_host ~$ kubectl get logs countdown-xfusion-xtgxv
error: the server doesn't have a resource type "logs"
thor@jump_host ~$ kubectl get log countdown-xfusion-xtgxv
error: the server doesn't have a resource type "log"
thor@jump_host ~$ kubectl logs countdown-xfusion-xtgxv
10
9
8
7
6
5
4
3
2
1
thor@jump_host ~$

--------------------------------------------------------------------------------------------------------------------------------
Task 58:

Rolling Updates And Rolling Back Deployments in Kubernetes

--------------------------------------------------------------------------------------------------------------------------------
Task 59: 19/May/2022
Ansible Ping Module Usage

The Nautilus DevOps team is planning to test several Ansible playbooks on different app servers in Stratos DC. Before that, some pre-requisites must be met. Essentially, the team needs to set up a password-less SSH connection between Ansible controller and Ansible managed nodes. One of the tickets is assigned to you; please complete the task as per details mentioned below:

a. Jump host is our Ansible controller, and we are going to run Ansible playbooks through thor user on jump host.

b.Make appropriate changes on jump host so that user thor on jump host can SSH into App Server 1 through its respective sudo user. (for example tony for app server 1).

c. There is an inventory file /home/thor/ansible/inventory on jump host. Using that inventory file test Ansible ping from jump host to App Server 1, make sure ping works.

1. check inventory file on jump_host
	thor@jump_host ~$ cd /home/thor/ansible/
	thor@jump_host ~/ansible$ ls -ahl
		total 12K
		drwxr-xr-x 2 thor thor 4.0K May 19 07:27 .
		drwxr----- 1 thor thor 4.0K May 19 07:27 ..
		-rw-r--r-- 1 thor thor  237 May 19 07:27 inventory
	thor@jump_host ~/ansible$ cat inventory 
		stapp01 ansible_host=172.16.238.10 ansible_ssh_pass=Ir0nM@n ansible_user=tony
		stapp02 ansible_host=172.16.238.11 ansible_ssh_pass=Am3ric@ ansible_user=steve
		stapp03 ansible_host=172.16.238.12 ansible_ssh_pass=BigGr33n ansible_user=banner


2. Generate ssh key for thor user		
	thor@jump_host ~/ansible$ ssh-keygen -t rsa -b 2048
			Generating public/private rsa key pair.
			Enter file in which to save the key (/home/thor/.ssh/id_rsa): 
			Enter passphrase (empty for no passphrase): 
			Enter same passphrase again: 
			Your identification has been saved in /home/thor/.ssh/id_rsa.
			Your public key has been saved in /home/thor/.ssh/id_rsa.pub.
			The key fingerprint is:
			SHA256:NzmC45AvaXU/9ysD+BFkpyScRmPmFCGR3249dJRIhcs thor@jump_host.stratos.xfusioncorp.com
			The key's randomart image is:
			+---[RSA 2048]----+
			|      o=O+ ..+.. |
			|      .*=.+ + o  |
			|       o.* + o   |
			|     . .. +.E .  |
			|    o + So=+ .   |
			|     * o.+=oo    |
			|    + o  ooo..   |
			|   . .    .oo.   |
			|             oo. |
			+----[SHA256]-----+

3. Copy ssh key to stapp01
	thor@jump_host ~/ansible$ ssh-copy-id tony@stapp01
			/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/home/thor/.ssh/id_rsa.pub"
			The authenticity of host 'stapp01 (172.16.238.10)' can't be established.
			ECDSA key fingerprint is SHA256:/0nejVh3XjAevkSdnZs2/CE2zqcM0ewv/P7fZsa1PW0.
			ECDSA key fingerprint is MD5:bc:71:bc:a7:af:cb:94:31:61:9c:f4:c2:a6:8c:6d:41.
			Are you sure you want to continue connecting (yes/no)? yes
			/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
			/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
			tony@stapp01's password: 

			Number of key(s) added: 1

			Now try logging into the machine, with:   "ssh 'tony@stapp01'"
			and check to make sure that only the key(s) you wanted were added.

4. Verify ping command using ansible on stapp01
	thor@jump_host ~/ansible$ ansible stapp01 -m ping -i inventory -v
		Using /etc/ansible/ansible.cfg as config file
		stapp01 | SUCCESS => {
		    "ansible_facts": {
		        "discovered_interpreter_python": "/usr/bin/python"
		    }, 
		    "changed": false, 
		    "ping": "pong"
		}
--------------------------------------------------------------------------------------------------------------------------------
Task 60: 20/May/2022
Docker Ports Mapping

The Nautilus DevOps team is planning to host an application on a nginx-based container. There are number of tickets already been created for similar tasks. One of the tickets has been assigned to set up a nginx container on Application Server 1 in Stratos Datacenter. Please perform the task as per details mentioned below:

a. Pull nginx:alpine-perl docker image on Application Server 1.

b. Create a container named blog using the image you pulled.

c. Map host port 5002 to container port 80. Please keep the container in running state.


thor@jump_host ~$ ssh tony@stapp01
		The authenticity of host 'stapp01 (172.16.238.10)' can't be established.
		ECDSA key fingerprint is SHA256:2U4MljMQTrcY/I3RlI2CxvvzdmhD0nAnVlhmv+swf2Q.
		ECDSA key fingerprint is MD5:5a:15:c9:96:07:43:d7:16:1a:18:b2:5d:55:f2:c4:5e.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'stapp01,172.16.238.10' (ECDSA) to the list of known hosts.
		tony@stapp01's password: 
 
[tony@stapp01 ~]$ sudo su - 

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for tony: 
[root@stapp01 ~]# 

[root@stapp01 ~]# docker ps
		CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES

[root@stapp01 ~]# docker images
		REPOSITORY   TAG       IMAGE ID   CREATED   SIZE

[root@stapp01 ~]# docker pull nginx:alpine-perl
		alpine-perl: Pulling from library/nginx
		df9b9388f04a: Pull complete 
		fb94416861d8: Pull complete 
		cc9b6bc3348b: Pull complete 
		a2a262a4bae0: Pull complete 
		9a73c45724b0: Pull complete 
		69c0568aaf5b: Pull complete 
		Digest: sha256:b446bc9ed53d02c8a769b52bbdc35414e8555188559e8be143732b10d3b21b8e
		Status: Downloaded newer image for nginx:alpine-perl
		docker.io/library/nginx:alpine-perl


[root@stapp01 ~]# docker images
		REPOSITORY   TAG           IMAGE ID       CREATED      SIZE
		nginx        alpine-perl   c0800a068427   2 days ago   58.4MB



[root@stapp01 ~]# docker container run -d --name blog -p 5002:80 nginx:alpine-perl
		b894ab1eef89fe6711927203135074b90e6b08c02f294bf89f615c285fc380b8



[root@stapp01 ~]# docker ps
		CONTAINER ID   IMAGE               COMMAND                  CREATED          STATUS          PORTS                  NAMES
		b894ab1eef89   nginx:alpine-perl   "/docker-entrypoint.…"   16 seconds ago   Up 12 seconds   0.0.0.0:5002->80/tcp   blog


[root@stapp01 ~]# curl http://localhost:5002/
		<!DOCTYPE html>
		<html>
		<head>
		<title>Welcome to nginx!</title>
		<style>
		html { color-scheme: light dark; }
		body { width: 35em; margin: 0 auto;
		font-family: Tahoma, Verdana, Arial, sans-serif; }
		</style>
		</head>
		<body>
		<h1>Welcome to nginx!</h1>
		<p>If you see this page, the nginx web server is successfully installed and
		working. Further configuration is required.</p>

		<p>For online documentation and support please refer to
		<a href="http://nginx.org/">nginx.org</a>.<br/>
		Commercial support is available at
		<a href="http://nginx.com/">nginx.com</a>.</p>

		<p><em>Thank you for using nginx.</em></p>
		</body>
		</html>
[root@stapp01 ~]#

--------------------------------------------------------------------------------------------------------------------------------
Task 61: 25/May/2022

Deploy Nginx Web Server on Kubernetes Cluster

Some of the Nautilus team developers are developing a static website and they want to deploy it on Kubernetes cluster. They want it to be highly available and scalable. Therefore, based on the requirements, the DevOps team has decided to create a deployment for it with multiple replicas. Below you can find more details about it:

    Create a deployment using nginx image with latest tag only and remember to mention the tag i.e nginx:latest. Name it as nginx-deployment. The container should be named as nginx-container, also make sure replica counts are 3.

    Create a NodePort type service named nginx-service. The nodePort should be 30011.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

thor@jump_host ~$ kubectl get namespace
NAME                 STATUS   AGE
default              Active   94m
kube-node-lease      Active   94m
kube-public          Active   94m
kube-system          Active   94m
local-path-storage   Active   93m
thor@jump_host ~$ kubectl get pods
No resources found in default namespace.
thor@jump_host ~$ 
thor@jump_host ~$ vi /tmp/nginx.yml
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ cat /tmp/nginx.yml 
apiVersion: v1

kind: Service

metadata:

  name: nginx-service

spec:

  type: NodePort

  selector:

    app: nginx-app

    type: front-end

  ports:

    - port: 80

      targetPort: 80

      nodePort: 30011

---

apiVersion: apps/v1

kind: Deployment

metadata:

  name: nginx-deployment

  labels:

    app: nginx-app

    type: front-end

spec:

  replicas: 3

  selector:

    matchLabels:

      app: nginx-app

      type: front-end

  template:

    metadata:

      labels:

        app: nginx-app

        type: front-end

    spec:

      containers:

        - name: nginx-container

          image: nginx:latest 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl create -f /tmp/nginx.yml
service/nginx-service created
deployment.apps/nginx-deployment created
thor@jump_host ~$ kubectl get pods
NAME                                READY   STATUS    RESTARTS   AGE
nginx-deployment-56cbd5d774-6v5jb   1/1     Running   0          20s
nginx-deployment-56cbd5d774-vm8kr   1/1     Running   0          20s
nginx-deployment-56cbd5d774-ww7jn   1/1     Running   0          20s
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl exec nginx-deployment-56cbd5d774-6v5jb --curl http://localhost
Error: unknown flag: --curl
See 'kubectl exec --help' for usage.
thor@jump_host ~$ kubectl exec nginx-deployment-56cbd5d774-6v5jb -- curl http://localhost
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
100   615  100   615    0     0   600k      0 --:--:-- --:--:-- --:--:--  600k
thor@jump_host ~$ 

--------------------------------------------------------------------------------------------------------------------------------
Task 62: 26/May/2022
Ansible Basic Playbook

One of the Nautilus DevOps team members was working on to test an Ansible playbook on jump host. However, he was only able to create the inventory, and due to other priorities that came in he has to work on other tasks. Please pick up this task from where he left off and complete it. Below are more details about the task:

    The inventory file /home/thor/ansible/inventory seems to be having some issues, please fix them. The playbook needs to be run on App Server 2 in Stratos DC, so inventory file needs to be updated accordingly.

    Create a playbook /home/thor/ansible/playbook.yml and add a task to create an empty file /tmp/file.txt on App Server 2.

Note: Validation will try to run the playbook using command ansible-playbook -i inventory playbook.yml so please make sure the playbook works this way without passing any extra arguments.

thor@jump_host ~$ vi /home/thor/ansible/inventory 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ cat  /home/thor/ansible/inventory 
stapp02 ansible_host=172.16.238.11 ansible_user=steve ansible_ssh_pass=Am3ric@
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ vi /home/thor/ansible/plabook.yml
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ cat /home/thor/ansible/plabook.yml
- name: Create file in appserver

  hosts: stapp02

  become: yes

  tasks:

    - name: Create the file

      file:

        path: /tmp/file.txt

        state: touch
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ ansible all -a "ls -ltr /tmp/" -i inventory
[WARNING]: Unable to parse /home/thor/inventory as an inventory source
[WARNING]: No inventory was parsed, only implicit localhost is available
[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'
thor@jump_host ~$ 
thor@jump_host ~$ ansible all -a "ls -ltr /tmp/" -i ansible/inventory
stapp02 | CHANGED | rc=0 >>
total 8
-rw------- 1 root  root     0 Aug  1  2019 yum.log
-rwx------ 1 root  root   836 Aug  1  2019 ks-script-rnBCJB
drwx------ 2 steve steve 4096 May 26 13:20 ansible_command_payload_Jn4wIF
thor@jump_host ~$ 
thor@jump_host ~$ ansible-playbook - i inventory play^C
thor@jump_host ~$ cd ansible/
thor@jump_host ~/ansible$ ansible-playbook -i inventory playbook^C
thor@jump_host ~/ansible$ ls -ahl
total 16K
drwxr-xr-x 2 thor thor 4.0K May 26 13:19 .
drwxr----- 1 thor thor 4.0K May 26 13:19 ..
-rw-r--r-- 1 thor thor   79 May 26 13:18 inventory
-rw-rw-r-- 1 thor thor  169 May 26 13:19 plabook.yml
thor@jump_host ~/ansible$ mv plabook.yml playbook.yml
thor@jump_host ~/ansible$ ls -ahl
total 16K
drwxr-xr-x 2 thor thor 4.0K May 26 13:21 .
drwxr----- 1 thor thor 4.0K May 26 13:19 ..
-rw-r--r-- 1 thor thor   79 May 26 13:18 inventory
-rw-rw-r-- 1 thor thor  169 May 26 13:19 playbook.yml
thor@jump_host ~/ansible$ cat /home/thor/ansible/playbook.yml
- name: Create file in appserver

  hosts: stapp02

  become: yes

  tasks:

    - name: Create the file

      file:

        path: /tmp/file.txt

        state: touch
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ ansible-playbook -i inventory playbook.yml 

PLAY [Create file in appserver] *************************************************************************************************************

TASK [Gathering Facts] **********************************************************************************************************************
ok: [stapp02]

TASK [Create the file] **********************************************************************************************************************
changed: [stapp02]

PLAY RECAP **********************************************************************************************************************************
stapp02                    : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   


thor@jump_host ~/ansible$ ansible all -a "ls -ltr /tmp/" -i ansible/inventory
[WARNING]: Unable to parse /home/thor/ansible/ansible/inventory as an inventory source
[WARNING]: No inventory was parsed, only implicit localhost is available
[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'


thor@jump_host ~/ansible$ ansible all -a "ls -ltr /tmp/" -i inventory
stapp02 | CHANGED | rc=0 >>
total 8
-rw------- 1 root  root     0 Aug  1  2019 yum.log
-rwx------ 1 root  root   836 Aug  1  2019 ks-script-rnBCJB
-rw-r--r-- 1 root  root     0 May 26 13:22 file.txt
drwx------ 2 steve steve 4096 May 26 13:22 ansible_command_payload_ld0RbL


--------------------------------------------------------------------------------------------------------------------------------
Task 63: 13/Jun/2022
Fix issue with PhpFpm Application Deployed on Kubernetes

We deployed a Nginx and PHPFPM based application on Kubernetes cluster last week and it had been working fine. This morning one of the team members was troubleshooting an issue with this stack and he was supposed to run Nginx welcome page for now on this stack till issue with phpfpm is fixed but he made a change somewhere which caused some issue and the application stopped working. Please look into the issue and fix the same:

The deployment name is nginx-phpfpm-dp and service name is nginx-service. Figure out the issues and fix them. FYI Nginx is configured to use default http port, node port is 30008 and copy index.php under /tmp/index.php to deployment under /var/www/html. Please do not try to delete/modify any other existing components like deployment name, service name etc.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

kubectl get deploy

kubectl get pods

kubectl get svc

kubectl get configmap

kubectl describe pods

kubectl get pods

kubectl describe service

kubectl edit service

kubectl edit configmap

kubectl logs





--------------------------------------------------------------------------------------------------------------------------------
Task 64: 19/Jun/2022

Deploy Tomcat App on Kubernetes

A new java-based application is ready to be deployed on a Kubernetes cluster. The development team had a meeting with the DevOps team to share the requirements and application scope. The team is ready to setup an application stack for it under their existing cluster. Below you can find the details for this:

    Create a namespace named tomcat-namespace-xfusion.

    Create a deployment for tomcat app which should be named as tomcat-deployment-xfusion under the same namespace you created. Replica count should be 1, the container should be named as tomcat-container-xfusion, its image should be gcr.io/kodekloud/centos-ssh-enabled:tomcat and its container port should be 8080.

    Create a service for tomcat app which should be named as tomcat-service-xfusion under the same namespace you created. Service type should be NodePort and nodePort should be 32227.

Before clicking on Check button please make sure the application is up and running.

You can use any labels as per your choice.

Note: The kubectl on jump_host has been configured to work with the kubernetes cluster.


1. kubectl get namespace
		NAME                 STATUS   AGE
		default              Active   118m
		kube-node-lease      Active   118m
		kube-public          Active   118m
		kube-system          Active   118m
		local-path-storage   Active   117m

2. kubectl get pods
		No resources found in default namespace.


3. kubectl create namespace tomcat-namespace-xfusion
		namespace/tomcat-namespace-xfusion created

	 kubectl get namespace
		NAME                       STATUS   AGE
		default                    Active   118m
		kube-node-lease            Active   118m
		kube-public                Active   118m
		kube-system                Active   118m
		local-path-storage         Active   118m
		tomcat-namespace-xfusion   Active   4s
4. vi /tmp/tomcat.yaml
  
   cat /tmp/tomcat.yaml 
			apiVersion: v1

			kind: Service

			metadata:

			  name: tomcat-service-xfusion

			  namespace: tomcat-namespace-xfusion

			spec:

			  type: NodePort

			  selector:

			    app: tomcat

			  ports:

			    - port: 80

			      protocol: TCP

			      targetPort: 8080

			      nodePort: 32227

			---

			apiVersion: apps/v1                          

			kind: Deployment

			metadata:

			  name: tomcat-deployment-xfusion

			  namespace: tomcat-namespace-xfusion

			spec:

			  replicas: 1

			  selector:

			    matchLabels:

			      app: tomcat

			  template:

			    metadata:

			      labels:

			        app: tomcat

			    spec:

			      containers:

			        - name: tomcat-container-xfusion

			          image: gcr.io/kodekloud/centos-ssh-enabled:tomcat

			          ports:

			            - containerPort: 8080

5. kubectl create -f /tmp/tomcat.yaml 
		service/tomcat-service-xfusion created
		deployment.apps/tomcat-deployment-xfusion created

6. kubectl get deploy -n tomcat-namespace-xfusion
		NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
		tomcat-deployment-xfusion   0/1     1            0           31s
   
   kubectl get pods -n tomcat-namespace-xfusion
		NAME                                         READY   STATUS    RESTARTS   AGE
		tomcat-deployment-xfusion-654c5b77ff-qvnqh   1/1     Running   0          52s

7. kubectl exec tomcat-deployment-xfusion-654c5b77ff-qvnqh -n tomcat-namespace-xfusion -- curl http://localhost:8080
		  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
		                                 Dload  Upload   Total   Spent    Left  Speed
		  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0<!DOCTYPE html>
		<!--
		To change this license header, choose License Headers in Project Properties.
		To change this template file, choose Tools | Templates
		and open the template in the editor.
		-->
		<html>
		    <head>
		        <title>SampleWebApp</title>
		        <meta charset="UTF-8">
		        <meta name="viewport" content="width=device-width, initial-scale=1.0">
		    </head>
		    <body>
		        <h2>Welcome to xFusionCorp Industries!</h2>
		        <br>
		    
		    </body>
		</html>
		100   471  100   471    0     0   1304      0 --:--:-- --:--:-- --:--:--  1304

--------------------------------------------------------------------------------------------------------------------------------
Task 65: 23/Jun/2022

Rolling Updates in Kubernetes

We have an application running on Kubernetes cluster using nginx web server. The Nautilus application development team has pushed some of the latest changes and those changes need be deployed. The Nautilus DevOps team has created an image nginx:1.19 with the latest changes.

Perform a rolling update for this application and incorporate nginx:1.19 image. The deployment name is nginx-deployment

Make sure all pods are up and running after the update.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.


1. kubectl get deploy
		NAME               READY   UP-TO-DATE   AVAILABLE   AGE
		nginx-deployment   3/3     3            3           23s

2.kubectl get pods
		NAME                                READY   STATUS    RESTARTS   AGE
		nginx-deployment-74fb588559-56499   1/1     Running   0          29s
		nginx-deployment-74fb588559-l5d6q   1/1     Running   0          28s
		nginx-deployment-74fb588559-qrt22   1/1     Running   0          28s

3. kubectl describe deployment nginx-deployment
		Name:                   nginx-deployment
		Namespace:              default
		CreationTimestamp:      Thu, 23 Jun 2022 03:07:35 +0000
		Labels:                 app=nginx-app
		                        type=front-end
		Annotations:            deployment.kubernetes.io/revision: 1
		Selector:               app=nginx-app
		Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
		StrategyType:           RollingUpdate
		MinReadySeconds:        0
		RollingUpdateStrategy:  25% max unavailable, 25% max surge
		Pod Template:
		  Labels:  app=nginx-app
		  Containers:
		   nginx-container:
		    Image:        nginx:1.16                                  <---------------------------------
		    Port:         <none>
		    Host Port:    <none>
		    Environment:  <none>
		    Mounts:       <none>
		  Volumes:        <none>
		Conditions:
		  Type           Status  Reason
		  ----           ------  ------
		  Available      True    MinimumReplicasAvailable
		  Progressing    True    NewReplicaSetAvailable
		OldReplicaSets:  <none>
		NewReplicaSet:   nginx-deployment-74fb588559 (3/3 replicas created)
		Events:
		  Type    Reason             Age   From                   Message
		  ----    ------             ----  ----                   -------
		  Normal  ScalingReplicaSet  55s   deployment-controller  Scaled up replica set nginx-deployment-74fb588559 to 3

4.kubectl set image deployment nginx-deployment nginx-container=nginx:1.19
		deployment.apps/nginx-deployment image updated

5. kubectl get pods
		NAME                                READY   STATUS              RESTARTS   AGE
		nginx-deployment-57bf6d6978-7b4ml   0/1     ContainerCreating   0          9s                          <--------------------------------
		nginx-deployment-74fb588559-56499   1/1     Running             0          3m4s
		nginx-deployment-74fb588559-l5d6q   1/1     Running             0          3m3s
		nginx-deployment-74fb588559-qrt22   1/1     Running             0          3m3s

6. kubectl get deploy
		NAME               READY   UP-TO-DATE   AVAILABLE   AGE
		nginx-deployment   3/3     3            3           3m23s

7. kubectl get pods
		NAME                                READY   STATUS    RESTARTS   AGE
		nginx-deployment-57bf6d6978-7b4ml   1/1     Running   0          35s                  <--------------
		nginx-deployment-57bf6d6978-7v8jj   1/1     Running   0          18s                  <--------------     
		nginx-deployment-57bf6d6978-tbnp4   1/1     Running   0          21s                  <-------------

8. kubectl describe deployment nginx-deployment
		Name:                   nginx-deployment
		Namespace:              default
		CreationTimestamp:      Thu, 23 Jun 2022 03:07:35 +0000
		Labels:                 app=nginx-app
		                        type=front-end
		Annotations:            deployment.kubernetes.io/revision: 2
		Selector:               app=nginx-app
		Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
		StrategyType:           RollingUpdate
		MinReadySeconds:        0
		RollingUpdateStrategy:  25% max unavailable, 25% max surge
		Pod Template:
		  Labels:  app=nginx-app
		  Containers:
		   nginx-container:
		    Image:        nginx:1.19                                                 <----------------------------------
		    Port:         <none>
		    Host Port:    <none>
		    Environment:  <none>
		    Mounts:       <none>
		  Volumes:        <none>
		Conditions:
		  Type           Status  Reason
		  ----           ------  ------
		  Available      True    MinimumReplicasAvailable
		  Progressing    True    NewReplicaSetAvailable
		OldReplicaSets:  <none>
		NewReplicaSet:   nginx-deployment-57bf6d6978 (3/3 replicas created)
		Events:
		  Type    Reason             Age    From                   Message
		  ----    ------             ----   ----                   -------
		  Normal  ScalingReplicaSet  3m50s  deployment-controller  Scaled up replica set nginx-deployment-74fb588559 to 3
		  Normal  ScalingReplicaSet  55s    deployment-controller  Scaled up replica set nginx-deployment-57bf6d6978 to 1
		  Normal  ScalingReplicaSet  41s    deployment-controller  Scaled down replica set nginx-deployment-74fb588559 to 2
		  Normal  ScalingReplicaSet  41s    deployment-controller  Scaled up replica set nginx-deployment-57bf6d6978 to 2
		  Normal  ScalingReplicaSet  38s    deployment-controller  Scaled down replica set nginx-deployment-74fb588559 to 1
		  Normal  ScalingReplicaSet  38s    deployment-controller  Scaled up replica set nginx-deployment-57bf6d6978 to 3
		  Normal  ScalingReplicaSet  35s    deployment-controller  Scaled down replica set nginx-deployment-74fb588559 to 0
		thor@jump_host ~$ 

9. kubectl get deploy
		NAME               READY   UP-TO-DATE   AVAILABLE   AGE
		nginx-deployment   3/3     3            3           4m32s

10. kubectl get pods
		NAME                                READY   STATUS    RESTARTS   AGE
		nginx-deployment-57bf6d6978-7b4ml   1/1     Running   0          103s
		nginx-deployment-57bf6d6978-7v8jj   1/1     Running   0          86s
		nginx-deployment-57bf6d6978-tbnp4   1/1     Running   0          89s

11. kubectl rollout status deployment nginx-deployment
		deployment "nginx-deployment" successfully rolled out

--------------------------------------------------------------------------------------------------------------------------------
Task 66: 27/Jun/2022
Create Namespaces in Kubernetes Cluster

The Nautilus DevOps team is planning to deploy some micro services on Kubernetes platform. The team has already set up a Kubernetes cluster and now they want set up some namespaces, deployments etc. Based on the current requirements, the team has shared some details as below:

Create a namespace named dev and create a POD under it; name the pod dev-nginx-pod and use nginx image with latest tag only and remember to mention tag i.e nginx:latest.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

1. kubectl get namespaces
		NAME                 STATUS   AGE
		default              Active   128m
		kube-node-lease      Active   128m
		kube-public          Active   128m
		kube-system          Active   128m
		local-path-storage   Active   127m

2. kubectl get pods
		No resources found in default namespace.

3. kubectl get namespace
		NAME                 STATUS   AGE
		default              Active   128m
		kube-node-lease      Active   128m
		kube-public          Active   128m
		kube-system          Active   128m
		local-path-storage   Active   128m

4. kubectl create namespace dev
		namespace/dev created

5. kubectl get namespace
		NAME                 STATUS   AGE
		default              Active   128m
		dev                  Active   5s
		kube-node-lease      Active   128m
		kube-public          Active   128m
		kube-system          Active   128m
		local-path-storage   Active   128m

6. kubectl run dev-nginx-pod --image=nginx:latest -n dev
		pod/dev-nginx-pod created

7. kubectl get pods -n dev
		NAME            READY   STATUS              RESTARTS   AGE
		dev-nginx-pod   0/1     ContainerCreating   0          9s

--------------------------------------------------------------------------------------------------------------------------------
Task 67: 4/Jul/2022
Ansible Create Users and Groups

Several new developers and DevOps engineers just joined the xFusionCorp industries. They have been assigned the Nautilus project, and as per the onboarding process we need to create user accounts for new joinees on at least one of the app servers in Stratos DC. We also need to create groups and make new users members of those groups. We need to accomplish this task using Ansible. Below you can find more information about the task.

There is already an inventory file ~/playbooks/inventory on jump host.

On jump host itself there is a list of users in ~/playbooks/data/users.yml file and there are two groups — admins and developers —that have list of different users. Create a playbook ~/playbooks/add_users.yml on jump host to perform the following tasks on app server 2 in Stratos DC.

a. Add all users given in the users.yml file on app server 2.

b. Also add developers and admins groups on the same server.

c. As per the list given in the users.yml file, make each user member of the respective group they are listed under.

d. Make sure home directory for all of the users under developers group is /var/www (not the default i.e /var/www/{USER}). Users under admins group should use the default home directory (i.e /home/devid for user devid).

e. Set password BruCStnMT5 for all of the users under developers group and LQfKeWWxWD for of the users under admins group. Make sure to use the password given in the ~/playbooks/secrets/vault.txt file as Ansible vault password to encrypt the original password strings. You can use ~/playbooks/secrets/vault.txt file as a vault secret file while running the playbook (make necessary changes in ~/playbooks/ansible.cfg file).

f. All users under admins group must be added as sudo users. To do so, simply make them member of the wheel group as well.

Note: Validation will try to run the playbook using command ansible-playbook -i inventory add_users.yml so please make sure playbook works this way, without passing any extra arguments.


Last login: Mon Jul  4 11:36:22 UTC 2022 on pts/0
1.cd playbooks/
  ls -ahl
		total 24K
		drwxr-xr-x 4 thor thor 4.0K Jul  4 11:36 .
		drwxr----- 1 thor thor 4.0K Jul  4 11:36 ..
		-rw-r--r-- 1 thor thor   36 Jul  4 11:36 ansible.cfg
		drwxr-xr-x 2 thor thor 4.0K Jul  4 10:30 data
		-rw-r--r-- 1 thor thor  237 Jul  4 11:36 inventory
		drwxr-xr-x 2 thor thor 4.0K Jul  4 11:36 secrets

2. cat data/users.yml 
		admins:
		  - rob
		  - david
		  - joy

		developers:
		  - tim
		  - ray
		  - jim
		  - mark

3. cat secrets/vault.txt 
		P@ss3or432

4.Add password vault file location in ansible.cfg 
	 vi ansible.cfg 
	 cat ansible.cfg 
		[defaults]
		host_key_checking = False
		vault_password_file = /home/thor/playbooks/secrets/vault.txt     <------------------- Add password vault file location in ansible.cfg

5. Verify ansible connection to mentioned server
  ansible stapp02 -a "cat /etc/passwd" -i inventory
		stapp02 | CHANGED | rc=0 >>
		root:x:0:0:root:/root:/bin/bash
		bin:x:1:1:bin:/bin:/sbin/nologin
		daemon:x:2:2:daemon:/sbin:/sbin/nologin
		adm:x:3:4:adm:/var/adm:/sbin/nologin
		lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin
		sync:x:5:0:sync:/sbin:/bin/sync
		shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown
		halt:x:7:0:halt:/sbin:/sbin/halt
		mail:x:8:12:mail:/var/spool/mail:/sbin/nologin
		operator:x:11:0:operator:/root:/sbin/nologin
		games:x:12:100:games:/usr/games:/sbin/nologin
		ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin
		nobody:x:99:99:Nobody:/:/sbin/nologin
		systemd-network:x:192:192:systemd Network Management:/:/sbin/nologin
		dbus:x:81:81:System message bus:/:/sbin/nologin
		sshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologin
		ansible:x:1000:1000::/home/ansible:/bin/bash
		steve:x:1001:1001::/home/steve:/bin/bash

6. vi ~/playbooks/add_users.yml
   cat add_users.yml 
			---                                                                                                              
			- name: Ansbile Add User & Group                                                                       
			  hosts: stapp02                                                                                                
			  become: yes                                                                                                    
			  tasks:                                                                                                         
			  - name: Creating Admin Groups                                                                                  
			    group:                                                                                                       
			     name:                                                                                                       
			      admins                                                                                                     
			     state: present                                                                                              
			  - name: Creating Dev Groups                                                                                    
			    group:                                                                                                       
			     name:                                                                                                       
			      developers                                                                                                 
			     state: present                                                                                              
			  - name: Creating Admins Group Users                                                                            
			    user:                                                                                                        
			     name: "{{ item }}"                                                                                          
			     password: "{{ 'LQfKeWWxWD' | password_hash ('sha512') }}"                                                   
			     groups: admins,wheel
			     state: present                                                                                              
			    loop:                                                                                                        
			    - rob                                                                                                        
			    - joy                                                                                                        
			    - david                                                                                                      
			  - name: Creating Developers Group Users                                                                        
			    user:                                                                                                        
			     name: "{{ item }}"                                                                                          
			     password: "{{ 'BruCStnMT5' | password_hash ('sha512') }}"                                                   
			     home: "/var/www/{{ item }}"                                                                                             
			     group: developers                                                                                           
			     state: present                                                                                              
			    loop:                                                                                                        
			    - tim                                                                                                        
			    - jim                                                                                                        
			    - mark                                                                                                       
			    - ray  

7. ansible-playbook -i inventory add_users.yml 

			PLAY [Ansbile Add User & Group] *************************************************************************************************************

			TASK [Gathering Facts] **********************************************************************************************************************
			ok: [stapp02]

			TASK [Creating Admin Groups] ****************************************************************************************************************
			changed: [stapp02]

			TASK [Creating Dev Groups] ******************************************************************************************************************
			changed: [stapp02]

			TASK [Creating Admins Group Users] **********************************************************************************************************
			changed: [stapp02] => (item=rob)
			changed: [stapp02] => (item=joy)
			changed: [stapp02] => (item=david)

			TASK [Creating Developers Group Users] ******************************************************************************************************
			changed: [stapp02] => (item=tim)
			changed: [stapp02] => (item=jim)
			changed: [stapp02] => (item=mark)
			changed: [stapp02] => (item=ray)

			PLAY RECAP **********************************************************************************************************************************
			stapp02                    : ok=5    changed=4    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

8. ansible stapp02 -a "cat /etc/passwd" -i inventory
			stapp02 | CHANGED | rc=0 >>
			root:x:0:0:root:/root:/bin/bash
			bin:x:1:1:bin:/bin:/sbin/nologin
			daemon:x:2:2:daemon:/sbin:/sbin/nologin
			adm:x:3:4:adm:/var/adm:/sbin/nologin
			lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin
			sync:x:5:0:sync:/sbin:/bin/sync
			shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown
			halt:x:7:0:halt:/sbin:/sbin/halt
			mail:x:8:12:mail:/var/spool/mail:/sbin/nologin
			operator:x:11:0:operator:/root:/sbin/nologin
			games:x:12:100:games:/usr/games:/sbin/nologin
			ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin
			nobody:x:99:99:Nobody:/:/sbin/nologin
			systemd-network:x:192:192:systemd Network Management:/:/sbin/nologin
			dbus:x:81:81:System message bus:/:/sbin/nologin
			sshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologin
			ansible:x:1000:1000::/home/ansible:/bin/bash
			steve:x:1001:1001::/home/steve:/bin/bash
			rob:x:1002:1004::/home/rob:/bin/bash
			joy:x:1003:1005::/home/joy:/bin/bash
			david:x:1004:1006::/home/david:/bin/bash
			tim:x:1005:1003::/var/www/tim:/bin/bash
			jim:x:1006:1003::/var/www/jim:/bin/bash
			mark:x:1007:1003::/var/www/mark:/bin/bash
			ray:x:1008:1003::/var/www/ray:/bin/bash

9. ssh rob@stapp02
		The authenticity of host 'stapp02 (172.16.238.11)' can't be established.
		ECDSA key fingerprint is SHA256:7RCf6YZUPBcZw8m3zXVF7SaKj4h3sMVtsQb1QBHyIwE.
		ECDSA key fingerprint is MD5:74:a3:7f:20:34:82:27:27:3a:27:4f:32:25:a3:67:c4.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'stapp02' (ECDSA) to the list of known hosts.
		rob@stapp02's password: 
		[rob@stapp02 ~]$ 

10. [rob@stapp02 ~]$ sudo su -
		[root@stapp02 ~]# 
		[root@stapp02 ~]# pwd
		/root
		[root@stapp02 ~]# exit
		logout
		[rob@stapp02 ~]$ pwd
		/home/rob
		[rob@stapp02 ~]$ 
		[rob@stapp02 ~]$ exit
		logout
		Connection to stapp02 closed.
11. ssh ray@stapp02
		ray@stapp02's password: 
		[ray@stapp02 ~]$ 
		[ray@stapp02 ~]$ 
		[ray@stapp02 ~]$ pwd
		/var/www/ray
		[ray@stapp02 ~]$ 
		[ray@stapp02 ~]$ 
		[ray@stapp02 ~]$ sudo su -

			We trust you have received the usual lecture from the local System
			Administrator. It usually boils down to these three things:

			    #1) Respect the privacy of others.
			    #2) Think before you type.
			    #3) With great power comes great responsibility.

			[sudo] password for ray: 
			ray is not in the sudoers file.  This incident will be reported.
		[ray@stapp02 ~]$ 
		[ray@stapp02 ~]$ 
		[ray@stapp02 ~]$ 
		[ray@stapp02 ~]$ exit
		logout
		Connection to stapp02 closed.
thor@jump_host ~/playbooks$

--------------------------------------------------------------------------------------------------------------------------------
Task 68: 9/Jul/2022

Set Limits for Resources in Kubernetes


Recently some of the performance issues were observed with some applications hosted on Kubernetes cluster. The Nautilus DevOps team has observed some resources constraints, where some of the applications are running out of resources like memory, cpu etc., and some of the applications are consuming more resources than needed. Therefore, the team has decided to add some limits for resources utilization. Below you can find more details.

Create a pod named httpd-pod and a container under it named as httpd-container, use httpd image with latest tag only and remember to mention tag i.e httpd:latest and set the following limits:

Requests: Memory: 15Mi, CPU: 100m

Limits: Memory: 20Mi, CPU: 100m

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

thor@jump_host ~$ kubectl get namespace
NAME                 STATUS   AGE
default              Active   4h4m
kube-node-lease      Active   4h4m
kube-public          Active   4h4m
kube-system          Active   4h4m
local-path-storage   Active   4h3m
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get pods
No resources found in default namespace.
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ vi /tmp/reslimit.yaml
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ cat /tmp/reslimit.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: httpd-pod
spec:
  containers:
  - name: httpd-container
    image: httpd:latest
    resources:
      requests:
        memory: "15Mi"
        cpu: "100m"
      limits:
        memory: "20Mi"
        cpu: "100m"
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl create -f /tmp/reslimit.yaml 
pod/httpd-pod created
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get pods
NAME        READY   STATUS    RESTARTS   AGE
httpd-pod   1/1     Running   0          14s
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl describe pods http-pod
Error from server (NotFound): pods "http-pod" not found
thor@jump_host ~$ kubectl describe pods httpd-pod
Name:         httpd-pod
Namespace:    default
Priority:     0
Node:         kodekloud-control-plane/172.17.0.2
Start Time:   Sat, 09 Jul 2022 03:52:03 +0000
Labels:       <none>
Annotations:  <none>
Status:       Running
IP:           10.244.0.5
IPs:
  IP:  10.244.0.5
Containers:
  httpd-container:
    Container ID:   containerd://445b86e0a90eb7e789f611a291bff2f5bc7280620d4a7c8a617b8d2b02731ef5
    Image:          httpd:latest
    Image ID:       docker.io/library/httpd@sha256:886f273536ebef2239ef7dc42e6486544fbace3e36e5a42735cfdc410e36d33c
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Sat, 09 Jul 2022 03:52:14 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  20Mi
    Requests:
      cpu:        100m
      memory:     15Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-5sbwr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-5sbwr:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-5sbwr
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  53s   default-scheduler  Successfully assigned default/httpd-pod to kodekloud-control-plane
  Normal  Pulling    52s   kubelet            Pulling image "httpd:latest"
  Normal  Pulled     43s   kubelet            Successfully pulled image "httpd:latest" in 9.316827757s
  Normal  Created    43s   kubelet            Created container httpd-container
  Normal  Started    42s   kubelet            Started container httpd-container


--------------------------------------------------------------------------------------------------------------------------------
Task 69: 14/Jul/2022

Ansible Copy Module


There is data on jump host that needs to be copied on all application servers in Stratos DC. Nautilus DevOps team want to perform this task using Ansible. Perform the task as per details mentioned below:

a. On jump host create an inventory file /home/thor/ansible/inventory and add all application servers as managed nodes.

b. On jump host create a playbook /home/thor/ansible/playbook.yml to copy /usr/src/sysops/index.html file to all application servers at location /opt/sysops.

Note: Validation will try to run the playbook using command ansible-playbook -i inventory playbook.yml so please make sure the playbook works this way without passing any extra arguments.

thor@jump_host ~$ cd /home/thor/ansible/
thor@jump_host ~/ansible$ ls -ahl
total 8.0K
drwxr-xr-x 2 thor thor 4.0K Jul 14 02:48 .
drwxr----- 1 thor thor 4.0K Jul 14 02:48 ..
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ vi inventory
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ cat inventory 
stapp01 ansible_host=172.16.238.10 ansible_ssh_pass=Ir0nM@n  ansible_user=tony

stapp02 ansible_host=172.16.238.11 ansible_ssh_pass=Am3ric@  ansible_user=steve

stapp03 ansible_host=172.16.238.12 ansible_ssh_pass=BigGr33n  ansible_user=banner
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ ansible all -a "ls -ahl /opt/sysops" -i inventory 
stapp03 | CHANGED | rc=0 >>
total 8.0K
drwxr-xr-x 2 root root 4.0K Jul 14 02:48 .
drwxr-xr-x 1 root root 4.0K Jul 14 02:48 ..
stapp02 | CHANGED | rc=0 >>
total 8.0K
drwxr-xr-x 2 root root 4.0K Jul 14 02:48 .
drwxr-xr-x 1 root root 4.0K Jul 14 02:48 ..
stapp01 | CHANGED | rc=0 >>
total 8.0K
drwxr-xr-x 2 root root 4.0K Jul 14 02:48 .
drwxr-xr-x 1 root root 4.0K Jul 14 02:48 ..
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ vi playbook.yml
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ cat playbook.yml 
- name: Ansible copy

  hosts: all

  become: yes

  tasks:

    - name: copy index.html to sysops folder

      copy: src=/usr/src/sysops/index.html dest=/opt/sysops
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ ansible-playbook -i inventory playbook.yml

PLAY [Ansible copy] *************************************************************************************************************************

TASK [Gathering Facts] **********************************************************************************************************************
ok: [stapp03]
ok: [stapp02]
ok: [stapp01]

TASK [copy index.html to sysops folder] *****************************************************************************************************
changed: [stapp03]
changed: [stapp02]
changed: [stapp01]

PLAY RECAP **********************************************************************************************************************************
stapp01                    : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
stapp02                    : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
stapp03                    : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

thor@jump_host ~/ansible$ ansible all -a "ls -ahl /opt/sysops" -i inventory 
stapp01 | CHANGED | rc=0 >>
total 12K
drwxr-xr-x 2 root root 4.0K Jul 14 02:52 .
drwxr-xr-x 1 root root 4.0K Jul 14 02:48 ..
-rw-r--r-- 1 root root   35 Jul 14 02:52 index.html
stapp03 | CHANGED | rc=0 >>
total 12K
drwxr-xr-x 2 root root 4.0K Jul 14 02:52 .
drwxr-xr-x 1 root root 4.0K Jul 14 02:48 ..
-rw-r--r-- 1 root root   35 Jul 14 02:52 index.html
stapp02 | CHANGED | rc=0 >>
total 12K
drwxr-xr-x 2 root root 4.0K Jul 14 02:52 .
drwxr-xr-x 1 root root 4.0K Jul 14 02:48 ..
-rw-r--r-- 1 root root   35 Jul 14 02:52 index.html
thor@jump_host ~/ansible$

--------------------------------------------------------------------------------------------------------------------------------
Task 70: 17/Jul/2022

Ansible Archive Module

The Nautilus DevOps team has some data on jump host in Stratos DC that they want to copy on all app servers in the same data center. However, they want to create an archive of data and copy it to the app servers. Additionally, there are some specific requirements for each server. Perform the task using Ansible playbook as per requirements mentioned below:

Create a playbook.yml under /home/thor/ansible on jump host, an inventory file is already placed under /home/thor/ansible/ on Jump Server itself.

    Create an archive official.tar.gz (make sure archive format is tar.gz) of /usr/src/itadmin/ directory ( present on each app server ) and copy it to /opt/itadmin/ directory on all app servers. The user and group owner of archive official.tar.gz should be tony for App Server 1, steve for App Server 2 and banner for App Server 3.

Note: Validation will try to run playbook using command ansible-playbook -i inventory playbook.yml so please make sure playbook works this way, without passing any extra arguments.
thor@jump_host ~$ cd /home/thor/ansible/
thor@jump_host ~/ansible$ ls -agl
total 16
drwxr-xr-x 2 thor 4096 Jul 17 16:14 .
drwxr----- 1 thor 4096 Jul 17 16:14 ..
-rw-r--r-- 1 thor   36 Jul 17 16:14 ansible.cfg
-rw-r--r-- 1 thor  237 Jul 17 16:14 inventory
thor@jump_host ~/ansible$ ls -ahl
total 16K
drwxr-xr-x 2 thor thor 4.0K Jul 17 16:14 .
drwxr----- 1 thor thor 4.0K Jul 17 16:14 ..
-rw-r--r-- 1 thor thor   36 Jul 17 16:14 ansible.cfg
-rw-r--r-- 1 thor thor  237 Jul 17 16:14 inventory
thor@jump_host ~/ansible$ cat inventory 
stapp01 ansible_host=172.16.238.10 ansible_ssh_pass=Ir0nM@n ansible_user=tony
stapp02 ansible_host=172.16.238.11 ansible_ssh_pass=Am3ric@ ansible_user=steve
stapp03 ansible_host=172.16.238.12 ansible_ssh_pass=BigGr33n ansible_user=bannerthor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ ansible all -a "ls -ahl /opt/sysops" -i inventory 
stapp03 | FAILED | rc=2 >>
ls: cannot access /opt/sysops: No such file or directorynon-zero return code
stapp02 | FAILED | rc=2 >>
ls: cannot access /opt/sysops: No such file or directorynon-zero return code
stapp01 | FAILED | rc=2 >>
ls: cannot access /opt/sysops: No such file or directorynon-zero return code
thor@jump_host ~/ansible$ ansible all -a "ls -ahl /opt/itadmin" -i inventory 
stapp03 | CHANGED | rc=0 >>
total 8.0K
drwxr-xr-x 2 root root 4.0K Jul 17 16:14 .
drwxr-xr-x 1 root root 4.0K Jul 17 16:14 ..
stapp01 | CHANGED | rc=0 >>
total 8.0K
drwxr-xr-x 2 root root 4.0K Jul 17 16:14 .
drwxr-xr-x 1 root root 4.0K Jul 17 16:14 ..
stapp02 | CHANGED | rc=0 >>
total 8.0K
drwxr-xr-x 2 root root 4.0K Jul 17 16:14 .
drwxr-xr-x 1 root root 4.0K Jul 17 16:14 ..
thor@jump_host ~/ansible$ vi playbook.yml
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ cat playbook.yml 
- name: Task create archive and copy to host

  hosts: stapp01, stapp02, stapp03

  become: yes

  tasks:

    - name: As per the task create the archive file and set the owner

      archive:

        path: /usr/src/itadmin/

        dest: /opt/itadmin/official.tar.gz

        format: gz

        force_archive: true

        owner: "{{ ansible_user }}"

        group: "{{ ansible_user }}"
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ ls -ahl /usr/src/itadmin
ls: cannot access /usr/src/itadmin: No such file or directory
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ ansible-playbook -i inventory playbook.yml

PLAY [Task create archive and copy to host] *************************************************************************************************

TASK [Gathering Facts] **********************************************************************************************************************
ok: [stapp03]
ok: [stapp01]
ok: [stapp02]

TASK [As per the task create the archive file and set the owner] ****************************************************************************
changed: [stapp03]
changed: [stapp02]
changed: [stapp01]

PLAY RECAP **********************************************************************************************************************************
stapp01                    : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
stapp02                    : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
stapp03                    : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ ansible all -a "ls -ahl /opt/itadmin" -i inventory 
stapp01 | CHANGED | rc=0 >>
total 12K
drwxr-xr-x 2 root root 4.0K Jul 17 16:19 .
drwxr-xr-x 1 root root 4.0K Jul 17 16:14 ..
-rw-r--r-- 1 tony tony  169 Jul 17 16:19 official.tar.gz
stapp03 | CHANGED | rc=0 >>
total 12K
drwxr-xr-x 2 root   root   4.0K Jul 17 16:19 .
drwxr-xr-x 1 root   root   4.0K Jul 17 16:14 ..
-rw-r--r-- 1 banner banner  166 Jul 17 16:19 official.tar.gz
stapp02 | CHANGED | rc=0 >>
total 12K
drwxr-xr-x 2 root  root  4.0K Jul 17 16:19 .
drwxr-xr-x 1 root  root  4.0K Jul 17 16:14 ..
-rw-r--r-- 1 steve steve  178 Jul 17 16:19 official.tar.gz
thor@jump_host ~/ansible$


--------------------------------------------------------------------------------------------------------------------------------
Task 71: 21/Jul/2022

 Puppet Manage Services

 New packages need to be installed on some of the app servers in Stratos Datacenter. The Nautilus DevOps team has decided to install the same using Puppet. Since jump host is already configured to run as Puppet master server and all app servers are already configured to work as puppet agent nodes, we need to create the required manifests on the Puppet master server so that it can be applied on the required Puppet agent node. Please find more details about the task below.

Create a Puppet programming file demo.pp under /etc/puppetlabs/code/environments/production/manifests directory on master node i.e Jump Host to perform the below given tasks.

    Install package httpd using puppet package resource and start its service using puppet service resource on Puppet agent node 2 i.e App Server 2.

Notes: :- Please make sure to run the puppet agent test using sudo on agent nodes, otherwise you can face certificate issues. In that case you will have to clean the certificates first and then you will be able to run the puppet agent test.

:- Before clicking on the Check button please make sure to verify puppet server and puppet agent services are up and running on the respective servers, also please make sure to run puppet agent test to apply/test the changes manually first.

:- Please note that once lab is loaded, the puppet server service should start automatically on puppet master server, however it can take upto 2-3 minutes to start.




thor@jump_host ~$ ssh -t steve@stapp02 "systemctl status httpd"
The authenticity of host 'stapp02 (172.16.238.11)' can't be established.
ECDSA key fingerprint is SHA256:cPKTsscSCR1Q3bxjworQ0ZblELpSiTS9fk6221Fg3qA.
ECDSA key fingerprint is MD5:90:4b:f5:fc:fa:44:1c:a0:01:d3:4d:21:b9:1a:4a:27.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'stapp02,172.16.238.11' (ECDSA) to the list of known hosts.
steve@stapp02's password: 
Unit httpd.service could not be found.
Connection to stapp02 closed.
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ sudo su -

We trust you have received the usual lecture from the local System
Administrator. It usually boils down to these three things:

    #1) Respect the privacy of others.
    #2) Think before you type.
    #3) With great power comes great responsibility.

[sudo] password for thor: 
root@jump_host ~# 
root@jump_host ~# 
root@jump_host ~# 
root@jump_host ~# cd /etc/puppetlabs/code/environments/production/manifests/
root@jump_host /etc/puppetlabs/code/environments/production/manifests# ls -ahl
total 8.0K
drwxr-xr-x 1 puppet puppet 4.0K Jul 13  2021 .
drwxr-xr-x 1 puppet puppet 4.0K Aug  9  2021 ..
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# vi demo.pp
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# cat demo.pp 
class httpd_installer {

    package {'httpd':

        ensure => installed

    }

    service {'httpd':

        ensure    => running,

        enable    => true,

    }

}

node 'stapp02.stratos.xfusioncorp.com' {

  include httpd_installer

}
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# puppet parser validate demo.pp
root@jump_host /etc/puppetlabs/code/environments/production/manifests# ssh  steve@stapp02
The authenticity of host 'stapp02 (172.16.238.11)' can't be established.
ECDSA key fingerprint is SHA256:cPKTsscSCR1Q3bxjworQ0ZblELpSiTS9fk6221Fg3qA.
ECDSA key fingerprint is MD5:90:4b:f5:fc:fa:44:1c:a0:01:d3:4d:21:b9:1a:4a:27.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'stapp02,172.16.238.11' (ECDSA) to the list of known hosts.
steve@stapp02's password: 
Last login: Thu Jul 21 14:37:42 2022 from jump_host.stratos.xfusioncorp.com
[steve@stapp02 ~]$ 
[steve@stapp02 ~]$ 
[steve@stapp02 ~]$ sudo su - 

We trust you have received the usual lecture from the local System
Administrator. It usually boils down to these three things:

    #1) Respect the privacy of others.
    #2) Think before you type.
    #3) With great power comes great responsibility.

[sudo] password for steve: 
[root@stapp02 ~]# 
[root@stapp02 ~]# 
[root@stapp02 ~]# 
[root@stapp02 ~]# puppet agent -tv
Info: Using configured environment 'production'
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Retrieving locales
Info: Caching catalog for stapp02.stratos.xfusioncorp.com
Info: Applying configuration version '1658414537'
Notice: /Stage[main]/Httpd_installer/Package[httpd]/ensure: created
Notice: /Stage[main]/Httpd_installer/Service[httpd]/ensure: ensure changed 'stopped' to 'running'
Info: /Stage[main]/Httpd_installer/Service[httpd]: Unscheduling refresh on Service[httpd]
Notice: Applied catalog in 23.21 seconds
[root@stapp02 ~]# systemctl status httpd
● httpd.service - The Apache HTTP Server
   Loaded: loaded (/usr/lib/systemd/system/httpd.service; enabled; vendor preset: disabled)
   Active: active (running) since Thu 2022-07-21 14:42:46 UTC; 32s ago
     Docs: man:httpd(8)
           man:apachectl(8)
 Main PID: 1462 (httpd)
   Status: "Total requests: 0; Current requests/sec: 0; Current traffic:   0 B/sec"
   CGroup: /docker/50268e59ce67a7e23d4897f43cdfb3200edd62d3198d32104728e7ec150d21c6/system.slice/httpd.service
           ├─1462 /usr/sbin/httpd -DFOREGROUND
           ├─1463 /usr/sbin/httpd -DFOREGROUND
           ├─1464 /usr/sbin/httpd -DFOREGROUND
           ├─1465 /usr/sbin/httpd -DFOREGROUND
           ├─1466 /usr/sbin/httpd -DFOREGROUND
           └─1467 /usr/sbin/httpd -DFOREGROUND

Jul 21 14:42:46 stapp02.stratos.xfusioncorp.com systemd[1]: httpd.service: got MAINPID=1462
Jul 21 14:42:46 stapp02.stratos.xfusioncorp.com systemd[1]: httpd.service: got READY=1
Jul 21 14:42:46 stapp02.stratos.xfusioncorp.com systemd[1]: httpd.service changed start -> running
Jul 21 14:42:46 stapp02.stratos.xfusioncorp.com systemd[1]: Job httpd.service/start finished, result=done
Jul 21 14:42:46 stapp02.stratos.xfusioncorp.com systemd[1]: Started The Apache HTTP Server.
Jul 21 14:42:46 stapp02.stratos.xfusioncorp.com systemd[1]: httpd.service: got STATUS=Processing requests...
Jul 21 14:43:16 stapp02.stratos.xfusioncorp.com systemd[1]: Got notification message for unit httpd.service
Jul 21 14:43:16 stapp02.stratos.xfusioncorp.com systemd[1]: httpd.service: Got notification message from PID 1462 (READY=1, STATUS=T...B/sec)
Jul 21 14:43:16 stapp02.stratos.xfusioncorp.com systemd[1]: httpd.service: got READY=1
Jul 21 14:43:16 stapp02.stratos.xfusioncorp.com systemd[1]: httpd.service: got STATUS=Total requests: 0; Current requests/sec: 0; Cu... B/sec
Hint: Some lines were ellipsized, use -l to show in full.
[root@stapp02 ~]# 
[root@stapp02 ~]# 
[root@stapp02 ~]#

--------------------------------------------------------------------------------------------------------------------------------
Task 72: 2/Aug/2022

Environment Variables in Kubernetes

There are a number of parameters that are used by the applications. We need to define these as environment variables, so that we can use them as needed within different configs. Below is a scenario which needs to be configured on Kubernetes cluster. Please find below more details about the same.

    Create a pod named envars.

    Container name should be fieldref-container, use image busybox preferable latest tag, use command 'sh', '-c' and args should be

'while true; do echo -en '/n'; printenv NODE_NAME POD_NAME; printenv POD_IP POD_SERVICE_ACCOUNT; sleep 10; done;'

(Note: please take care of indentations)

    Define Four environment variables as mentioned below:

a.) The first env should be named as NODE_NAME, set valueFrom fieldref and fieldPath should be spec.nodeName.

b.) The second env should be named as POD_NAME, set valueFrom fieldref and fieldPath should be metadata.name.

c.) The third env should be named as POD_IP, set valueFrom fieldref and fieldPath should be status.podIP.

d.) The fourth env should be named as POD_SERVICE_ACCOUNT, set valueFrom fieldref and fieldPath shoulbe be spec.serviceAccountName.

    Set restart policy to Never.

    To check the output, exec into the pod and use printenv command.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.


thor@jump_host ~$ kubectl get namespace
NAME                 STATUS   AGE
default              Active   96m
kube-node-lease      Active   96m
kube-public          Active   96m
kube-system          Active   97m
local-path-storage   Active   96m
thor@jump_host ~$ kubectl get pods
No resources found in default namespace.

thor@jump_host ~$ vi /tmp/envars.yml
thor@jump_host ~$ cat /tmp/envars.yml 
apiVersion: v1
kind: Pod
metadata:
  name: envars
  namespace: default
spec:
  restartPolicy: Never
  containers:
    - name: fieldref-container
      image: busybox:latest
      command: ["sh", "-c"]
      args:
        - while true; do
          echo -en '\n';
          printenv NODE_NAME POD_NAME;
          printenv POD_IP POD_SERVICE_ACCOUNT;
          sleep 10;
          done;
      env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: POD_SERVICE_ACCOUNT
          valueFrom:
            fieldRef:
              fieldPath: spec.serviceAccountName

thor@jump_host ~$ kubectl create -f /tmp/envars.yml 
pod/envars created
thor@jump_host ~$ 

thor@jump_host ~$ kubectl get pods
NAME     READY   STATUS    RESTARTS   AGE
envars   1/1     Running   0          6s

thor@jump_host ~$ 
thor@jump_host ~$ 
 
thor@jump_host ~$ kubectl exec -it envars  -n default  -- /bin/sh

/ # printenv
		POD_IP=10.244.0.5
		KUBERNETES_SERVICE_PORT=443
		KUBERNETES_PORT=tcp://10.96.0.1:443
		HOSTNAME=envars
		SHLVL=1
		HOME=/root
		NODE_NAME=kodekloud-control-plane
		TERM=xterm
		POD_NAME=envars
		KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1
		PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
		POD_SERVICE_ACCOUNT=default
		KUBERNETES_PORT_443_TCP_PORT=443
		KUBERNETES_PORT_443_TCP_PROTO=tcp
		KUBERNETES_SERVICE_PORT_HTTPS=443
		KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443
		KUBERNETES_SERVICE_HOST=10.96.0.1
		PWD=/
/ # 

--------------------------------------------------------------------------------------------------------------------------------
Task 73: 4/Aug/2022

 Puppet String Manipulation

 There is some data on App Server 1 in Stratos DC. The Nautilus development team shared some requirement with the DevOps team to alter some of the data as per recent changes. The DevOps team is working to prepare a Puppet programming file to accomplish this. Below you can find more details about the task.

Create a Puppet programming file beta.pp under /etc/puppetlabs/code/environments/production/manifests directory on Puppet master node i.e Jump Server and by using puppet file_line resource perform the following tasks.

    We have a file /opt/data/beta.txt on App Server 1. Use the Puppet programming file mentioned above to replace line Welcome to Nautilus Industries! to Welcome to xFusionCorp Industries!, no other data should be altered in this file.

Notes: :- Please make sure to run the puppet agent test using sudo on agent nodes, otherwise you can face certificate issues. In that case you will have to clean the certificates first and then you will be able to run the puppet agent test.

:- Before clicking on the Check button please make sure to verify puppet server and puppet agent services are up and running on the respective servers, also please make sure to run puppet agent test to apply/test the changes manually first.

:- Please note that once lab is loaded, the puppet server service should start automatically on puppet master server, however it can take upto 2-3 minutes to start.




thor@jump_host$ sudo su -

We trust you have received the usual lecture from the local System
Administrator. It usually boils down to these three things:

    #1) Respect the privacy of others.
    #2) Think before you type.
    #3) With great power comes great responsibility.

[sudo] password for thor: 

root@jump_host ~# 
root@jump_host ~# cd /etc/puppetlabs/code/environments/production/manifests/
root@jump_host /etc/puppetlabs/code/environments/production/manifests# ls -ahl
total 8.0K
drwxr-xr-x 1 puppet puppet 4.0K Jul 13  2021 .
drwxr-xr-x 1 puppet puppet 4.0K Aug  9  2021 ..
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# vi beta.pp
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# cat beta.pp 
class data_replacer {

  file_line { 'line_replace':

    path => '/opt/data/beta.txt',

    match => 'Welcome to Nautilus Industries!',

    line  => 'Welcome to xFusionCorp Industries!',

  }

}

node 'stapp01.stratos.xfusioncorp.com' {

  include data_replacer

}
root@jump_host /etc/puppetlabs/code/environments/production/manifests# puppet  parser validate beta.pp
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# ssh tony@stapp01
The authenticity of host 'stapp01 (172.16.238.10)' can't be established.
ECDSA key fingerprint is SHA256:WbglaHl1g4fRfeVoPlwyr5SjSs6/Zzzo+KrvtnxmOqc.
ECDSA key fingerprint is MD5:bb:62:cb:94:db:24:9d:e7:b1:fa:56:bb:1b:02:09:8d.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'stapp01,172.16.238.10' (ECDSA) to the list of known hosts.
tony@stapp01's password: 
[tony@stapp01 ~]$ 
[tony@stapp01 ~]$ sudo su -

We trust you have received the usual lecture from the local System
Administrator. It usually boils down to these three things:

    #1) Respect the privacy of others.
    #2) Think before you type.
    #3) With great power comes great responsibility.

[sudo] password for tony: 
[root@stapp01 ~]# puppet  agent -tv
Info: Using configured environment 'production'
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Retrieving locales
Info: Loading facts
Info: Caching catalog for stapp01.stratos.xfusioncorp.com
Info: Applying configuration version '1659637132'
Notice: Applied catalog in 0.18 seconds
[root@stapp01 ~]# 
[root@stapp01 ~]# 
[root@stapp01 ~]# cat /opt/data/beta.txt 
This is  Nautilus sample file, created using Puppet!
Welcome to xFusionCorp Industries!
Please do not modify this file manually!
[root@stapp01 ~]#

--------------------------------------------------------------------------------------------------------------------------------
Task 74: 11/Aug/2022

Create a Docker Network


The Nautilus DevOps team needs to set up several docker environments for different applications. One of the team members has been assigned a ticket where he has been asked to create some docker networks to be used later. Complete the task based on the following ticket description:

a. Create a docker network named as ecommerce on App Server 1 in Stratos DC.

b. Configure it to use bridge drivers.

c. Set it to use subnet 172.168.0.0/24 and iprange 172.168.0.1/24.

1.
thor@jump_host ~$ ssh tony@stapp01
	tony@stapp01's password: 
	Last login: Thu Aug 11 06:00:49 2022 from jump_host.devops-docker-network-v2_app_net
 
[tony@stapp01 ~]$ sudo su -
	[sudo] password for tony: 
	Last login: Thu Aug 11 06:01:06 UTC 2022 on pts/0

2.
[root@stapp01 ~]# docker network ls
	NETWORK ID     NAME      DRIVER    SCOPE
	820c238da3c4   bridge    bridge    local
	283b6c449fa5   host      host      local
	373edcfd5a8e   none      null      local

3. 
[root@stapp01 ~]# docker network create -d bridge --subnet=172.168.0.0/24 --ip-range=172.168.0.1/24 ecommerce
	f167ca63f027883998a63bf2d8f633522a7e7b400f09785afa9da0ccef2948d6

4.
[root@stapp01 ~]# docker network ls
	NETWORK ID     NAME        DRIVER    SCOPE
	820c238da3c4   bridge      bridge    local
	f167ca63f027   ecommerce   bridge    local
	283b6c449fa5   host        host      local
	373edcfd5a8e   none        null      local

5.
[root@stapp01 ~]# docker network inspect ecommerce
		[
		    {
		        "Name": "ecommerce",
		        "Id": "f167ca63f027883998a63bf2d8f633522a7e7b400f09785afa9da0ccef2948d6",
		        "Created": "2022-08-11T06:03:22.153851571Z",
		        "Scope": "local",
		        "Driver": "bridge",
		        "EnableIPv6": false,
		        "IPAM": {
		            "Driver": "default",
		            "Options": {},
		            "Config": [
		                {
		                    "Subnet": "172.168.0.0/24",
		                    "IPRange": "172.168.0.1/24"
		                }
		            ]
		        },
		        "Internal": false,
		        "Attachable": false,
		        "Ingress": false,
		        "ConfigFrom": {
		            "Network": ""
		        },
		        "ConfigOnly": false,
		        "Containers": {},
		        "Options": {},
		        "Labels": {}
		    }
		]

--------------------------------------------------------------------------------------------------------------------------------
Task 75: 20/Aug/2022

 Deploy Nagios on Kubernetes


The Nautilus DevOps team is planning to set up a Nagios monitoring tool to monitor some applications, services etc. They are planning to deploy it on Kubernetes cluster. Below you can find more details.

1) Create a deployment nagios-deployment for Nagios core. The container name must be nagios-container and it must use jasonrivers/nagios image.

2) Create a user and password for the Nagios core web interface, user must be xFusionCorp and password must be LQfKeWWxWD. (you can manually perform this step after deployment)

3) Create a service nagios-service for Nagios, which must be of targetPort type. nodePort must be 30008.

You can use any labels as per your choice.

Note: The kubectl on jump_host has been configured to work with the kubernetes cluster.


thor@jump_host ~$ kubectl get deploy
	No resources found in default namespace.
 
thor@jump_host ~$ kubectl get service
	NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
	kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4h48m

thor@jump_host ~$ vi /tmp/nagios.yaml
thor@jump_host ~$ cat /tmp/nagios.yaml 
		---
		apiVersion: apps/v1
		kind: Deployment
		metadata:
		  name: nagios-deployment
		spec:
		  replicas: 1
		  selector:
		    matchLabels:
		      app: nagios-core
		  template:
		    metadata:
		      labels:
		        app: nagios-core
		    spec:
		      containers:
		        - name: nagios-container
		          image: jasonrivers/nagios
		---
		apiVersion: v1
		kind: Service
		metadata:
		  name: nagios-service
		spec:
		  type: NodePort
		  selector:
		    app: nagios-core
		  ports:
		    - port: 80
		      targetPort: 80
		      nodePort: 30008
 
thor@jump_host ~$ kubectl create -f /tmp/nagios.yaml 
	deployment.apps/nagios-deployment created
	service/nagios-service created


thor@jump_host ~$ kubectl get deploy
	NAME                READY   UP-TO-DATE   AVAILABLE   AGE
	nagios-deployment   0/1     1            0           9s

thor@jump_host ~$ kubectl get service
	NAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
	kubernetes       ClusterIP   10.96.0.1      <none>        443/TCP        4h51m
	nagios-service   NodePort    10.96.133.33   <none>        80:30008/TCP   17s
 
thor@jump_host ~$ kubectl get pods -o wide
	NAME                                 READY   STATUS              RESTARTS   AGE   IP       NODE                      NOMINATED NODE   READINESS GATES
	nagios-deployment-6674945696-hhlzz   0/1     ContainerCreating   0          30s   <none>   kodekloud-control-plane   <none>           <none>


thor@jump_host ~$ kubectl get pods -o wide
	NAME                                 READY   STATUS    RESTARTS   AGE   IP           NODE                      NOMINATED NODE   READINESS GATES
	nagios-deployment-6674945696-hhlzz   1/1     Running   0          61s   10.244.0.5   kodekloud-control-plane   <none>           <none>

thor@jump_host ~$ kubectl get deploy
	NAME                READY   UP-TO-DATE   AVAILABLE   AGE
	nagios-deployment   1/1     1            1           66s

thor@jump_host ~$ kubectl exec -it nagios-deployment-6674945696-hhlzz -- /bin/bash
		root@nagios-deployment-6674945696-hhlzz:/# 
		root@nagios-deployment-6674945696-hhlzz:/# 
		root@nagios-deployment-6674945696-hhlzz:/# hostname
		nagios-deployment-6674945696-hhlzz
		root@nagios-deployment-6674945696-hhlzz:/# 
		root@nagios-deployment-6674945696-hhlzz:/# 
		root@nagios-deployment-6674945696-hhlzz:/# 
		root@nagios-deployment-6674945696-hhlzz:/# 
		root@nagios-deployment-6674945696-hhlzz:/# htpasswd /opt/nagios/etc/htpasswd.users xFusionCorp
		New password: 
		Re-type new password: 
		Adding password for user xFusionCorp
		root@nagios-deployment-6674945696-hhlzz:/# 
		root@nagios-deployment-6674945696-hhlzz:/# 
		root@nagios-deployment-6674945696-hhlzz:/# 
		root@nagios-deployment-6674945696-hhlzz:/# curl -u xFusionCorp http://localhost/
		Enter host password for user 'xFusionCorp':
					<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Frameset//EN" "http://www.w3.org/TR/html4/frameset.dtd">

					<html>
					<head>
					        <meta name="ROBOTS" content="NOINDEX, NOFOLLOW">
					        <title>Nagios: localhost</title>
					        <link rel="shortcut icon" href="images/favicon.ico" type="image/ico">

					        <script LANGUAGE="javascript">
					                var n = Math.round(Math.random() * 10000000000);
					                document.cookie = "NagFormId=" + n.toString(16);
					        </script>
					</head>

					<frameset cols="180,*" style="border: 0px; framespacing: 0px">
					        <frame src="side.php" name="side" frameborder="0" style="">
					        <frame src="main.php" name="main" frameborder="0" style="">

					        <noframes>
					                <!-- This page requires a web browser which supports frames. -->
					                <h2>Nagios Core</h2>
					                <p align="center">
					                        <a href="https://www.nagios.org/">www.nagios.org</a><br>
					                        Copyright &copy; 2010-2022 Nagios Core Development Team and Community Contributors.
					                        Copyright &copy; 1999-2010 Ethan Galstad<br>
					                </p>
					                <p>
					                        <i>Note: These pages require a browser which supports frames</i>
					                </p>
					        </noframes>
					</frameset>

					</html>
		root@nagios-deployment-6674945696-hhlzz:/#


View Port : 30008 on browser username /password 

--------------------------------------------------------------------------------------------------------------------------------
Task 76: 23/Aug/2022

Docker Copy Operations

The Nautilus DevOps team has some conditional data present on App Server 1 in Stratos Datacenter. There is a container ubuntu_latest running on the same server. We received a request to copy some of the data from the docker host to the container. Below are more details about the task:

On App Server 1 in Stratos Datacenter copy an encrypted file /tmp/nautilus.txt.gpg from docker host to ubuntu_latest container (running on same server) in /tmp/ location. Please do not try to modify this file in any way.


thor@jump_host ~$ ssh tony@stapp01
		The authenticity of host 'stapp01 (172.16.238.10)' can't be established.
		ECDSA key fingerprint is SHA256:mLdWSn3TjPUitnlhOHi1JPUIS6a/P9zTxc5hRMxVVnA.
		ECDSA key fingerprint is MD5:ef:03:06:06:c2:d5:d5:8f:f8:79:3f:3d:86:bb:4c:81.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'stapp01,172.16.238.10' (ECDSA) to the list of known hosts.
		tony@stapp01's password: 
 
[tony@stapp01 ~]$ sudo su - 

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for tony: 

[root@stapp01 ~]# docker ps
		CONTAINER ID   IMAGE     COMMAND   CREATED              STATUS              PORTS     NAMES
		f30b949d9b53   ubuntu    "bash"    About a minute ago   Up About a minute             ubuntu_latest

[root@stapp01 ~]# docker cp /tmp/nautilus.txt.gpg ubuntu_latest:/tmp/

[root@stapp01 ~]# docker exec ubuntu_latest ls -ahl /tmp/
		total 12K
		drwxrwxrwt  2 root root 4.0K Aug 23 13:54 .
		drwxr-xr-x 17 root root 4.0K Aug 23 13:52 ..
		-rw-r--r--  1 root root   74 Aug 23 13:51 nautilus.txt.gpg

--------------------------------------------------------------------------------------------------------------------------------
Task 77: 1/Sep/2022

 Kubernetes Sidecar Containers


We have a web server container running the nginx image. The access and error logs generated by the web server are not critical enough to be placed on a persistent volume. However, Nautilus developers need access to the last 24 hours of logs so that they can trace issues and bugs. Therefore, we need to ship the access and error logs for the web server to a log-aggregation service. Following the separation of concerns principle, we implement the Sidecar pattern by deploying a second container that ships the error and access logs from nginx. Nginx does one thing, and it does it well—serving web pages. The second container also specializes in its task—shipping logs. Since containers are running on the same Pod, we can use a shared emptyDir volume to read and write logs.

    Create a pod named webserver.

    Create an emptyDir volume shared-logs.

    Create two containers from nginx and ubuntu images with latest tag only and remember to mention tag i.e nginx:latest, nginx container name should be nginx-container and ubuntu container name should be sidecar-container on webserver pod.

    Add command on sidecar-container "sh","-c","while true; do cat /var/log/nginx/access.log /var/log/nginx/error.log; sleep 30; done"

    Mount the volume shared-logs on both containers at location /var/log/nginx, all containers should be up and running.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.


thor@jump_host ~$ kubectl get services
		NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
		kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   117m

thor@jump_host ~$ kubectl get pods
		No resources found in default namespace.

thor@jump_host ~$ vi /tmp/webserver.yaml
thor@jump_host ~$ cat /tmp/webserver.yaml
		apiVersion: v1

		kind: Pod

		metadata:

		  name: webserver

		  labels:

		    name: webserver

		spec:

		  volumes:

		    - name: shared-logs

		      emptyDir: {}

		  containers:

		    - name: nginx-container

		      image: nginx:latest

		      volumeMounts:

		        - name: shared-logs

		          mountPath: /var/log/nginx

		    - name: sidecar-container

		      image: ubuntu:latest

		      command:

		        [

		          "/bin/bash",

		          "-c",

		          "while true; do cat /var/log/nginx/access.log /var/log/nginx/error.log; sleep 30; done",

		        ]

		      volumeMounts:

		        - name: shared-logs

		          mountPath: /var/log/nginx
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl create -f /tmp/webserver.yaml 
		pod/webserver created
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get pods
		NAME        READY   STATUS              RESTARTS   AGE
		webserver   0/2     ContainerCreating   0          8s
thor@jump_host ~$ kubectl get pods
		NAME        READY   STATUS    RESTARTS   AGE
		webserver   2/2     Running   0          32s
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl describe pods webserver
		Name:         webserver
		Namespace:    default
		Priority:     0
		Node:         kodekloud-control-plane/172.17.0.2
		Start Time:   Thu, 01 Sep 2022 11:47:55 +0000
		Labels:       name=webserver
		Annotations:  <none>
		Status:       Running
		IP:           10.244.0.5
		IPs:
		  IP:  10.244.0.5
		Containers:
		  nginx-container:
		    Container ID:   containerd://5085d8516606c1b9dfe2346d7b126a097c2422a5472b53bbb044d275720e39f1
		    Image:          nginx:latest
		    Image ID:       docker.io/library/nginx@sha256:b95a99feebf7797479e0c5eb5ec0bdfa5d9f504bc94da550c2f58e839ea6914f
		    Port:           <none>
		    Host Port:      <none>
		    State:          Running
		      Started:      Thu, 01 Sep 2022 11:48:12 +0000
		    Ready:          True
		    Restart Count:  0
		    Environment:    <none>
		    Mounts:
		      /var/log/nginx from shared-logs (rw)
		      /var/run/secrets/kubernetes.io/serviceaccount from default-token-jc4dn (ro)
		  sidecar-container:
		    Container ID:  containerd://85c6684de2540bc374787b3eb3226b831a44450449a64d37e2896dbe08058f36
		    Image:         ubuntu:latest
		    Image ID:      docker.io/library/ubuntu@sha256:34fea4f31bf187bc915536831fd0afc9d214755bf700b5cdb1336c82516d154e
		    Port:          <none>
		    Host Port:     <none>
		    Command:
		      /bin/bash
		      -c
		      while true; do cat /var/log/nginx/access.log /var/log/nginx/error.log; sleep 30; done
		    State:          Running
		      Started:      Thu, 01 Sep 2022 11:48:21 +0000
		    Ready:          True
		    Restart Count:  0
		    Environment:    <none>
		    Mounts:
		      /var/log/nginx from shared-logs (rw)
		      /var/run/secrets/kubernetes.io/serviceaccount from default-token-jc4dn (ro)
		Conditions:
		  Type              Status
		  Initialized       True 
		  Ready             True 
		  ContainersReady   True 
		  PodScheduled      True 
		Volumes:
		  shared-logs:
		    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
		    Medium:     
		    SizeLimit:  <unset>
		  default-token-jc4dn:
		    Type:        Secret (a volume populated by a Secret)
		    SecretName:  default-token-jc4dn
		    Optional:    false
		QoS Class:       BestEffort
		Node-Selectors:  <none>
		Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
		                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
		Events:
		  Type    Reason     Age   From               Message
		  ----    ------     ----  ----               -------
		  Normal  Scheduled  46s   default-scheduler  Successfully assigned default/webserver to kodekloud-control-plane
		  Normal  Pulling    43s   kubelet            Pulling image "nginx:latest"
		  Normal  Pulled     30s   kubelet            Successfully pulled image "nginx:latest" in 12.903081783s
		  Normal  Created    30s   kubelet            Created container nginx-container
		  Normal  Started    29s   kubelet            Started container nginx-container
		  Normal  Pulling    29s   kubelet            Pulling image "ubuntu:latest"
		  Normal  Pulled     23s   kubelet            Successfully pulled image "ubuntu:latest" in 6.621630852s
		  Normal  Created    22s   kubelet            Created container sidecar-container
		  Normal  Started    20s   kubelet            Started container sidecar-container 

--------------------------------------------------------------------------------------------------------------------------------
Task 78: 9/Sep/2022

 Print Environment Variables

 The Nautilus DevOps team is working on to setup some pre-requisites for an application that will send the greetings to different users. There is a sample deployment, that needs to be tested. Below is a scenario which needs to be configured on Kubernetes cluster. Please find below more details about it.

    Create a pod named print-envars-greeting.

    Configure spec as, the container name should be print-env-container and use bash image.

    Create three environment variables:

a. GREETING and its value should be Welcome to

b. COMPANY and its value should be Nautilus

c. GROUP and its value should be Datacenter

    Use command to echo ["$(GREETING) $(COMPANY) $(GROUP)"] message.

    You can check the output using <kubctl logs -f [ pod-name ]> command.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

1.
thor@jump_host ~$ kubectl get services
		NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
		kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   88m

thor@jump_host ~$ kubectl get pods
		No resources found in default namespace.

2.
thor@jump_host ~$ vi /tmp/env.yml

thor@jump_host ~$ cat /tmp/env.yml
		apiVersion: v1

		kind: Pod

		metadata:

		  name: print-envars-greeting

		  labels:

		    name: print-envars-greeting

		spec:

		  containers:

		    - name: print-env-container

		      image: bash

		      env:

		        - name: GREETING

		          value: "Welcome to"

		        - name: COMPANY

		          value: "Nautilus"

		        - name: GROUP

		          value: "Datacenter"

		      command: ["echo"]

		      args: ["$(GREETING) $(COMPANY) $(GROUP)"]


3.
thor@jump_host ~$ kubectl create -f /tmp/env.yml
		pod/print-envars-greeting created

4.
thor@jump_host ~$ kubectl get pods
		NAME                    READY   STATUS      RESTARTS   AGE
		print-envars-greeting   0/1     Completed   1          14s

thor@jump_host ~$ kubectl get pods
		NAME                    READY   STATUS      RESTARTS   AGE
		print-envars-greeting   0/1     Completed   2          30s

5.
thor@jump_host ~$ kubectl logs -f print-envars-greeting
		Welcome to Nautilus Datacenter
thor@jump_host ~$


--------------------------------------------------------------------------------------------------------------------------------
Task 79: 18/Sep/2022

Manage Secrets in Kubernetes


The Nautilus DevOps team is working to deploy some tools in Kubernetes cluster. Some of the tools are licence based so that licence information needs to be stored securely within Kubernetes cluster. Therefore, the team wants to utilize Kubernetes secrets to store those secrets. Below you can find more details about the requirements:

    We already have a secret key file news.txt under /opt location on jump host. Create a generic secret named news, it should contain the password/license-number present in news.txt file.

    Also create a pod named secret-xfusion.

    Configure pod's spec as container name should be secret-container-xfusion, image should be fedora preferably with latest tag (remember to mention the tag with image). Use sleep command for container so that it remains in running state. Consume the created secret and mount it under /opt/cluster within the container.

    To verify you can exec into the container secret-container-xfusion, to check the secret key under the mounted path /opt/cluster. Before hitting the Check button please make sure pod/pods are in running state, also validation can take some time to complete so keep patience.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

1.
thor@jump_host ~$ cat /opt/news.txt 
	5ecur3

2. 
thor@jump_host ~$ kubectl create secret generic news --from-file=/opt/news.txt 
	secret/news created
 
thor@jump_host ~$ ls -ahl /opt/
	total 12K
	drwxr-xr-x 1 thor thor 4.0K Sep 18 17:35 .
	drwxr-xr-x 1 root root 4.0K Sep 18 17:35 ..
	-rw-r--r-- 1 root root    7 Sep 18 17:35 news.txt

3.
thor@jump_host ~$ vi /tmp/secret.yml
 
thor@jump_host ~$ cat /tmp/secret.yml 
		apiVersion: v1

		kind: Pod

		metadata:

		  name: secret-xfusion

		  labels:

		    name: myapp

		spec:

		  volumes:

		    - name: secret-volume-xfusion

		      secret:

		        secretName: news

		  containers:

		    - name: secret-container-xfusion

		      image: fedora:latest

		      command: ["/bin/bash", "-c", "sleep 10000"]

		      volumeMounts:

		        - name: secret-volume-xfusion

		          mountPath: /opt/cluster

		          readOnly: true

4.
thor@jump_host ~$ kubectl create -f /tmp/secret.yml 
	pod/secret-xfusion created

5.
thor@jump_host ~$ kubectl get pods 
	NAME             READY   STATUS              RESTARTS   AGE
	secret-xfusion   0/1     ContainerCreating   0          7s

thor@jump_host ~$ kubectl get pods 
	NAME             READY   STATUS              RESTARTS   AGE
	secret-xfusion   0/1     ContainerCreating   0          16s

thor@jump_host ~$ kubectl get pods 
	NAME             READY   STATUS    RESTARTS   AGE
	secret-xfusion   1/1     Running   0          78s

6.	
thor@jump_host ~$ kubectl exec secret-xfusion -- cat /opt/cluster/news.txt
	5ecur3
thor@jump_host ~$ 


--------------------------------------------------------------------------------------------------------------------------------
Task 80: 20/Sep/2022

Creating Soft Links Using Ansible


The Nautilus DevOps team is practicing some of the Ansible modules and creating and testing different Ansible playbooks to accomplish tasks. Recently they started testing an Ansible file module to create soft links on all app servers. Below you can find more details about it.

Write a playbook.yml under /home/thor/ansible directory on jump host, an inventory file is already present under /home/thor/ansible directory on jump host itself. Using this playbook accomplish below given tasks:

    Create an empty file /opt/security/blog.txt on app server 1; its user owner and group owner should be tony. Create a symbolic link of source path /opt/security to destination /var/www/html.

    Create an empty file /opt/security/story.txt on app server 2; its user owner and group owner should be steve. Create a symbolic link of source path /opt/security to destination /var/www/html.

    Create an empty file /opt/security/media.txt on app server 3; its user owner and group owner should be banner. Create a symbolic link of source path /opt/security to destination /var/www/html.

Note: Validation will try to run the playbook using command ansible-playbook -i inventory playbook.yml so please make sure playbook works this way without passing any extra arguments.


1. Verify the ansible inventory file

thor@jump_host ~$ cd /home/thor/ansible/

thor@jump_host ~/ansible$ ls -ahl
	total 16K
	drwxr-xr-x 2 thor thor 4.0K Sep 20 14:52 .
	drwxr----- 1 thor thor 4.0K Sep 20 14:52 ..
	-rw-r--r-- 1 thor thor   36 Sep 20 14:52 ansible.cfg
	-rw-r--r-- 1 thor thor  237 Sep 20 14:52 inventory
 
thor@jump_host ~/ansible$ ansible all -a "ls -ahl /opt/security" -i inventory
	stapp01 | CHANGED | rc=0 >>
	total 8.0K
	drwxr-xr-x 2 root root 4.0K Sep 20 14:52 .
	drwxr-xr-x 1 root root 4.0K Sep 20 14:52 ..
	stapp02 | CHANGED | rc=0 >>
	total 8.0K
	drwxr-xr-x 2 root root 4.0K Sep 20 14:52 .
	drwxr-xr-x 1 root root 4.0K Sep 20 14:52 ..
	stapp03 | CHANGED | rc=0 >>
	total 8.0K
	drwxr-xr-x 2 root root 4.0K Sep 20 14:52 .
	drwxr-xr-x 1 root root 4.0K Sep 20 14:52 ..

2. Create the playbook.yml file

thor@jump_host ~/ansible$ pwd
	/home/thor/ansible

thor@jump_host ~/ansible$ vi playbook.yml
 
thor@jump_host ~/ansible$ cat playbook.yml 
	- name: Create text files and create soft link

	  hosts: stapp01, stapp02, stapp03

	  become: yes

	  tasks:

	    - name: Create the blog.txt on stapp01

	      file:

	        path: /opt/security/blog.txt

	        owner: tony

	        group: tony

	        state: touch

	      when: inventory_hostname == "stapp01"

	    - name: Create the story.txt on stapp02

	      file:

	        path: /opt/security/story.txt

	        owner: steve

	        group: steve

	        state: touch

	      when: inventory_hostname == "stapp02"

	    - name: Create the media.txt on stapp03

	      file:

	        path: /opt/security/media.txt

	        owner: banner

	        group: banner

	        state: touch

	      when: inventory_hostname == "stapp03"

	    - name: Link /opt/security directory

	      file:

	        src: /opt/security/

	        dest: /var/www/html

	        state: link

3. Run the playbook	        
thor@jump_host ~/ansible$ ansible-playbook -i inventory playbook.yml 

		PLAY [Create text files and create soft link] ***********************************************************************************************

		TASK [Gathering Facts] **********************************************************************************************************************
		ok: [stapp03]
		ok: [stapp01]
		ok: [stapp02]

		TASK [Create the blog.txt on stapp01] *******************************************************************************************************
		skipping: [stapp02]
		skipping: [stapp03]
		changed: [stapp01]

		TASK [Create the story.txt on stapp02] ******************************************************************************************************
		skipping: [stapp01]
		skipping: [stapp03]
		changed: [stapp02]

		TASK [Create the media.txt on stapp03] ******************************************************************************************************
		skipping: [stapp02]
		skipping: [stapp01]
		changed: [stapp03]

		TASK [Link /opt/security directory] *********************************************************************************************************
		changed: [stapp01]
		changed: [stapp03]
		changed: [stapp02]

		PLAY RECAP **********************************************************************************************************************************
		stapp01                    : ok=3    changed=2    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0   
		stapp02                    : ok=3    changed=2    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0   
		stapp03                    : ok=3    changed=2    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0   

4. Verify the file created and soft link made.

thor@jump_host ~/ansible$ ansible all -a "ls -ahl /opt/security" -i inventory
	stapp01 | CHANGED | rc=0 >>
	total 8.0K
	drwxr-xr-x 2 root root 4.0K Sep 20 14:57 .
	drwxr-xr-x 1 root root 4.0K Sep 20 14:52 ..
	-rw-r--r-- 1 tony tony    0 Sep 20 14:57 blog.txt
	stapp03 | CHANGED | rc=0 >>
	total 8.0K
	drwxr-xr-x 2 root   root   4.0K Sep 20 14:57 .
	drwxr-xr-x 1 root   root   4.0K Sep 20 14:52 ..
	-rw-r--r-- 1 banner banner    0 Sep 20 14:57 media.txt
	stapp02 | CHANGED | rc=0 >>
	total 8.0K
	drwxr-xr-x 2 root  root  4.0K Sep 20 14:57 .
	drwxr-xr-x 1 root  root  4.0K Sep 20 14:52 ..
	-rw-r--r-- 1 steve steve    0 Sep 20 14:57 story.txt

thor@jump_host ~/ansible$ ansible all -a "ls -ahl /var/www/html" -i inventory
	stapp02 | CHANGED | rc=0 >>
	lrwxrwxrwx 1 root root 14 Sep 20 14:57 /var/www/html -> /opt/security/
	stapp01 | CHANGED | rc=0 >>
	lrwxrwxrwx 1 root root 14 Sep 20 14:57 /var/www/html -> /opt/security/
	stapp03 | CHANGED | rc=0 >>
	lrwxrwxrwx 1 root root 14 Sep 20 14:57 /var/www/html -> /opt/security/

thor@jump_host ~/ansible$

--------------------------------------------------------------------------------------------------------------------------------
Task 81: 22/Sep/2022

Setup Puppet Certs Autosign


During last weekly meeting, the Nautilus DevOps team has decided to use Puppet autosign config to auto sign the certificates for all Puppet agent nodes that they will keep adding under the Puppet master in Stratos DC. The Puppet master and CA servers are currently running on jump host and all three app servers are configured as Puppet agents. To set up autosign configuration on the Puppet master server, some configuration settings must be done. Please find below more details:

The Puppet server package is already installed on puppet master i.e jump server and the Puppet agent package is already installed on all App Servers. However, you may need to start the required services on all of these servers.

    Configure autosign configuration on the Puppet master i.e jump server (by creating an autosign.conf in the puppet configuration directory) and assign the certificates for master node as well as for the all agent nodes. Use the respective host's FDQN to assign the certificates.

    Use alias puppet (dns_alt_names) for master node and add its entry in /etc/hosts config file on master i.e Jump Server as well as on the all agent nodes i.e App Servers.

Notes: :- Please make sure to run the puppet agent test using sudo on agent nodes, otherwise you can face certificate issues. In that case you will have to clean the certificates first and then you will be able to run the puppet agent test.

:- Before clicking on the Check button please make sure to verify puppet server and puppet agent services are up and running on the respective servers, also please make sure to run puppet agent test to apply/test the changes manually first.

:- Please note that once lab is loaded, the puppet server service should start automatically on puppet master server, however it can take upto 2-3 minutes to start.


1. Switch to root user to avoid reenter password

thor@jump_host ~$ sudo su -
	We trust you have received the usual lecture from the local System
	Administrator. It usually boils down to these three things:
	    #1) Respect the privacy of others.
	    #2) Think before you type.
	    #3) With great power comes great responsibility.
	 [sudo] password for thor:
root@jump_host ~#

2. Check /etc/hosts file and add alias for puppet server to jump_hosts

root@jump_host ~# cat /etc/hosts
		127.0.0.1       localhost
		::1     localhost ip6-localhost ip6-loopback
		fe00::0 ip6-localnet
		ff00::0 ip6-mcastprefix
		ff02::1 ip6-allnodes
		ff02::2 ip6-allrouters
		172.16.238.10   stapp01.stratos.xfusioncorp.com
		172.16.238.11   stapp02.stratos.xfusioncorp.com
		172.16.238.12   stapp03.stratos.xfusioncorp.com
		172.16.238.3    jump_host.stratos.xfusioncorp.com jump_host
		172.16.239.5    jump_host.stratos.xfusioncorp.com jump_host
		172.17.0.4      jump_host.stratos.xfusioncorp.com jump_host


root@jump_host ~# ping puppet
		PING puppet.stratos.xfusioncorp.com (216.245.213.73) 56(84) bytes of data.
		64 bytes from 73-213-245-216.static.reverse.lstn.net (216.245.213.73): icmp_seq=1 ttl=57 time=15.9 ms
		64 bytes from 73-213-245-216.static.reverse.lstn.net (216.245.213.73): icmp_seq=2 ttl=57 time=15.4 ms
		64 bytes from 73-213-245-216.static.reverse.lstn.net (216.245.213.73): icmp_seq=3 ttl=57 time=15.4 ms
		^C
		--- puppet.stratos.xfusioncorp.com ping statistics ---
		10 packets transmitted, 10 received, 0% packet loss, time 9191ms
		rtt min/avg/max/mdev = 15.438/15.530/15.933/0.148 ms

root@jump_host ~# vi /etc/hosts

root@jump_host ~# cat /etc/hosts
		127.0.0.1       localhost
		::1     localhost ip6-localhost ip6-loopback
		fe00::0 ip6-localnet
		ff00::0 ip6-mcastprefix
		ff02::1 ip6-allnodes
		ff02::2 ip6-allrouters
		172.16.238.10   stapp01.stratos.xfusioncorp.com
		172.16.238.11   stapp02.stratos.xfusioncorp.com
		172.16.238.12   stapp03.stratos.xfusioncorp.com
		172.16.238.3    jump_host.stratos.xfusioncorp.com jump_host puppet                 <--------------------------
		172.16.239.5    jump_host.stratos.xfusioncorp.com jump_host
		172.17.0.4      jump_host.stratos.xfusioncorp.com jump_host

root@jump_host ~# ping puppet
		PING jump_host.stratos.xfusioncorp.com (172.16.238.3) 56(84) bytes of data.
		64 bytes from jump_host.stratos.xfusioncorp.com (172.16.238.3): icmp_seq=1 ttl=64 time=0.021 ms
		64 bytes from jump_host.stratos.xfusioncorp.com (172.16.238.3): icmp_seq=2 ttl=64 time=0.043 ms
		64 bytes from jump_host.stratos.xfusioncorp.com (172.16.238.3): icmp_seq=3 ttl=64 time=0.040 ms
		64 bytes from jump_host.stratos.xfusioncorp.com (172.16.238.3): icmp_seq=4 ttl=64 time=0.044 ms
		64 bytes from jump_host.stratos.xfusioncorp.com (172.16.238.3): icmp_seq=5 ttl=64 time=0.048 ms
		^C
		--- jump_host.stratos.xfusioncorp.com ping statistics ---
		5 packets transmitted, 5 received, 0% packet loss, time 4082ms
		rtt min/avg/max/mdev = 0.021/0.039/0.048/0.010 ms


3. Create autosign.conf configuration file and define all App Server FQDN 

root@jump_host ~# vi /etc/puppetlabs/puppet/autosign.conf

root@jump_host ~# cat /etc/puppetlabs/puppet/autosign.conf
		jump_host.stratos.xfusioncorp.com

		stapp01.stratos.xfusioncorp.com

		stapp02.stratos.xfusioncorp.com

		stapp03.stratos.xfusioncorp.com

4. Restart Puppet Server for changes to take into effect

root@jump_host ~# systemctl restart puppetserver

root@jump_host ~# systemctl status  puppetserver
		● puppetserver.service - puppetserver Service
		   Loaded: loaded (/usr/lib/systemd/system/puppetserver.service; disabled; vendor preset: disabled)
		   Active: active (running) since Fri 2022-09-23 03:00:02 UTC; 10s ago
		  Process: 13695 ExecStop=/opt/puppetlabs/server/apps/puppetserver/bin/puppetserver stop (code=exited, status=0/SUCCESS)
		  Process: 13885 ExecStart=/opt/puppetlabs/server/apps/puppetserver/bin/puppetserver start (code=exited, status=0/SUCCESS)
		 Main PID: 13946 (java)
		    Tasks: 88 (limit: 4915)
		   CGroup: /docker/f7fa4e392bae13c82fd14067c17a62828c77c574648a1fefb2cf6cf5a06e2503/system.slice/puppetserver.service
		           └─13946 /usr/bin/java -Xms512m -Xmx512m -Djruby.logger.class=com.puppetlabs.jruby_utils.jruby.Slf4jLogger -XX:OnOutOfMemoryErro...

5. Check for any certificates existing on puppet server

root@jump_host ~# puppetserver ca list --all
		Signed Certificates:
		    964369dd618d.c.argo-prod-us-east1.internal       (SHA256)  8C:19:47:FD:59:97:CE:0C:1D:DF:52:92:A3:BA:41:22:F2:3D:95:D4:38:D9:EF:85:65:9A:7B:B5:59:D5:CA:E6       alt names: ["DNS:puppet", "DNS:964369dd618d.c.argo-prod-us-east1.internal"]     authorization extensions: [pp_cli_auth: true]
		    jump_host.stratos.xfusioncorp.com                (SHA256)  54:42:58:8A:55:21:89:C0:B0:96:E0:6E:DE:DA:55:45:61:0A:D7:70:C1:4F:26:50:5C:61:DD:BB:05:0A:09:EE       alt names: ["DNS:puppet", "DNS:jump_host.stratos.xfusioncorp.com"]      authorization extensions: [pp_cli_auth: true]


6. Login to app server and switch to root  

root@jump_host ~# ssh tony@stapp01
		The authenticity of host 'stapp01 (172.16.238.10)' can't be established.
		ECDSA key fingerprint is SHA256:l8lVvUaAxJzrg3ebcyecrCOol9GJqR/es1wvGjViQpg.
		ECDSA key fingerprint is MD5:c4:ad:6a:3d:5f:49:15:14:c8:81:dc:fc:ba:3e:48:d4.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'stapp01,172.16.238.10' (ECDSA) to the list of known hosts.
		tony@stapp01's password: 

[tony@stapp01 ~]$ sudo su -

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for tony: 

7. Make changes in /etc/hosts file to include alias for puppet server

[root@stapp01 ~]# vi /etc/hosts
 
[root@stapp01 ~]# cat /etc/hosts
		127.0.0.1       localhost
		::1     localhost ip6-localhost ip6-loopback
		fe00::0 ip6-localnet
		ff00::0 ip6-mcastprefix
		ff02::1 ip6-allnodes
		ff02::2 ip6-allrouters
		172.16.238.3    jump_host.stratos.xfusioncorp.com puppet                                <--------------------------
		172.16.238.10   stapp01.stratos.xfusioncorp.com stapp01
		172.16.239.3    stapp01.stratos.xfusioncorp.com stapp01
		172.17.0.6      stapp01.stratos.xfusioncorp.com stapp01

8. Verify /etc/hosts changes 

[root@stapp01 ~]# ping puppet
		PING jump_host.stratos.xfusioncorp.com (172.16.238.3) 56(84) bytes of data.
		64 bytes from jump_host.stratos.xfusioncorp.com (172.16.238.3): icmp_seq=1 ttl=64 time=0.059 ms
		64 bytes from jump_host.stratos.xfusioncorp.com (172.16.238.3): icmp_seq=2 ttl=64 time=0.071 ms
		64 bytes from jump_host.stratos.xfusioncorp.com (172.16.238.3): icmp_seq=3 ttl=64 time=0.065 ms
		^C
		--- jump_host.stratos.xfusioncorp.com ping statistics ---
		3 packets transmitted, 3 received, 0% packet loss, time 2050ms
		rtt min/avg/max/mdev = 0.059/0.065/0.071/0.005 ms

9. Restart puppet agent on the app server  

[root@stapp01 ~]# systemctl restart puppet

[root@stapp01 ~]# systemctl status puppet
		● puppet.service - Puppet agent
		   Loaded: loaded (/usr/lib/systemd/system/puppet.service; disabled; vendor preset: disabled)
		   Active: active (running) since Fri 2022-09-23 03:03:01 UTC; 10s ago
		 Main PID: 484 (puppet)
		   CGroup: /docker/a6e6ce41b55f652076ebaa21096dd19ff744035edef2d11926e7765ca43bbdf1/system.slice/puppet.service
		           └─484 /opt/puppetlabs/puppet/bin/ruby /opt/puppetlabs/puppet/bin/puppet agent --no-daemonize

		Sep 23 03:03:01 stapp01.stratos.xfusioncorp.com systemd[1]: Converting job puppet.service/restart -> puppet.service/start
		Sep 23 03:03:01 stapp01.stratos.xfusioncorp.com systemd[1]: puppet.service: cgroup is empty
		Sep 23 03:03:01 stapp01.stratos.xfusioncorp.com systemd[1]: About to execute: /opt/puppetlabs/puppet/bin/puppet agent $PUPPET_EXTRA_...monize
		Sep 23 03:03:01 stapp01.stratos.xfusioncorp.com systemd[1]: Forked /opt/puppetlabs/puppet/bin/puppet as 484
		Sep 23 03:03:01 stapp01.stratos.xfusioncorp.com systemd[1]: puppet.service changed dead -> running
		Sep 23 03:03:01 stapp01.stratos.xfusioncorp.com systemd[1]: Job puppet.service/start finished, result=done
		Sep 23 03:03:01 stapp01.stratos.xfusioncorp.com systemd[1]: Started Puppet agent.
		Sep 23 03:03:01 stapp01.stratos.xfusioncorp.com systemd[484]: Executing: /opt/puppetlabs/puppet/bin/puppet agent --no-daemonize
		Sep 23 03:03:04 stapp01.stratos.xfusioncorp.com puppet-agent[484]: Starting Puppet client version 6.15.0
		Sep 23 03:03:08 stapp01.stratos.xfusioncorp.com puppet-agent[506]: Applied catalog in 0.07 seconds
		Hint: Some lines were ellipsized, use -l to show in full.

10. Validate by running the puppet agent

[root@stapp01 ~]# puppet agent -tv 
		Info: Using configured environment 'production'
		Info: Retrieving pluginfacts
		Info: Retrieving plugin
		Info: Retrieving locales
		Info: Caching catalog for stapp01.stratos.xfusioncorp.com
		Info: Applying configuration version '1663902221'
		Notice: Applied catalog in 0.01 seconds

11. List the certificate auto signed on puppet server jump_hosts

root@jump_host ~# puppetserver ca list --all
		Signed Certificates:
		    964369dd618d.c.argo-prod-us-east1.internal       (SHA256)  8C:19:47:FD:59:97:CE:0C:1D:DF:52:92:A3:BA:41:22:F2:3D:95:D4:38:D9:EF:85:65:9A:7B:B5:59:D5:CA:E6       alt names: ["DNS:puppet", "DNS:964369dd618d.c.argo-prod-us-east1.internal"]     authorization extensions: [pp_cli_auth: true]
		    jump_host.stratos.xfusioncorp.com                (SHA256)  54:42:58:8A:55:21:89:C0:B0:96:E0:6E:DE:DA:55:45:61:0A:D7:70:C1:4F:26:50:5C:61:DD:BB:05:0A:09:EE       alt names: ["DNS:puppet", "DNS:jump_host.stratos.xfusioncorp.com"]      authorization extensions: [pp_cli_auth: true]
		    stapp01.stratos.xfusioncorp.com                  (SHA256)  8D:B1:DC:89:72:FC:12:00:C8:9D:D3:1A:9A:A4:76:C9:EA:0E:31:E2:F5:2D:DF:F3:27:D6:32:57:EA:04:CF:34       alt names: ["DNS:stapp01.stratos.xfusioncorp.com"]

12. Repeat steps 6 to 10 for app server 2 and app server 3 

13. List the certificate auto signed on puppet server jump_hosts

root@jump_host ~# puppetserver ca list --all
		Signed Certificates:
		    964369dd618d.c.argo-prod-us-east1.internal       (SHA256)  8C:19:47:FD:59:97:CE:0C:1D:DF:52:92:A3:BA:41:22:F2:3D:95:D4:38:D9:EF:85:65:9A:7B:B5:59:D5:CA:E6       alt names: ["DNS:puppet", "DNS:964369dd618d.c.argo-prod-us-east1.internal"]     authorization extensions: [pp_cli_auth: true]
		    jump_host.stratos.xfusioncorp.com                (SHA256)  54:42:58:8A:55:21:89:C0:B0:96:E0:6E:DE:DA:55:45:61:0A:D7:70:C1:4F:26:50:5C:61:DD:BB:05:0A:09:EE       alt names: ["DNS:puppet", "DNS:jump_host.stratos.xfusioncorp.com"]      authorization extensions: [pp_cli_auth: true]
		    stapp02.stratos.xfusioncorp.com                  (SHA256)  92:66:53:C5:67:D9:AE:18:E9:36:C2:7D:FF:38:FA:BE:60:12:CF:63:42:1C:C6:E8:E1:8C:2E:DC:3C:8F:B8:FC       alt names: ["DNS:stapp02.stratos.xfusioncorp.com"]
		    stapp03.stratos.xfusioncorp.com                  (SHA256)  9B:37:00:61:23:4A:55:7A:68:D1:9E:F6:6D:40:E8:73:18:8E:19:2E:B8:33:6E:3A:D0:BD:75:B0:9B:19:1F:88       alt names: ["DNS:stapp03.stratos.xfusioncorp.com"]
		    stapp01.stratos.xfusioncorp.com                  (SHA256)  8D:B1:DC:89:72:FC:12:00:C8:9D:D3:1A:9A:A4:76:C9:EA:0E:31:E2:F5:2D:DF:F3:27:D6:32:57:EA:04:CF:34       alt names: ["DNS:stapp01.stratos.xfusioncorp.com"]

--------------------------------------------------------------------------------------------------------------------------------
Task 82: 27/Sep/2022

Deploy Jenkins on Kubernetes

The Nautilus DevOps team is planning to set up a Jenkins CI server to create/manage some deployment pipelines for some of the projects. They want to set up the Jenkins server on Kubernetes cluster. Below you can find more details about the task:

1) Create a namespace jenkins

2) Create a Service for jenkins deployment. Service name should be jenkins-service under jenkins namespace, type should be NodePort, nodePort should be 30008

3) Create a Jenkins Deployment under jenkins namespace, It should be name as jenkins-deployment , labels app should be jenkins , container name should be jenkins-container , use jenkins/jenkins image , containerPort should be 8080 and replicas count should be 1.

Make sure to wait for the pods to be in running state and make sure you are able to access the Jenkins login screen in the browser before hitting the Check button.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.


1. Check kubectl utility configuration and working on jump_host
thor@jump_host ~$ kubectl get namespace
		NAME                 STATUS   AGE
		default              Active   4h50m
		kube-node-lease      Active   4h50m
		kube-public          Active   4h50m
		kube-system          Active   4h50m
		local-path-storage   Active   4h49m
 
thor@jump_host ~$ kubectl get service
		NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
		kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4h50m

2. Create namespace  and verify
thor@jump_host ~$ kubectl create namespace jenkins
		namespace/jenkins created

thor@jump_host ~$ kubectl get namespace
		NAME                 STATUS   AGE
		default              Active   4h50m
		jenkins              Active   4s 															<-----------------------
		kube-node-lease      Active   4h50m
		kube-public          Active   4h50m
		kube-system          Active   4h50m
		local-path-storage   Active   4h50m

3. Create jenkins.yaml file as per requirement 
thor@jump_host ~$ vi /tmp/jenkins.yaml

thor@jump_host ~$ cat /tmp/jenkins.yaml
		apiVersion: v1

		kind: Service

		metadata:

		  name: jenkins-service

		  namespace: jenkins

		spec:

		  type: NodePort

		  selector:

		    app: jenkins

		  ports:

		    - port: 8080

		      targetPort: 8080

		      nodePort: 30008

		---

		apiVersion: apps/v1

		kind: Deployment

		metadata:

		  name: jenkins-deployment

		  namespace: jenkins

		  labels:

		    app: jenkins

		spec:

		  replicas: 1

		  selector:

		    matchLabels:

		      app: jenkins

		  template:

		    metadata:

		      labels:

		        app: jenkins

		    spec:

		      containers:

		        - name: jenkins-container

		          image: jenkins/jenkins

		          ports:

		            - containerPort: 8080

4. Create pod
thor@jump_host ~$ kubectl create -f /tmp/jenkins.yaml 
		service/jenkins-service created
		deployment.apps/jenkins-deployment created

5. Wait for deployment and pods  to running status
thor@jump_host ~$ kubectl get deploy -n jenkins 
		NAME                 READY   UP-TO-DATE   AVAILABLE   AGE
		jenkins-deployment   0/1     1            0           23s
 
thor@jump_host ~$ kubectl get deploy -n jenkins 
		NAME                 READY   UP-TO-DATE   AVAILABLE   AGE
		jenkins-deployment   1/1     1            1           39s

thor@jump_host ~$ kubectl get pods -n jenkins 
		NAME                                  READY   STATUS    RESTARTS   AGE
		jenkins-deployment-6b6c78f968-vw882   1/1     Running   0          48s
 
thor@jump_host ~$ kubectl get service -n jenkins 
		NAME              TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE
		jenkins-service   NodePort   10.96.50.65   <none>        8080:30008/TCP   58s

6. Validate by executing curl against the pod or Open Port on Host1
thor@jump_host ~$ kubectl exec jenkins-deployment-6b6c78f968-vw882 -n jenkins -- curl http://localhost:8080
		  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
		                                 Dload  Upload   Total   Spent    Left  Speed
		100   541  100   541    0     0   1176      0 --:--:-- --:--:-- --:--:--  1176
		<html><head><meta http-equiv='refresh' content='1;url=/login?from=%2F'/><script>window.location.replace('/login?from=%2F');</script></head><body style='background-color:white; color:white;'>


		Authentication required
		<!--
		-->

		</body></html>                                                                                                                                                                      
thor@jump_host ~$ 

--------------------------------------------------------------------------------------------------------------------------------
Task 83: 29/Sep/2022

 Puppet Setup SSH Keys

 The Puppet master and Puppet agent nodes have been set up by the Nautilus DevOps team to perform some testing. In Stratos DC all app servers have been configured as Puppet agent nodes. They want to setup a password less SSH connection between Puppet master and Puppet agent nodes and this task needs to be done using Puppet itself. Below are details about the task:

Create a Puppet programming file games.pp under /etc/puppetlabs/code/environments/production/manifests directory on the Puppet master node i.e on Jump Server. Define a class ssh_node1 for agent node 1 i.e App Server 1, ssh_node2 for agent node 2 i.e App Server 2, ssh_node3 for agent node3 i.e App Server 3. You will need to generate a new ssh key for thor user on Jump Server, that needs to be added on all App Servers.

Configure a password less SSH connection from puppet master i.e jump host to all App Servers. However, please make sure the key is added to the authorized_keys file of each app's sudo user (i.e tony for App Server 1).

Notes: :- Before clicking on the Check button please make sure to verify puppet server and puppet agent services are up and running on the respective servers, also please make sure to run puppet agent test to apply/test the changes manually first.

:- Please note that once lab is loaded, the puppet server service should start automatically on puppet master server, however it can take upto 2-3 minutes to start.


1. Generate ssh key-pair for user thor on jump_host and check public key

thor@jump_host ~$ ssh-keygen
		Generating public/private rsa key pair.
		Enter file in which to save the key (/home/thor/.ssh/id_rsa): 
		Enter passphrase (empty for no passphrase): 
		Enter same passphrase again: 
		Your identification has been saved in /home/thor/.ssh/id_rsa.
		Your public key has been saved in /home/thor/.ssh/id_rsa.pub.
		The key fingerprint is:
		SHA256:zo/J8SxD46ejJSHPlSaDUX5GE69oTZyDGRjyyhbchtI thor@jump_host.stratos.xfusioncorp.com
		The key's randomart image is:
		+---[RSA 2048]----+
		|  . .oo +.       |
		| o =.o * +       |
		|. E = + B .      |
		| o + o * +       |
		|  + o * S        |
		| .   = Oo        |
		|      +o=.       |
		|       +=B.      |
		|      ..=*+      |
		+----[SHA256]-----+

thor@jump_host ~$ cat /home/thor/.ssh/id_rsa.pub 
		ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDRFnX7CXiMsSTQ77+9hZMRGtiiCGygqeS1I/qmmj0N6AoyvJypyJRBiTqGhtxMiTgEAp2Ju0KS/LfW2EKTObLFQ7AH1GGvckp4v0NXoY0TgxINKTPm8sD7/Z9Neiz1i2uOz+GvCbTvssrSyuo9jQxPAxJHowaVP+AIQW9XwrnlB0b4YzVGXp/eCugwGJfqzU4HN/lSR/UMwOiKjHvJf8VvhXTkehqI97wfqptkt7WvQ6crHmgor6SDmfAk7+9fNxHqxgQK9QiTHtnI8kjb4JIbNJvUIegjCx/l5Q2FgU5g/ETylmpPkG1ViTR844ICaVyo2Q3JPYsjGVt/XQzCdjor thor@jump_host.stratos.xfusioncorp.com


2. Create puppet programming file in the mentioned folder
thor@jump_host ~$ sudo su -

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for thor: 
 
root@jump_host ~# cd /etc/puppetlabs/code/environments/production/manifests/

root@jump_host /etc/puppetlabs/code/environments/production/manifests# ls -ahl
		total 8.0K
		drwxr-xr-x 1 puppet puppet 4.0K Jul 13  2021 .
		drwxr-xr-x 1 puppet puppet 4.0K Aug  9  2021 ..

root@jump_host /etc/puppetlabs/code/environments/production/manifests# vi games.pp

root@jump_host /etc/puppetlabs/code/environments/production/manifests# cat games.pp

		$public_key =  'AAAAB3NzaC1yc2EAAAADAQABAAABAQDRFnX7CXiMsSTQ77+9hZMRGtiiCGygqeS1I/qmmj0N6AoyvJypyJRBiTqGhtxMiTgEAp2Ju0KS/LfW2EKTObLFQ7AH1GGvckp4v0NXoY0TgxINKTPm8sD7/Z9Neiz1i2uOz+GvCbTvssrSyuo9jQxPAxJHowaVP+AIQW9XwrnlB0b4YzVGXp/eCugwGJfqzU4HN/lSR/UMwOiKjHvJf8VvhXTkehqI97wfqptkt7WvQ6crHmgor6SDmfAk7+9fNxHqxgQK9QiTHtnI8kjb4JIbNJvUIegjCx/l5Q2FgU5g/ETylmpPkG1ViTR844ICaVyo2Q3JPYsjGVt/XQzCdjor'

		class ssh_node1 {

		   ssh_authorized_key { 'tony@stapp01':

		     ensure => present,

		    user   => 'tony',

		     type   => 'ssh-rsa',

		     key    => $public_key,

		   }

		 }

		 class ssh_node2 {

		   ssh_authorized_key { 'steve@stapp02':

		     ensure => present,

		     user   => 'steve',

		     type   => 'ssh-rsa',

		     key    => $public_key,

		   }

		 }

		 class ssh_node3 {

		   ssh_authorized_key { 'banner@stapp03':

		     ensure => present,

		     user   => 'banner',

		     type   => 'ssh-rsa',

		     key    => $public_key,

		   }

		 }

		 node stapp01.stratos.xfusioncorp.com {

		   include ssh_node1

		 }

		 node stapp02.stratos.xfusioncorp.com {

		   include ssh_node2

		 }

		 node stapp03.stratos.xfusioncorp.com {

		   include ssh_node3

		 }

3. Validate the puppet file
root@jump_host /etc/puppetlabs/code/environments/production/manifests# puppet parser validate games.pp
 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# exit
		logout

4. Login to all app server(stapp01,stapp02,stapp03) and switch to root 
thor@jump_host ~$ ssh tony@stapp01
		The authenticity of host 'stapp01 (172.16.238.10)' can't be established.
		ECDSA key fingerprint is SHA256:6lM6dokqSC7w9D0MiUtgwL6o80mug/nLEa/HkCD2yg8.
		ECDSA key fingerprint is MD5:51:e4:3f:d8:50:89:9b:c0:8b:a1:50:33:fc:5d:8d:aa.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'stapp01,172.16.238.10' (ECDSA) to the list of known hosts.
		tony@stapp01's password: 
 
[tony@stapp01 ~]$ sudo su -

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for tony: 

5. Run the puppet agent to pull the configuration from puppet master server
[root@stapp01 ~]# puppet agent -tv
		Info: Using configured environment 'production'
		Info: Retrieving pluginfacts
		Info: Retrieving plugin
		Info: Retrieving locales
		Info: Caching catalog for stapp01.stratos.xfusioncorp.com
		Info: Applying configuration version '1664451362'
		Notice: /Stage[main]/Ssh_node1/Ssh_authorized_key[tony@stapp01]/ensure: created
		Notice: Applied catalog in 0.21 seconds

[root@stapp01 ~]# exit
		logout
[tony@stapp01 ~]$ exit
		logout
		Connection to stapp01 closed.
 
6. Validate by login on app server without password

thor@jump_host ~$ ssh tony@stapp01
		Last login: Thu Sep 29 11:35:32 2022 from jump_host.stratos.xfusioncorp.com
[tony@stapp01 ~]$ exit
		logout
		Connection to stapp01 closed.

thor@jump_host ~$ ssh steve@stapp02
		Last login: Thu Sep 29 11:36:32 2022 from jump_host.stratos.xfusioncorp.com
[steve@stapp02 ~]$ exit
		logout
		Connection to stapp02 closed.

thor@jump_host ~$ ssh banner@stapp03
		Last login: Thu Sep 29 11:37:17 2022 from jump_host.stratos.xfusioncorp.com
[banner@stapp03 ~]$ exit
		logout
		Connection to stapp03 closed.

thor@jump_host ~$
--------------------------------------------------------------------------------------------------------------------------------
Task 84: 03/Oct/2022

Kubernetes Redis Deployment

The Nautilus application development team observed some performance issues with one of the application that is deployed in Kubernetes cluster. After looking into number of factors, the team has suggested to use some in-memory caching utility for DB service. After number of discussions, they have decided to use Redis. Initially they would like to deploy Redis on kubernetes cluster for testing and later they will move it to production. Please find below more details about the task:

Create a redis deployment with following parameters:

    Create a config map called my-redis-config having maxmemory 2mb in redis-config.

    Name of the deployment should be redis-deployment, it should use redis:alpine image and container name should be redis-container. Also make sure it has only 1 replica.

    The container should request for 1 CPU.

    Mount 2 volumes:

a. An Empty directory volume called data at path /redis-master-data.

b. A configmap volume called redis-config at path /redis-master.

c. The container should expose the port 6379.

    Finally, redis-deployment should be in an up and running state.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

1. Check existing running pods and services
thor@jump_host ~$ kubectl get all
		NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
		service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   68m

2. Create a YAML file as per required configuration 
thor@jump_host ~$ vi /tmp/redis.yml

thor@jump_host ~$ cat /tmp/redis.yml 
		---
		kind: ConfigMap
		apiVersion: v1
		metadata:
		  name: my-redis-config
		data:
		  maxmemory: 2mb
		---
		apiVersion: apps/v1
		kind: Deployment
		metadata:
		  name: redis-deployment
		spec:
		  replicas: 1
		  selector:
		    matchLabels:
		      app: redis
		  template:
		    metadata:
		      labels:
		        app: redis
		    spec:
		      containers:
		        - name: redis-container
		          image: redis:alpine
		          ports:
		            - containerPort: 6379
		          resources:
		            requests:
		              cpu: "1000m"
		          volumeMounts:
		            - mountPath: /redis-master-data
		              name: data
		            - mountPath: /redis-master
		              name: redis-config
		      volumes:
		      - name: data
		        emptyDir: {}
		      - name: redis-config
		        configMap:
		          name: my-redis-config

3. Create redis deployment using the yaml file 
thor@jump_host ~$ kubectl create -f /tmp/redis.yml 
		configmap/my-redis-config created
		deployment.apps/redis-deployment created

4. Wait for pods and deployment to running status
thor@jump_host ~$ kubectl get all
		NAME                                    READY   STATUS    RESTARTS   AGE
		pod/redis-deployment-544d696886-nmqbl   1/1     Running   0          6s

		NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
		service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   69m

		NAME                               READY   UP-TO-DATE   AVAILABLE   AGE
		deployment.apps/redis-deployment   1/1     1            1           6s

		NAME                                          DESIRED   CURRENT   READY   AGE
		replicaset.apps/redis-deployment-544d696886   1         1         1       6s

5. Verify configmap
thor@jump_host ~$ kubectl get configmap
		NAME               DATA   AGE
		kube-root-ca.crt   1      69m
		my-redis-config    1      26s

thor@jump_host ~$ kubectl describe configmap my-redis-config
		Name:         my-redis-config
		Namespace:    default
		Labels:       <none>
		Annotations:  <none>

		Data
		====
		maxmemory:
		----
		2mb
		Events:  <none>

--------------------------------------------------------------------------------------------------------------------------------
Task 85: 05/Oct/2022

Puppet Setup NTP Server

While troubleshooting one of the issues on app servers in Stratos Datacenter DevOps team identified the root cause that the time isn't synchronized properly among the all app servers which causes issues sometimes. So team has decided to use a specific time server for all app servers, so that they all remain in sync. This task needs to be done using Puppet so as per details mentioned below please compete the task:

    Create a puppet programming file apps.pp under /etc/puppetlabs/code/environments/production/manifests directory on puppet master node i.e on Jump Server. Within the programming file define a custom class ntpconfig to install and configure ntp server on app server 1.

    Add NTP Server server 1.africa.pool.ntp.org in default configuration file on app server 1, also remember to use iburst option for faster synchronization at startup.

    Please note that do not try to start/restart/stop ntp service, as we already have a scheduled restart for this service tonight and we don't want these changes to be applied right now.

Notes: :- Please make sure to run the puppet agent test using sudo on agent nodes, otherwise you can face certificate issues. In that case you will have to clean the certificates first and then you will be able to run the puppet agent test.

:- Before clicking on the Check button please make sure to verify puppet server and puppet agent services are up and running on the respective servers, also please make sure to run puppet agent test to apply/test the changes manually first.

:- Please note that once lab is loaded, the puppet server service should start automatically on puppet master server, however it can take upto 2-3 minutes to start.



1. Switch to root , list all puppet module and if not found install ntpd module
thor@jump_host ~$ sudo su -

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for thor: 
 
root@jump_host ~# puppet module list
		/etc/puppetlabs/code/environments/production/modules (no modules installed)
		/etc/puppetlabs/code/modules (no modules installed)
		/opt/puppetlabs/puppet/modules (no modules installed)

root@jump_host ~# puppet module install puppetlabs-ntp
		Notice: Preparing to install into /etc/puppetlabs/code/environments/production/modules ...
		Notice: Downloading from https://forgeapi.puppet.com ...
		Notice: Installing -- do not interrupt ...
		/etc/puppetlabs/code/environments/production/modules
		└─┬ puppetlabs-ntp (v9.2.0)
		  └── puppetlabs-stdlib (v8.4.0)

root@jump_host ~# puppet module list
		/etc/puppetlabs/code/environments/production/modules
		├── puppetlabs-ntp (v9.2.0)
		└── puppetlabs-stdlib (v8.4.0)
		/etc/puppetlabs/code/modules (no modules installed)
		/opt/puppetlabs/puppet/modules (no modules installed)

2. Create the puppet programming file as per requirement
root@jump_host ~# cd /etc/puppetlabs/code/environments/production/manifests

root@jump_host /etc/puppetlabs/code/environments/production/manifests# ls -ahl
		total 8.0K
		drwxr-xr-x 1 puppet puppet 4.0K Jul 13  2021 .
		drwxr-xr-x 1 puppet puppet 4.0K Aug  9  2021 ..

root@jump_host /etc/puppetlabs/code/environments/production/manifests# vi apps.pp

root@jump_host /etc/puppetlabs/code/environments/production/manifests# cat apps.pp
		class { 'ntp':

		  servers => [ 'server 1.africa.pool.ntp.org iburst' ],                                               

		}    

		class ntpconfig {

		  include ntp

		}  

		node 'stapp01.stratos.xfusioncorp.com' {

		  include ntpconfig

		}

		node 'stapp02.stratos.xfusioncorp.com' {

		  include ntpconfig

		}

		node 'stapp03.stratos.xfusioncorp.com' {

		  include ntpconfig

		}

3. Validate the puppet file		
root@jump_host /etc/puppetlabs/code/environments/production/manifests# puppet parser validate apps.pp


4. Login to app servers 1,2 and 3 and switch to root users 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# ssh tony@stapp01
		The authenticity of host 'stapp01 (172.16.238.10)' can't be established.
		ECDSA key fingerprint is SHA256:6/+jfJXDk6LzuxE5Dp95v1rwgND7rHqTZfBXDoY61WE.
		ECDSA key fingerprint is MD5:d8:fb:8c:3b:92:f0:5a:82:bf:6f:d8:ab:6a:c6:a4:e4.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'stapp01,172.16.238.10' (ECDSA) to the list of known hosts.
		tony@stapp01's password: 

[tony@stapp01 ~]$ sudo su -

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for tony:

5. Check ntpd service running 		 
[root@stapp01 ~]# puppet resource service ntpd
		service { 'ntpd':
		  ensure   => 'stopped',
		  enable   => 'false',
		  provider => 'systemd',
		}
 
6. Run Puppet agent to pull the configuration from puppet server  
[root@stapp01 ~]# puppet agent -tv
		Info: Using configured environment 'production'
		Info: Retrieving pluginfacts
		Info: Retrieving plugin
		Info: Retrieving locales
		Info: Loading facts
		Info: Caching catalog for stapp01.stratos.xfusioncorp.com
		Info: Applying configuration version '1664992996'
		Notice: /Stage[main]/Ntp::Install/Package[ntp]/ensure: created
		Notice: /Stage[main]/Ntp::Config/File[/etc/ntp.conf]/content: 
		--- /etc/ntp.conf       2019-11-27 16:47:41.000000000 +0000
		+++ /tmp/puppet-file20221005-572-vd56vb 2022-10-05 18:03:52.763950667 +0000
		@@ -1,58 +1,30 @@
		-# For more information about this file, see the man pages
		-# ntp.conf(5), ntp_acc(5), ntp_auth(5), ntp_clock(5), ntp_misc(5), ntp_mon(5).
		+# ntp.conf: Managed by puppet.
		+#
		+# Enable next tinker options:
		+# panic - keep ntpd from panicking in the event of a large clock skew
		+# when a VM guest is suspended and resumed;
		+# stepout - allow ntpd change offset faster
		+tinker panic 0
		+disable monitor
		 
		-driftfile /var/lib/ntp/drift
		+statsdir /var/log/ntpstats
		 
		 # Permit time synchronization with our time source, but do not
		 # permit the source to query or modify the service on this system.
		-restrict default nomodify notrap nopeer noquery
		-
		-# Permit all access over the loopback interface.  This could
		-# be tightened as well, but to do so would effect some of
		-# the administrative functions.
		-restrict 127.0.0.1 
		-restrict ::1
		-
		-# Hosts on local network are less restricted.
		-#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap
		-
		-# Use public servers from the pool.ntp.org project.
		-# Please consider joining the pool (http://www.pool.ntp.org/join.html).
		-server 0.centos.pool.ntp.org iburst
		-server 1.centos.pool.ntp.org iburst
		-server 2.centos.pool.ntp.org iburst
		-server 3.centos.pool.ntp.org iburst
		-
		-#broadcast 192.168.1.255 autokey       # broadcast server
		-#broadcastclient                       # broadcast client
		-#broadcast 224.0.1.1 autokey           # multicast server
		-#multicastclient 224.0.1.1             # multicast client
		-#manycastserver 239.255.254.254                # manycast server
		-#manycastclient 239.255.254.254 autokey # manycast client
		-
		-# Enable public key cryptography.
		-#crypto
		-
		-includefile /etc/ntp/crypto/pw
		+restrict default kod nomodify notrap nopeer noquery
		+restrict -6 default kod nomodify notrap nopeer noquery
		+restrict 127.0.0.1
		+restrict -6 ::1
		+
		+# Set up servers for ntpd with next options:
		+# server - IP address or DNS name of upstream NTP server
		+# burst - send a burst of eight packets instead of the usual one.
		+# iburst - allow send sync packages faster if upstream unavailable
		+# prefer - select preferrable server
		+# minpoll - set minimal update frequency
		+# maxpoll - set maximal update frequency
		+# noselect - do not sync with this server
		+server server 1.africa.pool.ntp.org iburst
		 
		-# Key file containing the keys and key identifiers used when operating
		-# with symmetric key cryptography. 
		-keys /etc/ntp/keys
		-
		-# Specify the key identifiers which are trusted.
		-#trustedkey 4 8 42
		-
		-# Specify the key identifier to use with the ntpdc utility.
		-#requestkey 8
		-
		-# Specify the key identifier to use with the ntpq utility.
		-#controlkey 8
		-
		-# Enable writing of statistics records.
		-#statistics clockstats cryptostats loopstats peerstats
		-
		-# Disable the monitoring facility to prevent amplification attacks using ntpdc
		-# monlist command when default restrict does not include the noquery flag. See
		-# CVE-2013-5211 for more details.
		-# Note: Monitoring will not be disabled with the limited restriction flag.
		-disable monitor
		+# Driftfile.
		+driftfile /var/lib/ntp/drift

		Info: Computing checksum on file /etc/ntp.conf
		Info: /Stage[main]/Ntp::Config/File[/etc/ntp.conf]: Filebucketed /etc/ntp.conf to puppet with sum dc9e5754ad2bb6f6c32b954c04431d0a
		Notice: /Stage[main]/Ntp::Config/File[/etc/ntp.conf]/content: content changed '{md5}dc9e5754ad2bb6f6c32b954c04431d0a' to '{md5}3ead0e8724aba2f2dccd54df6fb0bed6'
		Notice: /Stage[main]/Ntp::Config/File[/etc/ntp/step-tickers]/content: 
		--- /etc/ntp/step-tickers       2019-11-27 16:47:41.000000000 +0000
		+++ /tmp/puppet-file20221005-572-1oe06ym        2022-10-05 18:03:52.859957523 +0000
		@@ -1,3 +1,3 @@
		 # List of NTP servers used by the ntpdate service.
		 
		-0.centos.pool.ntp.org
		+server 1.africa.pool.ntp.org iburst																																				<-----------------------------

		Info: Computing checksum on file /etc/ntp/step-tickers
		Info: /Stage[main]/Ntp::Config/File[/etc/ntp/step-tickers]: Filebucketed /etc/ntp/step-tickers to puppet with sum 9b77b3b3eb41daf0b9abb8ed01c5499b
		Notice: /Stage[main]/Ntp::Config/File[/etc/ntp/step-tickers]/content: content changed '{md5}9b77b3b3eb41daf0b9abb8ed01c5499b' to '{md5}f4c913e757420b9f50c2fbd38715cbc5'
		Info: Class[Ntp::Config]: Scheduling refresh of Class[Ntp::Service]
		Info: Class[Ntp::Service]: Scheduling refresh of Service[ntp]
		Notice: /Stage[main]/Ntp::Service/Service[ntp]/ensure: ensure changed 'stopped' to 'running'								<---------------------------------
		Info: /Stage[main]/Ntp::Service/Service[ntp]: Unscheduling refresh on Service[ntp]
		Notice: Applied catalog in 19.25 seconds

7. Validate the task by checking resource service ntpd
[root@stapp01 ~]# puppet resource service ntpd
		service { 'ntpd':
		  ensure   => 'running',
		  enable   => 'true',
		  provider => 'systemd',
		}

[root@stapp01 ~]# exit
		logout

[tony@stapp01 ~]$ exit
		logout
		Connection to stapp01 closed.

8. Repeat the tasks for App server 2 and app server 3.		

--------------------------------------------------------------------------------------------------------------------------------
Task 86: 07/Oct/2022

Using Ansible Conditionals


The Nautilus DevOps team had a discussion about, how they can train different team members to use Ansible for different automation tasks. There are numerous ways to perform a particular task using Ansible, but we want to utilize each aspect that Ansible offers. The team wants to utilise Ansible's conditionals to perform the following task:

An inventory file is already placed under /home/thor/ansible directory on jump host, with all the Stratos DC app servers included.

Create a playbook /home/thor/ansible/playbook.yml and make sure to use Ansible's when conditionals statements to perform the below given tasks.

    Copy blog.txt file present under /usr/src/devops directory on jump host to App Server 1 under /opt/devops directory. Its user and group owner must be user tony and its permissions must be 0777 .

    Copy story.txt file present under /usr/src/devops directory on jump host to App Server 2 under /opt/devops directory. Its user and group owner must be user steve and its permissions must be 0777 .

    Copy media.txt file present under /usr/src/devops directory on jump host to App Server 3 under /opt/devops directory. Its user and group owner must be user banner and its permissions must be 0777 .

NOTE: You can use ansible_nodename variable from gathered facts with when condition. Additionally, please make sure you are running the play for all hosts i.e use - hosts: all.

Note: Validation will try to run the playbook using command ansible-playbook -i inventory playbook.yml, so please make sure the playbook works this way without passing any extra arguments.

1. Check the inventory file and run ansible to verify.
thor@jump_host ~$ cd /home/thor/ansible/

thor@jump_host ~/ansible$ ls -ahl
		total 16K
		drwxr-xr-x 2 thor thor 4.0K Oct  7 10:25 .
		drwxr----- 1 thor thor 4.0K Oct  7 10:25 ..
		-rw-r--r-- 1 thor thor   36 Oct  7 10:25 ansible.cfg
		-rw-r--r-- 1 thor thor  237 Oct  7 10:25 inventory

thor@jump_host ~/ansible$ cat inventory 
		stapp01 ansible_host=172.16.238.10 ansible_ssh_pass=Ir0nM@n ansible_user=tony
		stapp02 ansible_host=172.16.238.11 ansible_ssh_pass=Am3ric@ ansible_user=steve
		stapp03 ansible_host=172.16.238.12 ansible_ssh_pass=BigGr33n ansible_user=banner

thor@jump_host ~/ansible$ ansible -i inventory all -a "ls -ahl /opt/devops"
		stapp03 | CHANGED | rc=0 >>
		total 8.0K
		drwxr-xr-x 2 root root 4.0K Oct  7 10:25 .
		drwxr-xr-x 1 root root 4.0K Oct  7 10:25 ..
		stapp01 | CHANGED | rc=0 >>
		total 8.0K
		drwxr-xr-x 2 root root 4.0K Oct  7 10:25 .
		drwxr-xr-x 1 root root 4.0K Oct  7 10:25 ..
		stapp02 | CHANGED | rc=0 >>
		total 8.0K
		drwxr-xr-x 2 root root 4.0K Oct  7 10:25 .
		drwxr-xr-x 1 root root 4.0K Oct  7 10:25 ..

2. Verify source files
thor@jump_host ~/ansible$ ls -ahl /usr/src/devops
		total 20K
		drwxr-xr-x 2 root root 4.0K Oct  7 10:26 .
		drwxr-xr-x 1 root root 4.0K Oct  7 10:25 ..
		-rw-r--r-- 1 root root   35 Oct  7 10:25 blog.txt
		-rw-r--r-- 1 root root   22 Oct  7 10:25 media.txt
		-rw-r--r-- 1 root root   27 Oct  7 10:25 story.txt

3. Create playbook as per given requirement
thor@jump_host ~/ansible$ pwd
		/home/thor/ansible

thor@jump_host ~/ansible$ vi playbook.yml
 
thor@jump_host ~/ansible$ cat playbook.yml 
		- name: Copy text files to Appservers

		  hosts: all

		  become: yes

		  tasks:

		    - name: Copy blog.txt to stapp01

		      ansible.builtin.copy:

		        src: /usr/src/devops/blog.txt

		        dest: /opt/devops/

		        owner: tony

		        group: tony

		        mode: "0777"

		      when: inventory_hostname == "stapp01"

		    - name: Copy story.txt to stapp02

		      ansible.builtin.copy:

		        src: /usr/src/devops/story.txt

		        dest: /opt/devops/

		        owner: steve

		        group: steve

		        mode: "0777"

		      when: inventory_hostname == "stapp02"

		    - name: Copy media.txt to stapp03

		      ansible.builtin.copy:

		        src: /usr/src/devops/media.txt

		        dest: /opt/devops/

		        owner: banner

		        group: banner

		        mode: "0777"

		      when: inventory_hostname == "stapp03"

4. Run/execute the playbook
thor@jump_host ~/ansible$ ansible-playbook -i inventory playbook.yml 

		PLAY [Copy text files to Appservers] ********************************************************************************************************

		TASK [Gathering Facts] **********************************************************************************************************************
		ok: [stapp03]
		ok: [stapp02]
		ok: [stapp01]

		TASK [Copy blog.txt to stapp01] *************************************************************************************************************
		skipping: [stapp02]
		skipping: [stapp03]
		changed: [stapp01]

		TASK [Copy story.txt to stapp02] ************************************************************************************************************
		skipping: [stapp03]
		skipping: [stapp01]
		changed: [stapp02]

		TASK [Copy media.txt to stapp03] ************************************************************************************************************
		skipping: [stapp01]
		skipping: [stapp02]
		changed: [stapp03]

		PLAY RECAP **********************************************************************************************************************************
		stapp01                    : ok=2    changed=1    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0   
		stapp02                    : ok=2    changed=1    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0   
		stapp03                    : ok=2    changed=1    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0   

5. Verify the changes on App servers
thor@jump_host ~/ansible$ ansible -i inventory all -a "ls -ahl /opt/devops"
		stapp03 | CHANGED | rc=0 >>
		total 12K
		drwxr-xr-x 2 root   root   4.0K Oct  7 10:31 .
		drwxr-xr-x 1 root   root   4.0K Oct  7 10:25 ..
		-rwxrwxrwx 1 banner banner   22 Oct  7 10:31 media.txt
		stapp02 | CHANGED | rc=0 >>
		total 12K
		drwxr-xr-x 2 root  root  4.0K Oct  7 10:31 .
		drwxr-xr-x 1 root  root  4.0K Oct  7 10:25 ..
		-rwxrwxrwx 1 steve steve   27 Oct  7 10:31 story.txt
		stapp01 | CHANGED | rc=0 >>
		total 12K
		drwxr-xr-x 2 root root 4.0K Oct  7 10:31 .
		drwxr-xr-x 1 root root 4.0K Oct  7 10:25 ..
		-rwxrwxrwx 1 tony tony   35 Oct  7 10:31 blog.txt

--------------------------------------------------------------------------------------------------------------------------------
Task 87: 08/Oct/2022

 Deploy Node App on Kubernetes

The Nautilus development team has completed development of one of the node applications, which they are planning to deploy on a Kubernetes cluster. They recently had a meeting with the DevOps team to share their requirements. Based on that, the DevOps team has listed out the exact requirements to deploy the app. Find below more details:

    Create a deployment using gcr.io/kodekloud/centos-ssh-enabled:node image, replica count must be 2.

    Create a service to expose this app, the service type must be NodePort, targetPort must be 8080 and nodePort should be 30012.

    Make sure all the pods are in Running state after the deployment.

    You can check the application by clicking on NodeApp button on top bar.

You can use any labels as per your choice.

Note: The kubectl on jump_host has been configured to work with the kubernetes cluster.


1. Check kubectl utility configuration and working on jump_host
thor@jump_host ~$ kubectl get all
		NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
		service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   51m

thor@jump_host ~$ kubectl get namespace
		NAME                 STATUS   AGE
		default              Active   55m
		kube-node-lease      Active   55m
		kube-public          Active   55m
		kube-system          Active   55m
		local-path-storage   Active   54m
 
thor@jump_host ~$ kubectl get pods
		No resources found in default namespace.

2. Create /tmp/node.yaml file as per requirement
thor@jump_host ~$ vi /tmp/node.yaml
 
thor@jump_host ~$ cat /tmp/node.yaml 
		apiVersion: v1
		kind: Service
		metadata:
		  name: node-service
		spec:
		  type: NodePort
		  selector:
		    app: node-app
		  ports:
		    - port: 80
		      targetPort: 8080
		      nodePort: 30012
		---
		apiVersion: apps/v1
		kind: Deployment
		metadata:
		  name: node-deployment
		spec:
		  replicas: 2
		  selector:
		    matchLabels:
		      app: node-app
		  template:
		    metadata:
		      labels:
		        app: node-app
		    spec:
		      containers:
		        - name: node-container-datacenter
		          image: gcr.io/kodekloud/centos-ssh-enabled:node
		          ports:
		            - containerPort: 80

3. Create deployment and service.
thor@jump_host ~$ kubectl create -f /tmp/node.yaml 
		service/node-service created
		deployment.apps/node-deployment created

4. Verify deployment and service creation, Wait for deployment and pods  to running status
thor@jump_host ~$ kubectl get all
		NAME                                   READY   STATUS              RESTARTS   AGE
		pod/node-deployment-5ffff8bcb4-ns2bc   0/1     ContainerCreating   0          28s
		pod/node-deployment-5ffff8bcb4-vhp7z   0/1     ContainerCreating   0          28s

		NAME                   TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
		service/kubernetes     ClusterIP   10.96.0.1      <none>        443/TCP        61m
		service/node-service   NodePort    10.96.18.249   <none>        80:30012/TCP   28s

		NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
		deployment.apps/node-deployment   0/2     2            0           28s

		NAME                                         DESIRED   CURRENT   READY   AGE
		replicaset.apps/node-deployment-5ffff8bcb4   2         2         0       28s

thor@jump_host ~$ kubectl get deploy
		NAME              READY   UP-TO-DATE   AVAILABLE   AGE
		node-deployment   0/2     2            0           52s

thor@jump_host ~$ kubectl get deploy
		NAME              READY   UP-TO-DATE   AVAILABLE   AGE
		node-deployment   0/2     2            0           59s

thor@jump_host ~$ kubectl get pods
		NAME                               READY   STATUS    RESTARTS   AGE
		node-deployment-5ffff8bcb4-ns2bc   1/1     Running   0          65s
		node-deployment-5ffff8bcb4-vhp7z   1/1     Running   0          65s

thor@jump_host ~$ kubectl get deploy
		NAME              READY   UP-TO-DATE   AVAILABLE   AGE
		node-deployment   2/2     2            2           71s

--------------------------------------------------------------------------------------------------------------------------------
Task 88 Onwards : Kodekloud Cheatsheat SrDevOps Task Commands.txt

--------------------------------------------------------------------------------------------------------------------------------
Init Containers in Kubernetes

There are some applications that need to be deployed on Kubernetes cluster and these apps have some pre-requisites where some configurations need to be changed before deploying the app container. Some of these changes cannot be made inside the images so the DevOps team has come up with a solution to use init containers to perform these tasks during deployment. Below is a sample scenario that the team is going to test first.

    Create a Deployment named as ic-deploy-devops.

    Configure spec as replicas should be 1, labels app should be ic-devops, template's metadata lables app should be the same ic-devops.

    The initContainers should be named as ic-msg-devops, use image debian, preferably with latest tag and use command '/bin/bash', '-c' and 'echo Init Done - Welcome to xFusionCorp Industries > /ic/news'. The volume mount should be named as ic-volume-devops and mount path should be /ic.

    Main container should be named as ic-main-devops, use image debian, preferably with latest tag and use command '/bin/bash', '-c' and 'while true; do cat /ic/news; sleep 5; done'. The volume mount should be named as ic-volume-devops and mount path should be /ic.

    Volume to be named as ic-volume-devops and it should be an emptyDir type.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.


1. Check existing running services,deployment and pods
thor@jump_host ~$ kubectl get all
	NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
	service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   59m

thor@jump_host ~$ kubectl get services
	NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
	kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   59m

thor@jump_host ~$ kubectl get deploy
	No resources found in default namespace.

thor@jump_host ~$ kubectl get pods
	No resources found in default namespace.

2. Create yaml file as per requirement.
thor@jump_host ~$ vi /tmp/init.yml
 
thor@jump_host ~$ cat /tmp/init.yml 
		apiVersion: apps/v1

		kind: Deployment

		metadata:

		  name: ic-deploy-devops

		  labels:

		    app: ic-devops

		spec:

		  replicas: 1

		  selector:

		    matchLabels:

		      app: ic-devops

		  template:

		    metadata:

		      labels:

		        app: ic-devops

		    spec:

		      volumes:

		        - name: ic-volume-devops

		          emptyDir: {}

		      initContainers:

		        - name: ic-msg-devops

		          image: debian:latest

		          command:

		            [

		              "/bin/bash",

		              "-c",

		              "echo Init Done - Welcome to xFusionCorp Industries > /ic/news",

		            ]

		          volumeMounts:

		            - name: ic-volume-devops

		              mountPath: /ic

		 

		      containers:

		        - name: ic-main-devops

		          image: debian:latest

		          command:

		            [

		              "/bin/bash",

		              "-c",

		              "while true; do cat /ic/news; sleep 5; done",

		            ]

		          volumeMounts:

		            - name: ic-volume-devops

		              mountPath: /ic

3. Create pod and deployment 
thor@jump_host ~$ kubectl create -f /tmp/init.yml 
	deployment.apps/ic-deploy-devops created

4. Wait for pod to get to running status
thor@jump_host ~$ kubectl get pods -w
	NAME                                READY   STATUS    RESTARTS   AGE
	ic-deploy-devops-68f4dcbdfb-grg6v   1/1     Running   0          23s
	^C

thor@jump_host ~$ kubectl get deploy
	NAME               READY   UP-TO-DATE   AVAILABLE   AGE
	ic-deploy-devops   1/1     1            1           60s

thor@jump_host ~$ kubectl get pods
	NAME                                READY   STATUS    RESTARTS   AGE
	ic-deploy-devops-68f4dcbdfb-grg6v   1/1     Running   0          66s

5. Validate the task by checking logs on created pod and cat on created file
thor@jump_host ~$ kubectl logs -f ic-deploy-devops-68f4dcbdfb-grg6v
		Init Done - Welcome to xFusionCorp Industries
		Init Done - Welcome to xFusionCorp Industries
		Init Done - Welcome to xFusionCorp Industries
		Init Done - Welcome to xFusionCorp Industries
		Init Done - Welcome to xFusionCorp Industries
		Init Done - Welcome to xFusionCorp Industries
		Init Done - Welcome to xFusionCorp Industries
		^C

thor@jump_host ~$ kubectl exec ic-deploy-devops-68f4dcbdfb-grg6v -- cat /ic/news
		Init Done - Welcome to xFusionCorp Industries

--------------------------------------------------------------------------------------------------------------------------------
Task 89: 13/Oct/2022

Troubleshoot Issue With Pods

One of the junior DevOps team members was working on to deploy a stack on Kubernetes cluster. Somehow the pod is not coming up and its failing with some errors. We need to fix this as soon as possible. Please look into it.

    There is a pod named webserver and the container under it is named as nginx-container. It is using image nginx:latest

    There is a sidecar container as well named sidecar-container which is using ubuntu:latest image.

Look into the issue and fix it, make sure pod is in running state and you are able to access the app.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.



Name:         webserver
Namespace:    default
Priority:     0
Node:         kodekloud-control-plane/172.17.0.2
Start Time:   Thu, 13 Oct 2022 12:03:02 +0000
Labels:       app=web-app
Annotations:  <none>
Status:       Pending
IP:           10.244.0.5
IPs:
  IP:  10.244.0.5
Containers:
  nginx-container:
    Container ID:   
    Image:          nginx:latests
    Image ID:       
    Port:           <none>
    Host Port:      <none>
    State:          Waiting
      Reason:       ImagePullBackOff
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/log/nginx from shared-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-2dgf4 (ro)
  sidecar-container:
    Container ID:  containerd://f186ca761ed3228d82850c85ff4fbf29b954ee664214699ac609288f80cfe75c
    Image:         ubuntu:latest
    Image ID:      docker.io/library/ubuntu@sha256:35fb073f9e56eb84041b0745cb714eff0f7b225ea9e024f703cab56aaa5c7720
    Port:          <none>
    Host Port:     <none>
    Command:
      sh
      -c
      while true; do cat /var/log/nginx/access.log /var/log/nginx/error.log; sleep 30; done
    State:          Running
      Started:      Thu, 13 Oct 2022 12:03:10 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/log/nginx from shared-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-2dgf4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  shared-logs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  default-token-2dgf4:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-2dgf4
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  61s                default-scheduler  Successfully assigned default/webserver to kodekloud-control-plane
  Normal   Pulling    60s                kubelet            Pulling image "ubuntu:latest"
  Normal   Started    53s                kubelet            Started container sidecar-container
  Normal   Pulled     53s                kubelet            Successfully pulled image "ubuntu:latest" in 6.384820926s
  Normal   Created    53s                kubelet            Created container sidecar-container
  Normal   BackOff    26s (x3 over 53s)  kubelet            Back-off pulling image "nginx:latests"
  Warning  Failed     26s (x3 over 53s)  kubelet            Error: ImagePullBackOff
  Normal   Pulling    12s (x3 over 60s)  kubelet            Pulling image "nginx:latests"
  Warning  Failed     11s (x3 over 60s)  kubelet            Failed to pull image "nginx:latests": rpc error: code = NotFound desc = failed to pull and unpack image "docker.io/library/nginx:latests": failed to resolve reference "docker.io/library/nginx:latests": docker.io/library/nginx:latests: not found
  Warning  Failed     11s (x3 over 60s)  kubelet            Error: ErrImagePull


1. Check pod running status using kubectl utility
thor@jump_host ~$ kubectl get pods
	NAME        READY   STATUS             RESTARTS   AGE
	webserver   1/2     ImagePullBackOff   0          30s

thor@jump_host ~$ kubectl get pods webserver
	NAME        READY   STATUS         RESTARTS   AGE
	webserver   1/2     ErrImagePull   0          39s

2. Check pod configuration and try to indentify error
thor@jump_host ~$ kubectl describe pod webserver
		Name:         webserver
		Namespace:    default
		Priority:     0
		Node:         kodekloud-control-plane/172.17.0.2
		Start Time:   Thu, 13 Oct 2022 12:03:02 +0000
		Labels:       app=web-app
		Annotations:  <none>
		Status:       Pending
		IP:           10.244.0.5
		IPs:
		  IP:  10.244.0.5
		Containers:
		|------------------------------------------------------------------------------------------------------------------------
		| nginx-container:
		|   Container ID:   
		|   Image:          nginx:latests											<-------- Error in container image name
		|   Image ID:       
		|   Port:           <none>
		|   Host Port:      <none>
		|   State:          Waiting
		|     Reason:       ImagePullBackOff
		|   Ready:          False
		|   Restart Count:  0
		|   Environment:    <none>
		|   Mounts:
		|     /var/log/nginx from shared-logs (rw)
		|     /var/run/secrets/kubernetes.io/serviceaccount from default-token-2dgf4 (ro)
		|------------------------------------------------------------------------------------------------------------------------      
		  sidecar-container:
		    Container ID:  containerd://f186ca761ed3228d82850c85ff4fbf29b954ee664214699ac609288f80cfe75c
		    Image:         ubuntu:latest
		    Image ID:      docker.io/library/ubuntu@sha256:35fb073f9e56eb84041b0745cb714eff0f7b225ea9e024f703cab56aaa5c7720
		    Port:          <none>
		    Host Port:     <none>
		    Command:
		      sh
		      -c
		      while true; do cat /var/log/nginx/access.log /var/log/nginx/error.log; sleep 30; done
		    State:          Running
		      Started:      Thu, 13 Oct 2022 12:03:10 +0000
		    Ready:          True
		    Restart Count:  0
		    Environment:    <none>
		    Mounts:
		      /var/log/nginx from shared-logs (rw)
		      /var/run/secrets/kubernetes.io/serviceaccount from default-token-2dgf4 (ro)
		Conditions:
		  Type              Status
		  Initialized       True 
		  Ready             False 
		  ContainersReady   False 
		  PodScheduled      True 
		Volumes:
		  shared-logs:
		    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
		    Medium:     
		    SizeLimit:  <unset>
		  default-token-2dgf4:
		    Type:        Secret (a volume populated by a Secret)
		    SecretName:  default-token-2dgf4
		    Optional:    false
		QoS Class:       BestEffort
		Node-Selectors:  <none>
		Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
		                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
		Events:
		  Type     Reason     Age                From               Message
		  ----     ------     ----               ----               -------
		  Normal   Scheduled  61s                default-scheduler  Successfully assigned default/webserver to kodekloud-control-plane
		  Normal   Pulling    60s                kubelet            Pulling image "ubuntu:latest"
		  Normal   Started    53s                kubelet            Started container sidecar-container
		  Normal   Pulled     53s                kubelet            Successfully pulled image "ubuntu:latest" in 6.384820926s
		  Normal   Created    53s                kubelet            Created container sidecar-container
		  Normal   BackOff    26s (x3 over 53s)  kubelet            Back-off pulling image "nginx:latests"
		  Warning  Failed     26s (x3 over 53s)  kubelet            Error: ImagePullBackOff
		  Normal   Pulling    12s (x3 over 60s)  kubelet            Pulling image "nginx:latests"
		  Warning  Failed     11s (x3 over 60s)  kubelet            Failed to pull image "nginx:latests": rpc error: code = NotFound desc = failed to pull and unpack image "docker.io/library/nginx:latests": failed to resolve reference "docker.io/library/nginx:latests": docker.io/library/nginx:latests: not found
		  Warning  Failed     11s (x3 over 60s)  kubelet            Error: ErrImagePull


3. Edit the pod to change the container image name (using vi editor or default editor)
thor@jump_host ~$ kubectl edit pod webserver
		pod/webserver edited

4. Wait for pod to running status
thor@jump_host ~$ kubectl get pod webserver
		NAME        READY   STATUS    RESTARTS   AGE
		webserver   2/2     Running   0          2m22s

thor@jump_host ~$ kubectl describe pod webserver
		Name:         webserver
		Namespace:    default
		Priority:     0
		Node:         kodekloud-control-plane/172.17.0.2
		Start Time:   Thu, 13 Oct 2022 12:03:02 +0000
		Labels:       app=web-app
		Annotations:  <none>
		Status:       Running
		IP:           10.244.0.5
		IPs:
		  IP:  10.244.0.5
		Containers:
		|------------------------------------------------------------------------------------------------------------------------
		| nginx-container:																										
		|   Container ID:   containerd://96e18a2d97d0dcd78289713a2656e1150b4615dd6139fc56c293bc283e147616							
		|   Image:          nginx:latest
		|   Image ID:       docker.io/library/nginx@sha256:2f770d2fe27bc85f68fd7fe6a63900ef7076bc703022fe81b980377fe3d27b70
		|   Port:           <none>
		|   Host Port:      <none>
		|   State:          Running																		<------------Error resolved, pod running
		|     Started:      Thu, 13 Oct 2022 12:05:22 +0000
		|   Ready:          True
		|   Restart Count:  0
		|   Environment:    <none>
		|   Mounts:
		|     /var/log/nginx from shared-logs (rw)
		|     /var/run/secrets/kubernetes.io/serviceaccount from default-token-2dgf4 (ro)
		|------------------------------------------------------------------------------------------------------------------------|
		  sidecar-container:
		    Container ID:  containerd://f186ca761ed3228d82850c85ff4fbf29b954ee664214699ac609288f80cfe75c
		    Image:         ubuntu:latest
		    Image ID:      docker.io/library/ubuntu@sha256:35fb073f9e56eb84041b0745cb714eff0f7b225ea9e024f703cab56aaa5c7720
		    Port:          <none>
		    Host Port:     <none>
		    Command:
		      sh
		      -c
		      while true; do cat /var/log/nginx/access.log /var/log/nginx/error.log; sleep 30; done
		    State:          Running
		      Started:      Thu, 13 Oct 2022 12:03:10 +0000
		    Ready:          True
		    Restart Count:  0
		    Environment:    <none>
		    Mounts:
		      /var/log/nginx from shared-logs (rw)
		      /var/run/secrets/kubernetes.io/serviceaccount from default-token-2dgf4 (ro)
		Conditions:
		  Type              Status
		  Initialized       True 
		  Ready             True 
		  ContainersReady   True 
		  PodScheduled      True 
		Volumes:
		  shared-logs:
		    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
		    Medium:     
		    SizeLimit:  <unset>
		  default-token-2dgf4:
		    Type:        Secret (a volume populated by a Secret)
		    SecretName:  default-token-2dgf4
		    Optional:    false
		QoS Class:       BestEffort
		Node-Selectors:  <none>
		Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
		                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
		Events:
		  Type     Reason     Age                  From               Message
		  ----     ------     ----                 ----               -------
		  Normal   Scheduled  2m28s                default-scheduler  Successfully assigned default/webserver to kodekloud-control-plane
		  Normal   Pulling    2m27s                kubelet            Pulling image "ubuntu:latest"
		  Normal   Started    2m20s                kubelet            Started container sidecar-container
		  Normal   Pulled     2m20s                kubelet            Successfully pulled image "ubuntu:latest" in 6.384820926s
		  Normal   Created    2m20s                kubelet            Created container sidecar-container
		  Normal   Pulling    99s (x3 over 2m27s)  kubelet            Pulling image "nginx:latests"
		  Warning  Failed     98s (x3 over 2m27s)  kubelet            Failed to pull image "nginx:latests": rpc error: code = NotFound desc = failed to pull and unpack image "docker.io/library/nginx:latests": failed to resolve reference "docker.io/library/nginx:latests": docker.io/library/nginx:latests: not found
		  Warning  Failed     98s (x3 over 2m27s)  kubelet            Error: ErrImagePull
		  Normal   BackOff    62s (x6 over 2m20s)  kubelet            Back-off pulling image "nginx:latests"
		  Warning  Failed     62s (x6 over 2m20s)  kubelet            Error: ImagePullBackOff

thor@jump_host ~$ kubectl get pods
	NAME        READY   STATUS    RESTARTS   AGE
	webserver   2/2     Running   0          3m46s

--------------------------------------------------------------------------------------------------------------------------------
Task 90: 15/Oct/2022

Persistent Volumes in Kubernetes HTTPD

The Nautilus DevOps team is working on a Kubernetes template to deploy a web application on the cluster. There are some requirements to create/use persistent volumes to store the application code, and the template needs to be designed accordingly. Please find more details below:

    Create a PersistentVolume named as pv-datacenter. Configure the spec as storage class should be manual, set capacity to 5Gi, set access mode to ReadWriteOnce, volume type should be hostPath and set path to /mnt/itadmin (this directory is already created, you might not be able to access it directly, so you need not to worry about it).

    Create a PersistentVolumeClaim named as pvc-datacenter. Configure the spec as storage class should be manual, request 1Gi of the storage, set access mode to ReadWriteOnce.

    Create a pod named as pod-datacenter, mount the persistent volume you created with claim name pvc-datacenter at document root of the web server, the container within the pod should be named as container-datacenter using image httpd with latest tag only (remember to mention the tag i.e httpd:latest).

    Create a node port type service named web-datacenter using node port 30008 to expose the web server running within the pod.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

1. Check kubectl utility for currently running services and pods
thor@jump_host ~$ kubectl get all
	NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
	service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   65m
 
thor@jump_host ~$ kubectl get pvc
	No resources found in default namespace.

2. Create YAML file as per requirements 
thor@jump_host ~$ vi /tmp/pvc_httpd.yml

thor@jump_host ~$ cat /tmp/pvc_httpd.yml 
		---
		apiVersion: v1
		kind: PersistentVolume
		metadata:
		  name: pv-datacenter
		spec:
		  capacity:
		    storage: 5Gi
		  accessModes:
		    - ReadWriteOnce
		  storageClassName: manual
		  hostPath:
		    path: /mnt/itadmin
		---
		apiVersion: v1
		kind: PersistentVolumeClaim
		metadata:
		  name: pvc-datacenter
		spec:
		  accessModes:
		    - ReadWriteOnce
		  storageClassName: manual
		  resources:
		    requests:
		      storage: 1Gi
		---
		apiVersion: v1
		kind: Pod
		metadata:
		  name: pod-datacenter
		  labels:
		     app: httpd
		spec:
		  volumes:
		    - name: storage-datacenter
		      persistentVolumeClaim:
		        claimName: pvc-datacenter
		  containers:
		    - name: container-datacenter
		      image: httpd:latest
		      ports:
		        - containerPort: 80
		      volumeMounts:
		        - name: storage-datacenter
		          mountPath:  /usr/local/apache2/htdocs/
		---                                                                                                           
		apiVersion: v1                                                                                                
		kind: Service                                                                                                 
		metadata:                                                                                                     
		  name: web-datacenter                                                                                         
		spec:                                                                                                         
		   type: NodePort                                                                                             
		   selector:                                                                                                  
		     app: httpd                                                                                     
		   ports:                                                                                                     
		     - port: 80                                                                                               
		       targetPort: 80                                                                                         
		       nodePort: 30008

3. Create the deployment , pods and persistent volumes
thor@jump_host ~$ kubectl create -f /tmp/pvc_httpd.yml 
	persistentvolume/pv-datacenter created
	persistentvolumeclaim/pvc-datacenter created
	pod/pod-datacenter created
	service/web-datacenter created

4. Wait for pods and services to running status
thor@jump_host ~$ kubectl get all
		NAME                 READY   STATUS    RESTARTS   AGE
		pod/pod-datacenter   0/1     Pending   0          6s

		NAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
		service/kubernetes       ClusterIP   10.96.0.1       <none>        443/TCP        72m
		service/web-datacenter   NodePort    10.96.147.100   <none>        80:30008/TCP   6s

 
thor@jump_host ~$ kubectl get all
	NAME                 READY   STATUS    RESTARTS   AGE
	pod/pod-datacenter   1/1     Running   0          30s

	NAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
	service/kubernetes       ClusterIP   10.96.0.1       <none>        443/TCP        73m
	service/web-datacenter   NodePort    10.96.147.100   <none>        80:30008/TCP   30s

5. Validate Psersistent Volume Claim and Peristent volume 
thor@jump_host ~$ kubectl get pvc
	NAME             STATUS   VOLUME          CAPACITY   ACCESS MODES   STORAGECLASS   AGE
	pvc-datacenter   Bound    pv-datacenter   5Gi        RWO            manual         40s
 
thor@jump_host ~$ kubectl get pv
	NAME            CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                    STORAGECLASS   REASON   AGE
	pv-datacenter   5Gi        RWO            Retain           Bound    default/pvc-datacenter   manual                  45s

6. Validate task by View port on website

--------------------------------------------------------------------------------------------------------------------------------
Task 91: 17/Oct/2022

Resolve Git Merge Conflicts

Sarah and Max were working on writting some stories which they have pushed to the repository. Max has recently added some new changes and is trying to push them to the repository but he is facing some issues. Below you can find more details:

SSH into storage server using user max and password Max_pass123. Under /home/max you will find the story-blog repository. Try to push the changes to the origin repo and fix the issues. The story-index.txt must have titles for all 4 stories. Additionally, there is a typo in The Lion and the Mooose line where Mooose should be Mouse.

Click on the Gitea UI button on the top bar. You should be able to access the Gitea page. You can login to Gitea server from UI using username sarah and password Sarah_pass123 or username max and password Max_pass123.

Note: For these kind of scenarios requiring changes to be done in a web UI, please take screenshots so that you can share it with us for review in case your task is marked incomplete. You may also consider using a screen recording software such as loom.com to record and share your work.

1. SSH to storage server with user max
thor@jump_host ~$ ssh max@ststor01
		The authenticity of host 'ststor01 (172.16.238.15)' can't be established.
		ECDSA key fingerprint is SHA256:0z85j/k+4Nf8WKbHJzxo1AOv4FeRA8LPET2N3BEkYyo.
		ECDSA key fingerprint is MD5:74:e6:4d:c4:b3:80:07:be:03:30:0a:bf:1e:eb:e6:82.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'ststor01,172.16.238.15' (ECDSA) to the list of known hosts.
max@ststor01's password: 
		Welcome to xFusionCorp Storage server.

2. Go to local git repo and check git status
max $ cd /home/max/story-blog/

max (master)$ ls -ahl
		total 32
		drwxr-sr-x    3 max      max         4.0K Oct 17 14:06 .
		drwxr-sr-x    1 max      max         4.0K Oct 17 14:06 ..
		drwxr-sr-x    8 max      max         4.0K Oct 17 14:06 .git
		-rw-r--r--    1 max      max          807 Oct 17 14:06 fox-and-grapes.txt
		-rw-r--r--    1 max      max          792 Oct 17 14:06 frogs-and-ox.txt
		-rw-r--r--    1 max      max         1.1K Oct 17 14:06 lion-and-mouse.txt
		-rw-r--r--    1 max      max          102 Oct 17 14:06 story-index.txt

max (master)$ git status
		On branch master
		Your branch is ahead of 'origin/master' by 1 commit.
		  (use "git push" to publish your local commits)
		nothing to commit, working directory clean
 
3.Check story-index.txt file
max (master)$ cat story-index.txt 
		1. The Lion and the Mooose
		2. The Frogs and the Ox
		3. The Fox and the Grapes
		4. The Donkey and the Dog

4. Try to push the changes to know the error		
max (master)$ git push
		Username for 'http://git.stratos.xfusioncorp.com': max
		Password for 'http://max@git.stratos.xfusioncorp.com': 
		To http://git.stratos.xfusioncorp.com/sarah/story-blog.git
		 ! [rejected]        master -> master (fetch first)
		error: failed to push some refs to 'http://git.stratos.xfusioncorp.com/sarah/story-blog.git'
		hint: Updates were rejected because the remote contains work that you do
		hint: not have locally. This is usually caused by another repository pushing
		hint: to the same ref. You may want to first integrate the remote changes
		hint: (e.g., 'git pull ...') before pushing again.
		hint: See the 'Note about fast-forwards' in 'git push --help' for details.

5. Conflict in between local repo and Gitex repo. Setglobal variables 

max (master)$ git config --global --add user.email max@stratos.xfusioncorp.com

max (master)$ git config --global --add user.name max

max (master)$ git config -l
	user.email=max@stratos.xfusioncorp.com
	user.name=max
	core.repositoryformatversion=0
	core.filemode=true
	core.bare=false
	core.logallrefupdates=true
	remote.origin.url=http://git.stratos.xfusioncorp.com/sarah/story-blog.git
	remote.origin.fetch=+refs/heads/*:refs/remotes/origin/*
	branch.master.remote=origin
	branch.master.merge=refs/heads/master

6. Pull the changes from gitex to local 
max (master)$ git  pull origin master
	remote: Enumerating objects: 4, done.
	remote: Counting objects: 100% (4/4), done.
	remote: Compressing objects: 100% (3/3), done.
	remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0
	Unpacking objects: 100% (3/3), done.
	From http://git.stratos.xfusioncorp.com/sarah/story-blog
	 * branch            master     -> FETCH_HEAD
	   9f02757..7edc0ea  master     -> origin/master
	Auto-merging story-index.txt
	CONFLICT (add/add): Merge conflict in story-index.txt
	Automatic merge failed; fix conflicts and then commit the result.

7. Conflict while merging story-index.txt. Edit and clear those conflict

max (master)$ vi story-index.txt

max (master)$ cat story-index.txt 
	1. The Lion and the Mouse
	2. The Frogs and the Ox
	3. The Fox and the Grapes
	4. The Donkey and the Dog

8. Push the changes back to remote repository

max (master)$ git add story-index.txt 

max (master)$ git commit -m "Fixed the issue"
	[master cf96f89] Fixed the issue
	 1 file changed, 6 deletions(-)

max (master)$ git push origin  master
	Username for 'http://git.stratos.xfusioncorp.com': max
	Password for 'http://max@git.stratos.xfusioncorp.com': 
	Counting objects: 3, done.
	Delta compression using up to 36 threads.
	Compressing objects: 100% (3/3), done.
	Writing objects: 100% (3/3), 280 bytes | 0 bytes/s, done.
	Total 3 (delta 2), reused 0 (delta 0)
	remote: . Processing 1 references
	remote: Processed 1 references in total
	To http://git.stratos.xfusioncorp.com/sarah/story-blog.git
	   acefab2..cf96f89  master -> master
 
max (master)$ git status
	On branch master
	Your branch is up-to-date with 'origin/master'.
	nothing to commit, working directory clean

max (master)$ git pull origin  master
	From http://git.stratos.xfusioncorp.com/sarah/story-blog
	 * branch            master     -> FETCH_HEAD
	Already up-to-date.
 
max (master)$ git status
	On branch master
	Your branch is up-to-date with 'origin/master'.
	nothing to commit, working directory clean

max (master)$ 

--------------------------------------------------------------------------------------------------------------------------------
Task 92: 19/Oct/2022

Managing Jinja2 Templates Using Ansible

One of the Nautilus DevOps team members is working on to develop a role for httpd installation and configuration. Work is almost completed, however there is a requirement to add a jinja2 template for index.html file. Additionally, the relevant task needs to be added inside the role. The inventory file ~/ansible/inventory is already present on jump host that can be used. Complete the task as per details mentioned below:

a. Update ~/ansible/playbook.yml playbook to run the httpd role on App Server 1.

b. Create a jinja2 template index.html.j2 under /home/thor/ansible/role/httpd/templates/ directory and add a line This file was created using Ansible on <respective server> (for example This file was created using Ansible on stapp01 in case of App Server 1). Also please make sure not to hard code the server name inside the template. Instead, use inventory_hostname variable to fetch the correct value.

c. Add a task inside /home/thor/ansible/role/httpd/tasks/main.yml to copy this template on App Server 1 under /var/www/html/index.html. Also make sure that /var/www/html/index.html file's permissions are 0777.

d. The user/group owner of /var/www/html/index.html file must be respective sudo user of the server (for example tony in case of stapp01).

Note: Validation will try to run the playbook using command ansible-playbook -i inventory playbook.yml so please make sure the playbook works this way without passing any extra arguments.


1. Go to mentioned folder and verify playbook and inventory

thor@jump_host ~$ cd /home/thor/ansible/

thor@jump_host ~/ansible$ ls -ahl
		total 20K
		drwxr-xr-x 3 thor thor 4.0K Oct 19 05:55 .
		drwxr----- 1 thor thor 4.0K Oct 19 05:55 ..
		-rw-r--r-- 1 thor thor  237 Oct 19 05:55 inventory
		-rw-r--r-- 1 thor thor   73 Oct 19 05:55 playbook.yml
		drwxr-xr-x 3 thor thor 4.0K Oct 19 05:55 role

thor@jump_host ~/ansible$ cat inventory 
		stapp01 ansible_host=172.16.238.10 ansible_user=tony ansible_ssh_pass=Ir0nM@n
		stapp02 ansible_host=172.16.238.11 ansible_user=steve ansible_ssh_pass=Am3ric@
		stapp03 ansible_host=172.16.238.12 ansible_user=banner ansible_ssh_pass=BigGr33n

 
thor@jump_host ~/ansible$ cat playbook.yml 
		---
		- hosts: 
		  become: yes
		  become_user: root
		  roles:
		    - role/httpd

2. Edit the playbook to add the mentioned host (stapp01)

thor@jump_host ~/ansible$ vi playbook.yml 

thor@jump_host ~/ansible$ cat playbook.yml 
		---
		- hosts: stapp01						<--------------------------
		  become: yes
		  become_user: root
		  roles:
		    - role/httpd

3. Create Jinja2 template with mentioned content

thor@jump_host ~/ansible$ vi /home/thor/ansible/role/httpd/templates/index.html.j2

thor@jump_host ~/ansible$ cat /home/thor/ansible/role/httpd/templates/index.html.j2
		This file was created using Ansible on {{ ansible_hostname }}


4. Edit the mian.yml file to add a task to copy the Jinja2 template  

thor@jump_host ~/ansible$ vi /home/thor/ansible/role/httpd/tasks/main.yml

thor@jump_host ~/ansible$ cat /home/thor/ansible/role/httpd/tasks/main.yml
		---
		# tasks file for role/test

		- name: install the latest version of HTTPD
		  yum:
		    name: httpd
		    state: latest

		- name: Start service httpd
		  service:
		    name: httpd
		    state: started

		- name: Use Jinja2 template to generate index.html
		  template:
		    src: /home/thor/ansible/role/httpd/templates/index.html.j2
		    dest: /var/www/html/index.html
		    mode: "0777"
		    owner: "{{ ansible_user }}"
		    group: "{{ ansible_user }}"


5. Run the playbook

thor@jump_host ~/ansible$ ansible-playbook -i inventory playbook.yml 

		PLAY [stapp01] ******************************************************************************************************************************

		TASK [Gathering Facts] **********************************************************************************************************************
		ok: [stapp01]

		TASK [role/httpd : install the latest version of HTTPD] *************************************************************************************
		changed: [stapp01]

		TASK [role/httpd : Start service httpd] *****************************************************************************************************
		changed: [stapp01]

		TASK [role/httpd : Use Jinja2 template to generate index.html] ******************************************************************************
		changed: [stapp01]

		PLAY RECAP **********************************************************************************************************************************
		stapp01                    : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   


6. Validate the changes on app server

thor@jump_host ~/ansible$ ansible stapp01 -i inventory  -a "cat /var/www/html/index.html"
		stapp01 | CHANGED | rc=0 >>
		This file was created using Ansible on stapp01

thor@jump_host ~/ansible$ ansible stapp01 -i inventory  -a "ls -ahl /var/www/html/index.html"
		stapp01 | CHANGED | rc=0 >>
		-rwxrwxrwx 1 tony tony 47 Oct 19 06:03 /var/www/html/index.html

thor@jump_host ~/ansible$ ansible stapp02 -i inventory  -a "cat /var/www/html/index.html"
		stapp02 | FAILED | rc=1 >>
		cat: /var/www/html/index.html: No such file or directorynon-zero return code

thor@jump_host ~/ansible$ ansible stapp03 -i inventory  -a "cat /var/www/html/index.html"
		stapp03 | FAILED | rc=1 >>
		cat: /var/www/html/index.html: No such file or directorynon-zero return code

--------------------------------------------------------------------------------------------------------------------------------
Task 93: 20/Oct/2022

Ansible Inventory Update

The Nautilus DevOps team has started testing their Ansible playbooks on different servers within the stack. They have placed some playbooks under /home/thor/playbook/ directory on jump host which they want to test. Some of these playbooks have already been tested on different servers, but now they want to test them on app server 3 in Stratos DC. However, they first need to create an inventory file so that Ansible can connect to the respective app. Below are some requirements:

a. Create an ini type Ansible inventory file /home/thor/playbook/inventory on jump host.

b. Add App Server 3 in this inventory along with required variables that are needed to make it work.

c. The inventory hostname of the host should be the server name as per the wiki, for example stapp01 for app server 1 in Stratos DC.

Note: Validation will try to run the playbook using command ansible-playbook -i inventory playbook.yml so please make sure the playbook works this way without passing any extra arguments.


1. Verify mentioned playbook and other files 

thor@jump_host ~$ cd /home/thor/playbook/

thor@jump_host ~/playbook$ ls -ahl
		total 16K
		drwxr-xr-x 2 thor thor 4.0K Oct 20 09:36 .
		drwxr----- 1 thor thor 4.0K Oct 20 09:36 ..
		-rw-r--r-- 1 thor thor   36 Oct 20 09:36 ansible.cfg
		-rw-r--r-- 1 thor thor  250 Oct 20 09:36 playbook.yml

thor@jump_host ~/playbook$ cat playbook.yml 
		---
		- hosts: all
		  become: yes
		  become_user: root
		  tasks:
		    - name: Install httpd package    
		      yum: 
		        name: httpd 
		        state: installed
		    
		    - name: Start service httpd
		      service:
		        name: httpd
		        state: started

2. Create the mentioned inventory file with App Server 3 details

thor@jump_host ~/playbook$ vi /home/thor/playbook/inventory

thor@jump_host ~/playbook$ cat /home/thor/playbook/inventory
		stapp03 ansible_host=172.16.238.12 ansible_ssh_pass=BigGr33n  ansible_user=banner

3. Exectue the playbook and verify.

thor@jump_host ~/playbook$ ansible-playbook -i inventory playbook.yml

		PLAY [all] **********************************************************************************************************************************

		TASK [Gathering Facts] **********************************************************************************************************************
		ok: [stapp03]

		TASK [Install httpd package] ****************************************************************************************************************
		changed: [stapp03]

		TASK [Start service httpd] ******************************************************************************************************************
		changed: [stapp03]

		PLAY RECAP **********************************************************************************************************************************
		stapp03                    : ok=3    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

thor@jump_host ~/playbook$

--------------------------------------------------------------------------------------------------------------------------------
Task 94: 22/Oct/2022

Git Fork a Repository


There is a Git server used by the Nautilus project teams. Recently a new developer Jon joined the team and needs to start working on a project. To do so, he needs to fork an existing Git repository. Below you can find more details:

    Click on the Gitea UI button on the top bar. You should be able to access the Gitea page.

    Login to Gitea server using username jon and password Jon_pass123.

    There you will see a Git repository sarah/story-blog, fork it under jon user.

Note: For these kind of scenarios requiring changes to be done in a web UI, please take screenshots so that you can share it with us for review in case your task is marked incomplete. You may also consider using a screen recording software such as loom.com to record and share your work.


1. As mentioned open Gitea UI

2. Login to Gitea server using username jon and password Jon_pass123

3. Go to repo sarah/story-blog

4. Right top corner option for fork the repo

5. The new forked repo will be under jon/story-blog

--------------------------------------------------------------------------------------------------------------------------------
Task 95: 23/Oct/2022

Create a Docker Image From Container

One of the Nautilus developer was working to test new changes on a container. He wants to keep a backup of his changes to the container. A new request has been raised for the DevOps team to create a new image from this container. Below are more details about it:

a. Create an image apps:xfusion on Application Server 2 from a container ubuntu_latest that is running on same server.


1. 1. Login on app server  & Switch to  root user

thor@jump_host ~$ ssh steve@stapp02
		The authenticity of host 'stapp02 (172.16.238.11)' can't be established.
		ECDSA key fingerprint is SHA256:LmeMEQk6Mx7vqZWW6o6Knsvsgqwb4FlOk7e/cSvtfms.
		ECDSA key fingerprint is MD5:b8:e8:31:0f:29:8b:c7:be:26:6e:42:aa:56:51:29:8c.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'stapp02,172.16.238.11' (ECDSA) to the list of known hosts.
		steve@stapp02's password: 

[steve@stapp02 ~]$ sudo su -

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for steve: 


2. Check existing docker container ubuntu_latest running status 

[root@stapp02 ~]# docker ps 
		CONTAINER ID   IMAGE     COMMAND   CREATED              STATUS              PORTS     NAMES
		e2d2e33ed57c   ubuntu    "bash"    About a minute ago   Up About a minute             ubuntu_latest

3. Check current docker images on your system

[root@stapp02 ~]# docker images
		REPOSITORY   TAG       IMAGE ID       CREATED       SIZE
		ubuntu       latest    216c552ea5ba   2 weeks ago   77.8MB

4. Create new image from given contianer as per task

[root@stapp02 ~]# docker commit ubuntu_latest apps:xfusion
		sha256:2158e1542b17e48950a0c1fd45ab50729fbd9dd344210b5771b0113955100168

5. Verify and validate the new  docker image created on your system

[root@stapp02 ~]# docker images
		REPOSITORY   TAG       IMAGE ID       CREATED         SIZE
		apps         xfusion   2158e1542b17   6 seconds ago   116MB
		ubuntu       latest    216c552ea5ba   2 weeks ago     77.8MB

--------------------------------------------------------------------------------------------------------------------------------
Task 96: 25/Oct/2022

Puppet Multi-Packages Installation

Some new changes need to be made on some of the app servers in Stratos Datacenter. There are some packages that need to be installed on the app server 2. We want to install these packages using puppet only.

    Puppet master is already installed on Jump Server.

    Create a puppet programming file blog.pp under /etc/puppetlabs/code/environments/production/manifests on master node i.e on Jump Server and perform below mentioned tasks using the same.

    Define a class multi_package_node for agent node 2 i.e app server 2. Install net-tools and unzip packages on the agent node 2.

Notes: :- Please make sure to run the puppet agent test using sudo on agent nodes, otherwise you can face certificate issues. In that case you will have to clean the certificates first and then you will be able to run the puppet agent test.

:- Before clicking on the Check button please make sure to verify puppet server and puppet agent services are up and running on the respective servers, also please make sure to run puppet agent test to apply/test the changes manually first.

:- Please note that once lab is loaded, the puppet server service should start automatically on puppet master server, however it can take upto 2-3 minutes to start.


1. Switch to root user on jump host to create puppet programming file

thor@jump_host ~$ sudo su - 

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for thor: 

2. Go to mentione folder location and create blog.pp  file with given requirements

root@jump_host ~# cd /etc/puppetlabs/code/environments/production/manifests/
 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# ls -ahl
		total 8.0K
		drwxr-xr-x 1 puppet puppet 4.0K Jul 13  2021 .
		drwxr-xr-x 1 puppet puppet 4.0K Aug  9  2021 ..
 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# vi blog.pp

root@jump_host /etc/puppetlabs/code/environments/production/manifests# cat blog.pp 
		class multi_package_node {
		$multi_package = [ 'net-tools', 'unzip']
		    package { $multi_package: ensure => 'installed' }
		}

		node 'stapp02.stratos.xfusioncorp.com' {
		  include multi_package_node
		}

3. Login to App Server 2 and switch to root user

root@jump_host /etc/puppetlabs/code/environments/production/manifests# ssh steve@stapp02
		The authenticity of host 'stapp02 (172.16.238.11)' can't be established.
		ECDSA key fingerprint is SHA256:JGrKQGk9+m0eDIxj8Ttwk2oL5abYgUcCazjXSp36dxs.
		ECDSA key fingerprint is MD5:ec:b0:32:f5:c4:6d:68:07:03:f0:ad:7a:a6:a7:f9:47.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'stapp02,172.16.238.11' (ECDSA) to the list of known hosts.
		steve@stapp02's password: 

[steve@stapp02 ~]$ sudo su -

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for steve: 

4. Check if given packages are already  installed

[root@stapp02 ~]# rpm -qa|grep -e net-tools -e unzip

5. Run the puppet agent to pull configuration from puppet server 

[root@stapp02 ~]# puppet agent -tv
		Info: Using configured environment 'production'
		Info: Retrieving pluginfacts
		Info: Retrieving plugin
		Info: Retrieving locales
		Info: Caching catalog for stapp02.stratos.xfusioncorp.com
		Info: Applying configuration version '1666708349'
		Notice: /Stage[main]/Multi_package_node/Package[net-tools]/ensure: created
		Notice: /Stage[main]/Multi_package_node/Package[unzip]/ensure: created
		Notice: Applied catalog in 14.67 seconds

6. Validate the task by checking required packages installed

[root@stapp02 ~]# rpm -qa|grep -e net-tools -e unzip
		unzip-6.0-24.el7_9.x86_64
		net-tools-2.0-0.25.20131004git.el7.x86_64

[root@stapp02 ~]#

--------------------------------------------------------------------------------------------------------------------------------
Task 97: 27/Oct/2022

Puppet Add Users


A new teammate has joined the Nautilus application development team, the application development team has asked the DevOps team to create a new user account for the new teammate on application server 1 in Stratos Datacenter. The task needs to be performed using Puppet only. You can find more details below about the task.

Create a Puppet programming file official.pp under /etc/puppetlabs/code/environments/production/manifests directory on master node i.e Jump Server, and using Puppet user resource add a user on all app servers as mentioned below:

    Create a user ravi and set its UID to 1879 on Puppet agent nodes 1 i.e App Servers 1.

Notes: :- Please make sure to run the puppet agent test using sudo on agent nodes, otherwise you can face certificate issues. In that case you will have to clean the certificates first and then you will be able to run the puppet agent test.

:- Before clicking on the Check button please make sure to verify puppet server and puppet agent services are up and running on the respective servers, also please make sure to run puppet agent test to apply/test the changes manually first.

:- Please note that once lab is loaded, the puppet server service should start automatically on puppet master server, however it can take upto 2-3 minutes to start.


1. Switch to root user and go to mentioned folder

thor@jump_host ~$ sudo su -

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for thor: 
 
root@jump_host ~# cd /etc/puppetlabs/code/environments/production/manifests/

root@jump_host /etc/puppetlabs/code/environments/production/manifests# ls -ahl
		total 8.0K
		drwxr-xr-x 1 puppet puppet 4.0K Jul 13  2021 .
		drwxr-xr-x 1 puppet puppet 4.0K Aug  9  2021 ..

2. Create the mentioned puppet programming file with given requirements 

root@jump_host /etc/puppetlabs/code/environments/production/manifests# vi official.pp

root@jump_host /etc/puppetlabs/code/environments/production/manifests# cat official.pp
		class user_creator {
			user { 'ravi':
						ensure   => present,
						uid => 1879,
		  }
		}

		node 'stapp01.stratos.xfusioncorp.com' {
			include user_creator
		}

3. Validate the puppet programming file 

root@jump_host /etc/puppetlabs/code/environments/production/manifests# puppet parser validate official.pp 

4. Login to app server 01 and switch to root 

root@jump_host /etc/puppetlabs/code/environments/production/manifests# ssh tony@stapp01
		The authenticity of host 'stapp01 (172.16.238.10)' can't be established.
		ECDSA key fingerprint is SHA256:CxGn3WKmroQOoXAC5aufEdVzoDTknATLtoQHDcP0WEI.
		ECDSA key fingerprint is MD5:f1:cb:d0:b8:e6:b4:74:04:70:d2:ba:4f:12:ce:a3:23.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'stapp01,172.16.238.10' (ECDSA) to the list of known hosts.
		tony@stapp01's password: 

[tony@stapp01 ~]$ sudo su -

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for tony: 

5. Run puppet agent to pull latest configuration from puppet server

[root@stapp01 ~]# puppet agent -tv
		Info: Using configured environment 'production'
		Info: Retrieving pluginfacts
		Info: Retrieving plugin
		Info: Retrieving locales
		Info: Caching catalog for stapp01.stratos.xfusioncorp.com
		Info: Applying configuration version '1666854162'
		Notice: /Stage[main]/User_creator/User[ravi]/ensure: created
		Notice: Applied catalog in 0.06 seconds

6. Verify the new user created 

[root@stapp01 ~]# cat /etc/passwd |grep ravi
	ravi:x:1879:1879::/home/ravi:/bin/bash

--------------------------------------------------------------------------------------------------------------------------------
Task 98: 28/Oct/2022

Kubernetes Time Check Pod

The Nautilus DevOps team want to create a time check pod in a particular Kubernetes namespace and record the logs. This might be initially used only for testing purposes, but later can be implemented in an existing cluster. Please find more details below about the task and perform it.

    Create a pod called time-check in the datacenter namespace. This pod should run a container called time-check, container should use the busybox image with latest tag only and remember to mention tag i.e busybox:latest.

    Create a config map called time-config with the data TIME_FREQ=12 in the same namespace.

    The time-check container should run the command: while true; do date; sleep $TIME_FREQ;done and should write the result to the location /opt/dba/time/time-check.log. Remember you will also need to add an environmental variable TIME_FREQ in the container, which should pick value from the config map TIME_FREQ key.

    Create a volume log-volume and mount the same on /opt/dba/time within the container.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.


1. Check kubectl utility config on jump server 

thor@jump_host ~$ kubectl get namespace
		NAME                 STATUS   AGE
		default              Active   115m
		kube-node-lease      Active   115m
		kube-public          Active   115m
		kube-system          Active   115m
		local-path-storage   Active   115m

thor@jump_host ~$ kubectl get pods
	No resources found in default namespace.

2. Create new namespace 

thor@jump_host ~$ kubectl create namespace datacenter
	namespace/datacenter created

thor@jump_host ~$ kubectl get namespace
		NAME                 STATUS   AGE
		datacenter           Active   4s
		default              Active   116m
		kube-node-lease      Active   116m
		kube-public          Active   116m
		kube-system          Active   116m
		local-path-storage   Active   115m

3. Create YAML file as per given requirements 

thor@jump_host ~$ vi /tmp/time_check.yaml
 
thor@jump_host ~$ cat /tmp/time_check.yaml 

		apiVersion: v1
		kind: Namespace
		metadata:
		  name: datacenter

		---
		apiVersion: v1
		kind: ConfigMap
		metadata:
		  name: time-config
		  namespace: datacenter
		data:
		    TIME_FREQ: "12"

		---    
		apiVersion: v1
		kind: Pod
		metadata:
		  name: time-check
		  namespace: datacenter
		spec:
		  containers:
		    - name: time-check
		      image: busybox:latest
		      command: [ "sh", "-c", "while true; do date; sleep $TIME_FREQ;done >> /opt/dba/time/time-check.log" ]
		      env:
		        - name: TIME_FREQ
		          valueFrom:
		            configMapKeyRef:
		              name: time-config
		              key: TIME_FREQ
		      volumeMounts:
		      - name: log-volume
		        mountPath: /opt/dba/time
		  volumes:
		    - name: log-volume
		      emptyDir : {}
		  restartPolicy: Never

4. Create Pod using the YAML file

thor@jump_host ~$ kubectl create -f  /tmp/time_check.yaml 
		configmap/time-config created
		pod/time-check created
		Error from server (AlreadyExists): error when creating "/tmp/time_check.yaml": namespaces "datacenter" already exists

5. Verify the pod running status 

thor@jump_host ~$ kubectl get pods -n datacenter
		NAME         READY   STATUS    RESTARTS   AGE
		time-check   1/1     Running   0          29s

thor@jump_host ~$ kubectl get pods -n datacenter
		NAME         READY   STATUS    RESTARTS   AGE
		time-check   1/1     Running   0          32s

--------------------------------------------------------------------------------------------------------------------------------
Task 99: 04/Nov/2022

Deploy MySQL on Kubernetes



A new MySQL server needs to be deployed on Kubernetes cluster. The Nautilus DevOps team was working on to gather the requirements. Recently they were able to finalize the requirements and shared them with the team members to start working on it. Below you can find the details:

1.) Create a PersistentVolume mysql-pv, its capacity should be 250Mi, set other parameters as per your preference.

2.) Create a PersistentVolumeClaim to request this PersistentVolume storage. Name it as mysql-pv-claim and request a 250Mi of storage. Set other parameters as per your preference.

3.) Create a deployment named mysql-deployment, use any mysql image as per your preference. Mount the PersistentVolume at mount path /var/lib/mysql.

4.) Create a NodePort type service named mysql and set nodePort to 30007.

5.) Create a secret named mysql-root-pass having a key pair value, where key is password and its value is YUIidhb667, create another secret named mysql-user-pass having some key pair values, where frist key is username and its value is kodekloud_pop, second key is password and value is GyQkFRVNr3, create one more secret named mysql-db-url, key name is database and value is kodekloud_db7

6.) Define some Environment variables within the container:

a) name: MYSQL_ROOT_PASSWORD, should pick value from secretKeyRef name: mysql-root-pass and key: password

b) name: MYSQL_DATABASE, should pick value from secretKeyRef name: mysql-db-url and key: database

c) name: MYSQL_USER, should pick value from secretKeyRef name: mysql-user-pass key key: username

d) name: MYSQL_PASSWORD, should pick value from secretKeyRef name: mysql-user-pass and key: password

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.


1. Check Kubectl service status and config

thor@jump_host ~$ kubectl get all
		NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
		service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   38m

thor@jump_host ~$ kubectl get service
		NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
		kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   38m

thor@jump_host ~$ kubectl get pv
		No resources found

thor@jump_host ~$ kubectl get pvc
		No resources found in default namespace.

2. Create Kubectl Secrets as per given task

thor@jump_host ~$ kubectl create secret generic mysql-root-pass --from-literal=password=YUIidhb667
		secret/mysql-root-pass created
 
thor@jump_host ~$ kubectl create secret generic mysql-user-pass --from-literal=username=kodekloud_pop --from-literal=password=GyQkFRVNr3
		secret/mysql-user-pass created

thor@jump_host ~$ kubectl create secret generic mysql-db-url --from-literal=database=kodekloud_db7
		secret/mysql-db-url created

3. Verify Secrets creation

thor@jump_host ~$ kubectl get all
		NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
		service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   40m

thor@jump_host ~$ kubectl get secrets
		NAME                  TYPE                                  DATA   AGE
		default-token-d62nx   kubernetes.io/service-account-token   3      40m
		mysql-db-url          Opaque                                1      16s
		mysql-root-pass       Opaque                                1      47s
		mysql-user-pass       Opaque                                2      28s

4. Create YAML file as per requirement
 
thor@jump_host ~$ vi /tmp/mysql_deploy.yaml
 
thor@jump_host ~$ cat /tmp/mysql_deploy.yaml 
		---                                                                                           
		apiVersion: v1                                                                                
		kind: PersistentVolume                                                                        
		metadata:                                                                                     
		  name: mysql-pv                                                                              
		spec:                                                                                         
		  capacity:                                                                                   
		    storage: 250Mi     
		  accessModes:                                                                                
		    - ReadWriteOnce                                                                           
		  hostPath:                                                                                   
		    path: "/var/lib/mysql"                                                                    
		---                                                                                           
		apiVersion: v1                                                                                
		kind: PersistentVolumeClaim                                                                   
		metadata:                                                                                     
		  name: mysql-pv-claim                                                                        
		spec:                                                                                         
		  accessModes:                                                                                
		    - ReadWriteOnce                                                                           
		  resources:                                                                                  
		    requests:                                                                                 
		      storage: 250Mi                                                                          
		---
		apiVersion: v1
		kind: Service
		metadata:
		  name: mysql
		spec:
		  type: NodePort
		  selector:
		    app: mysql
		  ports:
		    - port: 3306
		      targetPort: 3306
		      nodePort: 30007
		---       
		apiVersion: apps/v1
		kind: Deployment
		metadata:
		  name: mysql-deployment
		  labels:
		    app: mysql
		spec:
		  replicas: 1
		  selector:
		    matchLabels:
		      app: mysql
		  template:
		    metadata:
		      labels:
		        app: mysql
		    spec:
		      volumes:
		      - name: mysql-pv
		        persistentVolumeClaim:
		          claimName: mysql-pv-claim
		      containers:
		      - name: mysql
		        image: mysql:latest
		        env:
		        - name: MYSQL_ROOT_PASSWORD
		          valueFrom:
		            secretKeyRef:
		              name: mysql-root-pass
		              key: password
		        - name: MYSQL_DATABASE
		          valueFrom:
		            secretKeyRef:
		              name: mysql-db-url
		              key: database
		        - name: MYSQL_USER
		          valueFrom:
		            secretKeyRef:
		              name: mysql-user-pass
		              key: username
		        - name: MYSQL_PASSWORD
		          valueFrom:
		            secretKeyRef:
		              name: mysql-user-pass
		              key: password
		        volumeMounts:
		        - name: mysql-pv
		          mountPath: /var/lib/mysql
		        ports:
		        - containerPort: 3306
		          name: mysql


5. Create Pods  using the YAML file

thor@jump_host ~$ kubectl create -f  /tmp/mysql_deploy.yaml 
		persistentvolume/mysql-pv created
		persistentvolumeclaim/mysql-pv-claim created
		service/mysql created
		deployment.apps/mysql-deployment created

6. Verify the components created 

thor@jump_host ~$ kubectl get pv
		NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                    STORAGECLASS   REASON   AGE
		mysql-pv                                   250Mi      RWO            Retain           Available                                                    13s
		pvc-c497c610-d100-4616-b073-56b04050cfa3   250Mi      RWO            Delete           Bound       default/mysql-pv-claim   standard                9s

thor@jump_host ~$ kubectl get pvc
		NAME             STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
		mysql-pv-claim   Bound    pvc-c497c610-d100-4616-b073-56b04050cfa3   250Mi      RWO            standard       19s

7. Wait for Pods to get into running status and check the ENV for MYSQL env variables in the pod created

thor@jump_host ~$ kubectl get all
		NAME                                    READY   STATUS              RESTARTS   AGE
		pod/mysql-deployment-6ccf56667c-dmrw5   0/1     ContainerCreating   0          32s

		NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
		service/kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP          42m
		service/mysql        NodePort    10.96.210.211   <none>        3306:30007/TCP   32s

		NAME                               READY   UP-TO-DATE   AVAILABLE   AGE
		deployment.apps/mysql-deployment   0/1     1            0           32s

		NAME                                          DESIRED   CURRENT   READY   AGE
		replicaset.apps/mysql-deployment-6ccf56667c   1         1         0       32s

thor@jump_host ~$ kubectl get all
		NAME                                    READY   STATUS    RESTARTS   AGE
		pod/mysql-deployment-6ccf56667c-dmrw5   1/1     Running   0          45s

		NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
		service/kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP          42m
		service/mysql        NodePort    10.96.210.211   <none>        3306:30007/TCP   45s

		NAME                               READY   UP-TO-DATE   AVAILABLE   AGE
		deployment.apps/mysql-deployment   1/1     1            1           45s

		NAME                                          DESIRED   CURRENT   READY   AGE
		replicaset.apps/mysql-deployment-6ccf56667c   1         1         1       45s


thor@jump_host ~$ kubectl exec -it mysql-deployment-6ccf56667c-dmrw5 -- /bin/bash

bash-4.4# printenv
		MYSQL_PASSWORD=GyQkFRVNr3
		MYSQL_PORT_3306_TCP_PROTO=tcp
		HOSTNAME=mysql-deployment-6ccf56667c-dmrw5
		MYSQL_DATABASE=kodekloud_db7
		KUBERNETES_PORT_443_TCP_PROTO=tcp
		MYSQL_SERVICE_PORT=3306
		KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1
		MYSQL_ROOT_PASSWORD=YUIidhb667
		MYSQL_PORT=tcp://10.96.210.211:3306
		KUBERNETES_PORT=tcp://10.96.0.1:443
		MYSQL_PORT_3306_TCP=tcp://10.96.210.211:3306
		PWD=/
		HOME=/root
		MYSQL_MAJOR=8.0
		GOSU_VERSION=1.14
		MYSQL_USER=kodekloud_pop
		MYSQL_PORT_3306_TCP_PORT=3306
		KUBERNETES_SERVICE_PORT_HTTPS=443
		MYSQL_PORT_3306_TCP_ADDR=10.96.210.211
		KUBERNETES_PORT_443_TCP_PORT=443
		MYSQL_VERSION=8.0.31-1.el8
		KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443
		TERM=xterm
		SHLVL=1
		KUBERNETES_SERVICE_PORT=443
		PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
		KUBERNETES_SERVICE_HOST=10.96.0.1
		MYSQL_SHELL_VERSION=8.0.31-1.el8
		MYSQL_SERVICE_HOST=10.96.210.211
		_=/usr/bin/printenv
bash-4.4# 

--------------------------------------------------------------------------------------------------------------------------------
Task 100: 05/Nov/2022

Git Install and Create Bare Repository

The Nautilus development team shared requirements with the DevOps team regarding new application development.—specifically, they want to set up a Git repository for that project. Create a Git repository on Storage server in Stratos DC as per details given below:

    Install git package using yum on Storage server.

    After that create a bare repository /opt/news.git (make sure to use exact name).


1. Login on storage server and switch to root user

thor@jump_host ~$ ssh natasha@ststor01
		The authenticity of host 'ststor01 (172.16.238.15)' can't be established.
		ECDSA key fingerprint is SHA256:IEbMfpjmum1MkMYwoKYJ6cm72/aRpYpB9G787u7jPMM.
		ECDSA key fingerprint is MD5:55:d5:fc:19:3e:f2:bb:47:77:9d:7d:a7:ea:cc:e7:61.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'ststor01,172.16.238.15' (ECDSA) to the list of known hosts.
		natasha@ststor01's password: 

[natasha@ststor01 ~]$ sudo su -

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for natasha: 

2. Install git package using yum 

[root@ststor01 ~]# rpm -qa |grep git

[root@ststor01 ~]# yum install -y git
		Loaded plugins: fastestmirror, ovl
		Determining fastest mirrors
		 * base: ftpmirror.your.org
		 * extras: mirror.genesishosting.com
		 * updates: tx-mirror.tier.net
		base                                                                                                                  | 3.6 kB  00:00:00     
		extras                                                                                                                | 2.9 kB  00:00:00     
		updates                                                                                                               | 2.9 kB  00:00:00     
		(1/4): base/7/x86_64/group_gz                                                                                         | 153 kB  00:00:00     
		(2/4): extras/7/x86_64/primary_db                                                                                     | 249 kB  00:00:00     
		(3/4): base/7/x86_64/primary_db                                                                                       | 6.1 MB  00:00:00     
		(4/4): updates/7/x86_64/primary_db                                                                                    |  17 MB  00:00:00     
		Resolving Dependencies
		--> Running transaction check
		---> Package git.x86_64 0:1.8.3.1-23.el7_8 will be installed
		--> Processing Dependency: perl-Git = 1.8.3.1-23.el7_8 for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl >= 5.008 for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: rsync for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(warnings) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(vars) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(strict) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(lib) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(Term::ReadKey) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(Git) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(Getopt::Long) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(File::stat) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(File::Temp) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(File::Spec) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(File::Path) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(File::Find) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(File::Copy) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(File::Basename) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(Exporter) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(Error) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: openssh-clients for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: less for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: /usr/bin/perl for package: git-1.8.3.1-23.el7_8.x86_64
		--> Running transaction check
		---> Package less.x86_64 0:458-9.el7 will be installed
		--> Processing Dependency: groff-base for package: less-458-9.el7.x86_64
		---> Package openssh-clients.x86_64 0:7.4p1-22.el7_9 will be installed
		--> Processing Dependency: openssh = 7.4p1-22.el7_9 for package: openssh-clients-7.4p1-22.el7_9.x86_64
		--> Processing Dependency: libedit.so.0()(64bit) for package: openssh-clients-7.4p1-22.el7_9.x86_64
		---> Package perl.x86_64 4:5.16.3-299.el7_9 will be installed
		--> Processing Dependency: perl-libs = 4:5.16.3-299.el7_9 for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(Socket) >= 1.3 for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(Scalar::Util) >= 1.10 for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl-macros for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl-libs for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(threads::shared) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(threads) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(constant) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(Time::Local) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(Time::HiRes) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(Storable) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(Socket) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(Scalar::Util) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(Pod::Simple::XHTML) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(Pod::Simple::Search) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(Filter::Util::Call) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(Carp) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: libperl.so()(64bit) for package: 4:perl-5.16.3-299.el7_9.x86_64
		---> Package perl-Error.noarch 1:0.17020-2.el7 will be installed
		---> Package perl-Exporter.noarch 0:5.68-3.el7 will be installed
		---> Package perl-File-Path.noarch 0:2.09-2.el7 will be installed
		---> Package perl-File-Temp.noarch 0:0.23.01-3.el7 will be installed
		---> Package perl-Getopt-Long.noarch 0:2.40-3.el7 will be installed
		--> Processing Dependency: perl(Pod::Usage) >= 1.14 for package: perl-Getopt-Long-2.40-3.el7.noarch
		--> Processing Dependency: perl(Text::ParseWords) for package: perl-Getopt-Long-2.40-3.el7.noarch
		---> Package perl-Git.noarch 0:1.8.3.1-23.el7_8 will be installed
		---> Package perl-PathTools.x86_64 0:3.40-5.el7 will be installed
		---> Package perl-TermReadKey.x86_64 0:2.30-20.el7 will be installed
		---> Package rsync.x86_64 0:3.1.2-11.el7_9 will be installed
		--> Running transaction check
		---> Package groff-base.x86_64 0:1.22.2-8.el7 will be installed
		---> Package libedit.x86_64 0:3.0-12.20121213cvs.el7 will be installed
		---> Package openssh.x86_64 0:7.4p1-21.el7 will be updated
		--> Processing Dependency: openssh = 7.4p1-21.el7 for package: openssh-server-7.4p1-21.el7.x86_64
		---> Package openssh.x86_64 0:7.4p1-22.el7_9 will be an update
		---> Package perl-Carp.noarch 0:1.26-244.el7 will be installed
		---> Package perl-Filter.x86_64 0:1.49-3.el7 will be installed
		---> Package perl-Pod-Simple.noarch 1:3.28-4.el7 will be installed
		--> Processing Dependency: perl(Pod::Escapes) >= 1.04 for package: 1:perl-Pod-Simple-3.28-4.el7.noarch
		--> Processing Dependency: perl(Encode) for package: 1:perl-Pod-Simple-3.28-4.el7.noarch
		---> Package perl-Pod-Usage.noarch 0:1.63-3.el7 will be installed
		--> Processing Dependency: perl(Pod::Text) >= 3.15 for package: perl-Pod-Usage-1.63-3.el7.noarch
		--> Processing Dependency: perl-Pod-Perldoc for package: perl-Pod-Usage-1.63-3.el7.noarch
		---> Package perl-Scalar-List-Utils.x86_64 0:1.27-248.el7 will be installed
		---> Package perl-Socket.x86_64 0:2.010-5.el7 will be installed
		---> Package perl-Storable.x86_64 0:2.45-3.el7 will be installed
		---> Package perl-Text-ParseWords.noarch 0:3.29-4.el7 will be installed
		---> Package perl-Time-HiRes.x86_64 4:1.9725-3.el7 will be installed
		---> Package perl-Time-Local.noarch 0:1.2300-2.el7 will be installed
		---> Package perl-constant.noarch 0:1.27-2.el7 will be installed
		---> Package perl-libs.x86_64 4:5.16.3-299.el7_9 will be installed
		---> Package perl-macros.x86_64 4:5.16.3-299.el7_9 will be installed
		---> Package perl-threads.x86_64 0:1.87-4.el7 will be installed
		---> Package perl-threads-shared.x86_64 0:1.43-6.el7 will be installed
		--> Running transaction check
		---> Package openssh-server.x86_64 0:7.4p1-21.el7 will be updated
		---> Package openssh-server.x86_64 0:7.4p1-22.el7_9 will be an update
		---> Package perl-Encode.x86_64 0:2.51-7.el7 will be installed
		---> Package perl-Pod-Escapes.noarch 1:1.04-299.el7_9 will be installed
		---> Package perl-Pod-Perldoc.noarch 0:3.20-4.el7 will be installed
		--> Processing Dependency: perl(parent) for package: perl-Pod-Perldoc-3.20-4.el7.noarch
		--> Processing Dependency: perl(HTTP::Tiny) for package: perl-Pod-Perldoc-3.20-4.el7.noarch
		---> Package perl-podlators.noarch 0:2.5.1-3.el7 will be installed
		--> Running transaction check
		---> Package perl-HTTP-Tiny.noarch 0:0.033-3.el7 will be installed
		---> Package perl-parent.noarch 1:0.225-244.el7 will be installed
		--> Finished Dependency Resolution

		Dependencies Resolved

		=============================================================================================================================================
		 Package                                  Arch                     Version                                   Repository                 Size
		=============================================================================================================================================
		Installing:
		 git                                      x86_64                   1.8.3.1-23.el7_8                          base                      4.4 M
		Installing for dependencies:
		 groff-base                               x86_64                   1.22.2-8.el7                              base                      942 k
		 less                                     x86_64                   458-9.el7                                 base                      120 k
		 libedit                                  x86_64                   3.0-12.20121213cvs.el7                    base                       92 k
		 openssh-clients                          x86_64                   7.4p1-22.el7_9                            updates                   655 k
		 perl                                     x86_64                   4:5.16.3-299.el7_9                        updates                   8.0 M
		 perl-Carp                                noarch                   1.26-244.el7                              base                       19 k
		 perl-Encode                              x86_64                   2.51-7.el7                                base                      1.5 M
		 perl-Error                               noarch                   1:0.17020-2.el7                           base                       32 k
		 perl-Exporter                            noarch                   5.68-3.el7                                base                       28 k
		 perl-File-Path                           noarch                   2.09-2.el7                                base                       26 k
		 perl-File-Temp                           noarch                   0.23.01-3.el7                             base                       56 k
		 perl-Filter                              x86_64                   1.49-3.el7                                base                       76 k
		 perl-Getopt-Long                         noarch                   2.40-3.el7                                base                       56 k
		 perl-Git                                 noarch                   1.8.3.1-23.el7_8                          base                       56 k
		 perl-HTTP-Tiny                           noarch                   0.033-3.el7                               base                       38 k
		 perl-PathTools                           x86_64                   3.40-5.el7                                base                       82 k
		 perl-Pod-Escapes                         noarch                   1:1.04-299.el7_9                          updates                    52 k
		 perl-Pod-Perldoc                         noarch                   3.20-4.el7                                base                       87 k
		 perl-Pod-Simple                          noarch                   1:3.28-4.el7                              base                      216 k
		 perl-Pod-Usage                           noarch                   1.63-3.el7                                base                       27 k
		 perl-Scalar-List-Utils                   x86_64                   1.27-248.el7                              base                       36 k
		 perl-Socket                              x86_64                   2.010-5.el7                               base                       49 k
		 perl-Storable                            x86_64                   2.45-3.el7                                base                       77 k
		 perl-TermReadKey                         x86_64                   2.30-20.el7                               base                       31 k
		 perl-Text-ParseWords                     noarch                   3.29-4.el7                                base                       14 k
		 perl-Time-HiRes                          x86_64                   4:1.9725-3.el7                            base                       45 k
		 perl-Time-Local                          noarch                   1.2300-2.el7                              base                       24 k
		 perl-constant                            noarch                   1.27-2.el7                                base                       19 k
		 perl-libs                                x86_64                   4:5.16.3-299.el7_9                        updates                   690 k
		 perl-macros                              x86_64                   4:5.16.3-299.el7_9                        updates                    44 k
		 perl-parent                              noarch                   1:0.225-244.el7                           base                       12 k
		 perl-podlators                           noarch                   2.5.1-3.el7                               base                      112 k
		 perl-threads                             x86_64                   1.87-4.el7                                base                       49 k
		 perl-threads-shared                      x86_64                   1.43-6.el7                                base                       39 k
		 rsync                                    x86_64                   3.1.2-11.el7_9                            updates                   408 k
		Updating for dependencies:
		 openssh                                  x86_64                   7.4p1-22.el7_9                            updates                   510 k
		 openssh-server                           x86_64                   7.4p1-22.el7_9                            updates                   459 k

		Transaction Summary
		=============================================================================================================================================
		Install  1 Package  (+35 Dependent packages)
		Upgrade             (  2 Dependent packages)

		Total download size: 19 M
		Downloading packages:
		Delta RPMs disabled because /usr/bin/applydeltarpm not installed.
		(1/38): groff-base-1.22.2-8.el7.x86_64.rpm                                                                            | 942 kB  00:00:00     
		(2/38): git-1.8.3.1-23.el7_8.x86_64.rpm                                                                               | 4.4 MB  00:00:00     
		(3/38): less-458-9.el7.x86_64.rpm                                                                                     | 120 kB  00:00:00     
		(4/38): libedit-3.0-12.20121213cvs.el7.x86_64.rpm                                                                     |  92 kB  00:00:00     
		(5/38): openssh-7.4p1-22.el7_9.x86_64.rpm                                                                             | 510 kB  00:00:00     
		(6/38): openssh-clients-7.4p1-22.el7_9.x86_64.rpm                                                                     | 655 kB  00:00:00     
		(7/38): openssh-server-7.4p1-22.el7_9.x86_64.rpm                                                                      | 459 kB  00:00:00     
		(8/38): perl-Carp-1.26-244.el7.noarch.rpm                                                                             |  19 kB  00:00:00     
		(9/38): perl-Error-0.17020-2.el7.noarch.rpm                                                                           |  32 kB  00:00:00     
		(10/38): perl-Exporter-5.68-3.el7.noarch.rpm                                                                          |  28 kB  00:00:00     
		(11/38): perl-File-Path-2.09-2.el7.noarch.rpm                                                                         |  26 kB  00:00:00     
		(12/38): perl-File-Temp-0.23.01-3.el7.noarch.rpm                                                                      |  56 kB  00:00:00     
		(13/38): perl-Filter-1.49-3.el7.x86_64.rpm                                                                            |  76 kB  00:00:00     
		(14/38): perl-Getopt-Long-2.40-3.el7.noarch.rpm                                                                       |  56 kB  00:00:00     
		(15/38): perl-Git-1.8.3.1-23.el7_8.noarch.rpm                                                                         |  56 kB  00:00:00     
		(16/38): perl-HTTP-Tiny-0.033-3.el7.noarch.rpm                                                                        |  38 kB  00:00:00     
		(17/38): perl-PathTools-3.40-5.el7.x86_64.rpm                                                                         |  82 kB  00:00:00     
		(18/38): perl-Pod-Escapes-1.04-299.el7_9.noarch.rpm                                                                   |  52 kB  00:00:00     
		(19/38): perl-Encode-2.51-7.el7.x86_64.rpm                                                                            | 1.5 MB  00:00:00     
		(20/38): perl-Pod-Perldoc-3.20-4.el7.noarch.rpm                                                                       |  87 kB  00:00:00     
		(21/38): perl-5.16.3-299.el7_9.x86_64.rpm                                                                             | 8.0 MB  00:00:00     
		(22/38): perl-Pod-Usage-1.63-3.el7.noarch.rpm                                                                         |  27 kB  00:00:00     
		(23/38): perl-Pod-Simple-3.28-4.el7.noarch.rpm                                                                        | 216 kB  00:00:00     
		(24/38): perl-Scalar-List-Utils-1.27-248.el7.x86_64.rpm                                                               |  36 kB  00:00:00     
		(25/38): perl-Storable-2.45-3.el7.x86_64.rpm                                                                          |  77 kB  00:00:00     
		(26/38): perl-TermReadKey-2.30-20.el7.x86_64.rpm                                                                      |  31 kB  00:00:00     
		(27/38): perl-Text-ParseWords-3.29-4.el7.noarch.rpm                                                                   |  14 kB  00:00:00     
		(28/38): perl-Time-HiRes-1.9725-3.el7.x86_64.rpm                                                                      |  45 kB  00:00:00     
		(29/38): perl-Time-Local-1.2300-2.el7.noarch.rpm                                                                      |  24 kB  00:00:00     
		(30/38): perl-Socket-2.010-5.el7.x86_64.rpm                                                                           |  49 kB  00:00:00     
		(31/38): perl-constant-1.27-2.el7.noarch.rpm                                                                          |  19 kB  00:00:00     
		(32/38): perl-parent-0.225-244.el7.noarch.rpm                                                                         |  12 kB  00:00:00     
		(33/38): perl-podlators-2.5.1-3.el7.noarch.rpm                                                                        | 112 kB  00:00:00     
		(34/38): perl-libs-5.16.3-299.el7_9.x86_64.rpm                                                                        | 690 kB  00:00:00     
		(35/38): perl-threads-shared-1.43-6.el7.x86_64.rpm                                                                    |  39 kB  00:00:00     
		(36/38): perl-threads-1.87-4.el7.x86_64.rpm                                                                           |  49 kB  00:00:00     
		(37/38): rsync-3.1.2-11.el7_9.x86_64.rpm                                                                              | 408 kB  00:00:00     
		(38/38): perl-macros-5.16.3-299.el7_9.x86_64.rpm                                                                      |  44 kB  00:00:00     
		---------------------------------------------------------------------------------------------------------------------------------------------
		Total                                                                                                         18 MB/s |  19 MB  00:00:01     
		Running transaction check
		Running transaction test
		Transaction test succeeded
		Running transaction
		  Installing : groff-base-1.22.2-8.el7.x86_64                                                                                           1/40 
		  Updating   : openssh-7.4p1-22.el7_9.x86_64                                                                                            2/40 
		  Installing : 1:perl-parent-0.225-244.el7.noarch                                                                                       3/40 
		  Installing : perl-HTTP-Tiny-0.033-3.el7.noarch                                                                                        4/40 
		  Installing : perl-podlators-2.5.1-3.el7.noarch                                                                                        5/40 
		  Installing : perl-Pod-Perldoc-3.20-4.el7.noarch                                                                                       6/40 
		  Installing : 1:perl-Pod-Escapes-1.04-299.el7_9.noarch                                                                                 7/40 
		  Installing : perl-Encode-2.51-7.el7.x86_64                                                                                            8/40 
		  Installing : perl-Text-ParseWords-3.29-4.el7.noarch                                                                                   9/40 
		  Installing : perl-Pod-Usage-1.63-3.el7.noarch                                                                                        10/40 
		  Installing : 4:perl-macros-5.16.3-299.el7_9.x86_64                                                                                   11/40 
		  Installing : 4:perl-Time-HiRes-1.9725-3.el7.x86_64                                                                                   12/40 
		  Installing : perl-Exporter-5.68-3.el7.noarch                                                                                         13/40 
		  Installing : perl-constant-1.27-2.el7.noarch                                                                                         14/40 
		  Installing : perl-Socket-2.010-5.el7.x86_64                                                                                          15/40 
		  Installing : perl-Time-Local-1.2300-2.el7.noarch                                                                                     16/40 
		  Installing : perl-Carp-1.26-244.el7.noarch                                                                                           17/40 
		  Installing : perl-Storable-2.45-3.el7.x86_64                                                                                         18/40 
		  Installing : perl-PathTools-3.40-5.el7.x86_64                                                                                        19/40 
		  Installing : perl-Scalar-List-Utils-1.27-248.el7.x86_64                                                                              20/40 
		  Installing : 1:perl-Pod-Simple-3.28-4.el7.noarch                                                                                     21/40 
		  Installing : perl-File-Temp-0.23.01-3.el7.noarch                                                                                     22/40 
		  Installing : perl-File-Path-2.09-2.el7.noarch                                                                                        23/40 
		  Installing : perl-threads-shared-1.43-6.el7.x86_64                                                                                   24/40 
		  Installing : perl-threads-1.87-4.el7.x86_64                                                                                          25/40 
		  Installing : perl-Filter-1.49-3.el7.x86_64                                                                                           26/40 
		  Installing : 4:perl-libs-5.16.3-299.el7_9.x86_64                                                                                     27/40 
		  Installing : perl-Getopt-Long-2.40-3.el7.noarch                                                                                      28/40 
		  Installing : 4:perl-5.16.3-299.el7_9.x86_64                                                                                          29/40 
		  Installing : 1:perl-Error-0.17020-2.el7.noarch                                                                                       30/40 
		  Installing : perl-TermReadKey-2.30-20.el7.x86_64                                                                                     31/40 
		  Installing : less-458-9.el7.x86_64                                                                                                   32/40 
		  Installing : libedit-3.0-12.20121213cvs.el7.x86_64                                                                                   33/40 
		  Installing : openssh-clients-7.4p1-22.el7_9.x86_64                                                                                   34/40 
		  Installing : rsync-3.1.2-11.el7_9.x86_64                                                                                             35/40 
		  Installing : perl-Git-1.8.3.1-23.el7_8.noarch                                                                                        36/40 
		  Installing : git-1.8.3.1-23.el7_8.x86_64                                                                                             37/40 
		  Updating   : openssh-server-7.4p1-22.el7_9.x86_64                                                                                    38/40 
		  Cleanup    : openssh-server-7.4p1-21.el7.x86_64                                                                                      39/40 
		  Cleanup    : openssh-7.4p1-21.el7.x86_64                                                                                             40/40 
		  Verifying  : perl-HTTP-Tiny-0.033-3.el7.noarch                                                                                        1/40 
		  Verifying  : perl-threads-shared-1.43-6.el7.x86_64                                                                                    2/40 
		  Verifying  : 4:perl-Time-HiRes-1.9725-3.el7.x86_64                                                                                    3/40 
		  Verifying  : openssh-clients-7.4p1-22.el7_9.x86_64                                                                                    4/40 
		  Verifying  : perl-Exporter-5.68-3.el7.noarch                                                                                          5/40 
		  Verifying  : perl-constant-1.27-2.el7.noarch                                                                                          6/40 
		  Verifying  : perl-PathTools-3.40-5.el7.x86_64                                                                                         7/40 
		  Verifying  : openssh-7.4p1-22.el7_9.x86_64                                                                                            8/40 
		  Verifying  : 4:perl-macros-5.16.3-299.el7_9.x86_64                                                                                    9/40 
		  Verifying  : git-1.8.3.1-23.el7_8.x86_64                                                                                             10/40 
		  Verifying  : 1:perl-parent-0.225-244.el7.noarch                                                                                      11/40 
		  Verifying  : perl-Socket-2.010-5.el7.x86_64                                                                                          12/40 
		  Verifying  : rsync-3.1.2-11.el7_9.x86_64                                                                                             13/40 
		  Verifying  : perl-TermReadKey-2.30-20.el7.x86_64                                                                                     14/40 
		  Verifying  : groff-base-1.22.2-8.el7.x86_64                                                                                          15/40 
		  Verifying  : perl-File-Temp-0.23.01-3.el7.noarch                                                                                     16/40 
		  Verifying  : 1:perl-Pod-Simple-3.28-4.el7.noarch                                                                                     17/40 
		  Verifying  : perl-Time-Local-1.2300-2.el7.noarch                                                                                     18/40 
		  Verifying  : 1:perl-Pod-Escapes-1.04-299.el7_9.noarch                                                                                19/40 
		  Verifying  : perl-Git-1.8.3.1-23.el7_8.noarch                                                                                        20/40 
		  Verifying  : perl-Carp-1.26-244.el7.noarch                                                                                           21/40 
		  Verifying  : 1:perl-Error-0.17020-2.el7.noarch                                                                                       22/40 
		  Verifying  : perl-Storable-2.45-3.el7.x86_64                                                                                         23/40 
		  Verifying  : perl-Scalar-List-Utils-1.27-248.el7.x86_64                                                                              24/40 
		  Verifying  : perl-Pod-Usage-1.63-3.el7.noarch                                                                                        25/40 
		  Verifying  : perl-Encode-2.51-7.el7.x86_64                                                                                           26/40 
		  Verifying  : perl-Pod-Perldoc-3.20-4.el7.noarch                                                                                      27/40 
		  Verifying  : perl-podlators-2.5.1-3.el7.noarch                                                                                       28/40 
		  Verifying  : 4:perl-5.16.3-299.el7_9.x86_64                                                                                          29/40 
		  Verifying  : perl-File-Path-2.09-2.el7.noarch                                                                                        30/40 
		  Verifying  : libedit-3.0-12.20121213cvs.el7.x86_64                                                                                   31/40 
		  Verifying  : perl-threads-1.87-4.el7.x86_64                                                                                          32/40 
		  Verifying  : openssh-server-7.4p1-22.el7_9.x86_64                                                                                    33/40 
		  Verifying  : perl-Filter-1.49-3.el7.x86_64                                                                                           34/40 
		  Verifying  : perl-Getopt-Long-2.40-3.el7.noarch                                                                                      35/40 
		  Verifying  : perl-Text-ParseWords-3.29-4.el7.noarch                                                                                  36/40 
		  Verifying  : 4:perl-libs-5.16.3-299.el7_9.x86_64                                                                                     37/40 
		  Verifying  : less-458-9.el7.x86_64                                                                                                   38/40 
		  Verifying  : openssh-7.4p1-21.el7.x86_64                                                                                             39/40 
		  Verifying  : openssh-server-7.4p1-21.el7.x86_64                                                                                      40/40 

		Installed:
		  git.x86_64 0:1.8.3.1-23.el7_8                                                                                                              

		Dependency Installed:
		  groff-base.x86_64 0:1.22.2-8.el7             less.x86_64 0:458-9.el7                      libedit.x86_64 0:3.0-12.20121213cvs.el7         
		  openssh-clients.x86_64 0:7.4p1-22.el7_9      perl.x86_64 4:5.16.3-299.el7_9               perl-Carp.noarch 0:1.26-244.el7                 
		  perl-Encode.x86_64 0:2.51-7.el7              perl-Error.noarch 1:0.17020-2.el7            perl-Exporter.noarch 0:5.68-3.el7               
		  perl-File-Path.noarch 0:2.09-2.el7           perl-File-Temp.noarch 0:0.23.01-3.el7        perl-Filter.x86_64 0:1.49-3.el7                 
		  perl-Getopt-Long.noarch 0:2.40-3.el7         perl-Git.noarch 0:1.8.3.1-23.el7_8           perl-HTTP-Tiny.noarch 0:0.033-3.el7             
		  perl-PathTools.x86_64 0:3.40-5.el7           perl-Pod-Escapes.noarch 1:1.04-299.el7_9     perl-Pod-Perldoc.noarch 0:3.20-4.el7            
		  perl-Pod-Simple.noarch 1:3.28-4.el7          perl-Pod-Usage.noarch 0:1.63-3.el7           perl-Scalar-List-Utils.x86_64 0:1.27-248.el7    
		  perl-Socket.x86_64 0:2.010-5.el7             perl-Storable.x86_64 0:2.45-3.el7            perl-TermReadKey.x86_64 0:2.30-20.el7           
		  perl-Text-ParseWords.noarch 0:3.29-4.el7     perl-Time-HiRes.x86_64 4:1.9725-3.el7        perl-Time-Local.noarch 0:1.2300-2.el7           
		  perl-constant.noarch 0:1.27-2.el7            perl-libs.x86_64 4:5.16.3-299.el7_9          perl-macros.x86_64 4:5.16.3-299.el7_9           
		  perl-parent.noarch 1:0.225-244.el7           perl-podlators.noarch 0:2.5.1-3.el7          perl-threads.x86_64 0:1.87-4.el7                
		  perl-threads-shared.x86_64 0:1.43-6.el7      rsync.x86_64 0:3.1.2-11.el7_9               

		Dependency Updated:
		  openssh.x86_64 0:7.4p1-22.el7_9                                   openssh-server.x86_64 0:7.4p1-22.el7_9                                  

		Complete!

[root@ststor01 ~]# rpm -qa |grep git
		git-1.8.3.1-23.el7_8.x86_64

3. Go to mentioned folder and init a Bare git repo 

[root@ststor01 ~]# cd /opt

[root@ststor01 opt]# ls -ahl
		total 8.0K
		drwxr-xr-x 1 root root 4.0K Apr 11  2018 .
		drwxr-xr-x 1 root root 4.0K Nov  5 15:29 ..
 
[root@ststor01 opt]# git init --bare news.git
		Initialized empty Git repository in /opt/news.git/

[root@ststor01 opt]# ls -ahl
		total 12K
		drwxr-xr-x 1 root root 4.0K Nov  5 15:32 .
		drwxr-xr-x 1 root root 4.0K Nov  5 15:29 ..
		drwxr-xr-x 7 root root 4.0K Nov  5 15:32 news.git

[root@ststor01 opt]# cd news.git/

[root@ststor01 news.git]# git status
		fatal: This operation must be run in a work tree

[root@ststor01 news.git]# ls -ahl
		total 40K
		drwxr-xr-x 7 root root 4.0K Nov  5 15:32 .
		drwxr-xr-x 1 root root 4.0K Nov  5 15:32 ..
		drwxr-xr-x 2 root root 4.0K Nov  5 15:32 branches
		-rw-r--r-- 1 root root   66 Nov  5 15:32 config
		-rw-r--r-- 1 root root   73 Nov  5 15:32 description
		-rw-r--r-- 1 root root   23 Nov  5 15:32 HEAD
		drwxr-xr-x 2 root root 4.0K Nov  5 15:32 hooks
		drwxr-xr-x 2 root root 4.0K Nov  5 15:32 info
		drwxr-xr-x 4 root root 4.0K Nov  5 15:32 objects
		drwxr-xr-x 4 root root 4.0K Nov  5 15:32 refs

[root@ststor01 news.git]# 

--------------------------------------------------------------------------------------------------------------------------------
Task 101: 16/Nov/2022

Deploy Nginx and Phpfpm on Kubernetes

The Nautilus Application Development team is planning to deploy one of the php-based application on Kubernetes cluster. As per discussion with DevOps team they have decided to use nginx and phpfpm. Additionally, they shared some custom configuration requirements. Below you can find more details. Please complete the task as per requirements mentioned below:

1) Create a service to expose this app, the service type must be NodePort, nodePort should be 30012.

2.) Create a config map nginx-config for nginx.conf as we want to add some custom settings for nginx.conf.

a) Change default port 80 to 8091 in nginx.conf.

b) Change default document root /usr/share/nginx to /var/www/html in nginx.conf.

c) Update directory index to index index.html index.htm index.php in nginx.conf.

3.) Create a pod named nginx-phpfpm .

b) Create a shared volume shared-files that will be used by both containers (nginx and phpfpm) also it should be a emptyDir volume.

c) Map the ConfigMap we declared above as a volume for nginx container. Name the volume as nginx-config-volume, mount path should be /etc/nginx/nginx.conf and subPath should be nginx.conf

d) Nginx container should be named as nginx-container and it should use nginx:latest image. PhpFPM container should be named as php-fpm-container and it should use php:7.0-fpm image.

e) The shared volume shared-files should be mounted at /var/www/html location in both containers. Copy /opt/index.php from jump host to the nginx document root inside nginx container, once done you can access the app using App button on the top bar.

Before clicking on finish button always make sure to check if all pods are in running state.

You can use any labels as per your choice.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

1. Check kubectl utility and current configuration

thor@jump_host ~$ kubectl get all
		NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
		service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   5h9m

2. Create the config YAML file as per the requirements 

thor@jump_host ~$ vi /tmp/nginx_phpfpm.yml

thor@jump_host ~$ cat /tmp/nginx_phpfpm.yml 
		#ConfigMap Configuration
		---
		apiVersion: v1
		kind: ConfigMap
		metadata:
		  name: nginx-config
		data:
		  nginx.conf: |
		    events {} 
		    http {
		      server {
		        listen 8091;
		        index index.html index.htm index.php;
		        root  /var/www/html;
		        location ~ \.php$ {
		          include fastcgi_params;
		          fastcgi_param REQUEST_METHOD $request_method;
		          fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
		          fastcgi_pass 127.0.0.1:9000;
		        }
		      }
		    }


		#Pod Configuration    
		---
		apiVersion: v1
		kind: Pod
		metadata:
		  name: nginx-phpfpm
		  labels:
		     app: nginx-phpfpm
		     tier: back-end
		spec:
		  volumes:
		    - name: shared-files
		      emptyDir: {}
		    - name: nginx-config-volume
		      configMap:
		        name: nginx-config
		  containers:
		    - name: nginx-container
		      image: nginx:latest
		      volumeMounts:
		        - name: shared-files
		          mountPath: /var/www/html
		        - name: nginx-config-volume
		          mountPath: /etc/nginx/nginx.conf
		          subPath: nginx.conf
		    - name: php-fpm-container
		      image: php:7.3-fpm
		      volumeMounts:
		        - name: shared-files
		          mountPath: /var/www/html

		#Service configuration
		---
		apiVersion: v1
		kind: Service
		metadata:
		  name: nginx-phpfpm
		spec:
		  type: NodePort
		  selector:
		    app: nginx-phpfpm
		    tier: back-end
		  ports:
		    - port: 8091
		      targetPort: 8091
		      nodePort: 30012

3. Create the service and pods

thor@jump_host ~$ kubectl create -f /tmp/nginx_phpfpm.yml 
		configmap/nginx-config created
		pod/nginx-phpfpm created
		service/nginx-phpfpm created

4. Check the configuration applied and wait for pod to get running  

thor@jump_host ~$ kubectl get all
		NAME               READY   STATUS              RESTARTS   AGE
		pod/nginx-phpfpm   0/2     ContainerCreating   0          10s

		NAME                   TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE
		service/kubernetes     ClusterIP   10.96.0.1     <none>        443/TCP          5h11m
		service/nginx-phpfpm   NodePort    10.96.77.59   <none>        8091:30012/TCP   10s
 
thor@jump_host ~$ kubectl get all
		NAME               READY   STATUS    RESTARTS   AGE
		pod/nginx-phpfpm   2/2     Running   0          60s

		NAME                   TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE
		service/kubernetes     ClusterIP   10.96.0.1     <none>        443/TCP          5h12m
		service/nginx-phpfpm   NodePort    10.96.77.59   <none>        8091:30012/TCP   60s

5. Copy the mentioned file in the given pod

thor@jump_host ~$ kubectl cp /opt/index.php nginx-phpfpm:/var/www/html --container=nginx-container

6. Validate by clicking on the App Button top right side side of the terminal. If the app is running then the task is done.

--------------------------------------------------------------------------------------------------------------------------------
Task 102: 18/Nov/2022

Run a Docker Container


Nautilus DevOps team is testing some applications deployment on some of the application servers. They need to deploy a nginx container on Application Server 3. Please complete the task as per details given below:

    On Application Server 3 create a container named nginx_3 using image nginx with alpine tag and make sure container is in running state.

1.Login to app server 3 and switch to root user 

thor@jump_host ~$ ssh banner@stapp03
		The authenticity of host 'stapp03 (172.16.238.12)' can't be established.
		ECDSA key fingerprint is SHA256:Zf7WuEVcvpXAR1ITq9DoENoxEcCaCyV0Y4JyuAJXgFc.
		ECDSA key fingerprint is MD5:48:49:21:90:b7:aa:00:ed:df:0f:a2:fb:b0:5d:92:8f.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'stapp03,172.16.238.12' (ECDSA) to the list of known hosts.
		banner@stapp03's password: 

[banner@stapp03 ~]$ sudo su -

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for banner: 

2. Check existing running docker images and containers

[root@stapp03 ~]# docker ps
		CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES

[root@stapp03 ~]# docker images
		REPOSITORY   TAG       IMAGE ID   CREATED   SIZE

3. Run the docker container as per requirement using the given image and name  

[root@stapp03 ~]# docker run -d --name nginx_3 -p 8080:80 nginx:alpine
		Unable to find image 'nginx:alpine' locally
		alpine: Pulling from library/nginx
		ca7dd9ec2225: Pull complete 
		76a48b0f5898: Pull complete 
		2f12a0e7c01d: Pull complete 
		1a7b9b9bbef6: Pull complete 
		b704883c57af: Pull complete 
		4342b1ab302e: Pull complete 
		Digest: sha256:455c39afebd4d98ef26dd70284aa86e6810b0485af5f4f222b19b89758cabf1e
		Status: Downloaded newer image for nginx:alpine
		d022a38a3abd331cea55f3b89f1da16373a3b58d34f9f6d08d554c37545554a5

4. Check the docker running status

[root@stapp03 ~]# docker ps
		CONTAINER ID   IMAGE          COMMAND                  CREATED         STATUS         PORTS                  NAMES
		d022a38a3abd   nginx:alpine   "/docker-entrypoint.…"   8 seconds ago   Up 5 seconds   0.0.0.0:8080->80/tcp   nginx_3

[root@stapp03 ~]# docker images
		REPOSITORY   TAG       IMAGE ID       CREATED      SIZE
		nginx        alpine    19dd4d73108a   6 days ago   23.5MB

5. Validate by clicking on the view port top right side side of the terminal and select port 80. If the container is running then we see nginx page and the task is done.

--------------------------------------------------------------------------------------------------------------------------------
Task 103: 20/Nov/2022

Git Create Branches

Nautilus developers are actively working on one of the project repositories, /usr/src/kodekloudrepos/apps. They recently decided to implement some new features in the application, and they want to maintain those new changes in a separate branch. Below are the requirements that have been shared with the DevOps team:

    On Storage server in Stratos DC create a new branch xfusioncorp_apps from master branch in /usr/src/kodekloudrepos/apps git repo.

    Please do not try to make any changes in code.


1. Login to Storage server and switch to root user 

thor@jump_host ~$ ssh natasha@ststor01
		The authenticity of host 'ststor01 (172.16.238.15)' can't be established.
		ECDSA key fingerprint is SHA256:bNwXFd+JrQ3QZwtPJNbAxE+nIRPwx+rAC8iwQL2UNUs.
		ECDSA key fingerprint is MD5:a1:23:7e:da:b3:40:86:fc:22:47:b7:f1:15:7d:ab:b3.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'ststor01,172.16.238.15' (ECDSA) to the list of known hosts.
		natasha@ststor01's password: 

[natasha@ststor01 ~]$ sudo su - 

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for natasha: 

2. Go to mentioned repository
[root@ststor01 ~]# cd /usr/src/kodekloudrepos/apps/

3. List current branches

[root@ststor01 apps]# git branch -a
		* kodekloud_apps
		  master
		  remotes/origin/master

4. Switch to master branch as we are required to create new branch from master 

[root@ststor01 apps]# git checkout master
		Switched to branch 'master'

[root@ststor01 apps]# git branch -a
		  kodekloud_apps
		* master
		  remotes/origin/master

[root@ststor01 apps]# git status
		# On branch master
		nothing to commit, working directory clean

5. Create the mentioned branch and checkout to it .

[root@ststor01 apps]# git branch xfusioncorp_apps
 
[root@ststor01 apps]# git checkout xfusioncorp_apps
		Switched to branch 'xfusioncorp_apps'

6. Validate the task 

[root@ststor01 apps]# git branch -a
		  kodekloud_apps
		  master
		* xfusioncorp_apps
		  remotes/origin/master

[root@ststor01 apps]# git status
		# On branch xfusioncorp_apps
		nothing to commit, working directory clean

--------------------------------------------------------------------------------------------------------------------------------
Task 104: 28/Nov/2022

Kubernetes Troubleshooting

One of the Nautilus DevOps team members was working on to update an existing Kubernetes template. Somehow, he made some mistakes in the template and it is failing while applying. We need to fix this as soon as possible, so take a look into it and make sure you are able to apply it without any issues. Also, do not remove any component from the template like pods/deployments/volumes etc.

/home/thor/mysql_deployment.yml is the template that needs to be fixed.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.


thor@jump_host ~$ pwd 
		/home/thor
thor@jump_host ~$ ls -ahl
		total 36K
		drwxr----- 1 thor thor 4.0K Nov 28 02:58 .
		drwxr-xr-x 1 root root 4.0K Oct 31  2020 ..
		-rwxrwx--- 1 thor thor   18 Oct 30  2018 .bash_logout
		-rwxrwx--- 1 thor thor  193 Oct 30  2018 .bash_profile
		-rwxrwx--- 1 thor thor  510 Nov 28 02:58 .bashrc
		drwxrwx--- 1 thor thor 4.0K Oct 31  2020 .config
		drwxrwx--- 2 thor thor 4.0K Nov 28 02:58 .kube
		-rw-r--r-- 1 thor thor 2.3K Nov 28 02:58 mysql_deployment.yml
		drwx------ 2 thor thor 4.0K Nov 28 02:58 .ssh

thor@jump_host ~$ kubectl get all
		NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
		service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   6h6m
 
thor@jump_host ~$ kubectl get secrets
		NAME                  TYPE                                  DATA   AGE
		default-token-z8mrm   kubernetes.io/service-account-token   3      6h6m
		mysql-db-url          Opaque                                1      37s
		mysql-root-pass       Opaque                                1      38s
		mysql-user-pass       Opaque                                2      37s
 
thor@jump_host ~$ cat mysql_deployment.yml 
		apiVersion: apps/v1 
		kind: PersistentVolume            
		metadata:
		  name: mysql-pv
		  labels: 
		  type: local 
		spec:
		  storageClassName: standard      
		  capacity:
		    storage: 250Mi
		  accessModes: 
		    - ReadWriteOnce
		  hostPath:                       
		  path: "/mnt/data" 
		  persistentVolumeReclaimPolicy:  
		  -  Retain  
		---    
		apiVersion: apps/v1 
		kind: PersistentVolumeClaim 
		metadata:                          
		  name: mysql-pv-claim
		  labels:
		  app: mysql-app 
		spec:                              
		  storageClassName: standard       
		  accessModes:
		    - ReadWriteOnce                
		  resources:
		    requests: 
		      storage: 250MB 
		---
		apiVersion: v1                    
		kind: Service                      
		metadata:
		  name: mysql         
		  labels:             
		    app: mysql-app
		spec:
		  type: NodePort
		  ports:
		    - targetPort: 3306
		      port: 3306
		      nodePort: 30011
		  selector:    
		    app: mysql-app
		  tier: mysql
		---
		apiVersion: v1 
		kind: Deployment            
		metadata:
		  name: mysql-deployment       
		  labels:                       
		    app: mysql-app 
		spec:
		  selector:
		    matchLabels:
		      app: mysql-app
		    tier: mysql 
		  strategy:
		    type: Recreate
		  template:                    
		    metadata:
		      labels:                  
		        app: mysql-app
		      tier: mysql 
		    spec:                       
		      containers: 
		      - images: mysql:5.6 
		        name: mysql
		        env:                        
		        - name: MYSQL_ROOT_PASSWORD 
		          valueFrom:                
		          secretKeyRef: 
		            name: mysql-root-pass 
		              key: password 
		        - name: MYSQL_DATABASE
		          valueFrom:
		          secretKeyRef: 
		            name: mysql-db-url 
		              key: database 
		        - name: MYSQL_USER
		          valueFrom:
		            secretKeyRef:
		              name: mysql-user-pass
		              key: username
		        - name: MYSQL_PASSWORD
		          valueFrom:
		            secretKeyRef:
		              name: mysql-user-pass
		              key: password
		        ports:
		        - containerPort: 3306        
		          name: mysql
		        volumeMounts:
		        - name: mysql-persistent-storage 
		          mountPath: /var/lib/mysql
		      volumes:                       
		      - name: mysql-persistent-storage
		          persistentVolumeClaim:
		          claimName: mysql-pv-claim

thor@jump_host ~$ kubectl create -f mysql_deployment.yml 
		unable to recognize "mysql_deployment.yml": no matches for kind "PersistentVolume" in version "apps/v1"
		unable to recognize "mysql_deployment.yml": no matches for kind "PersistentVolumeClaim" in version "apps/v1"
		error validating "mysql_deployment.yml": error validating data: ValidationError(Service.spec): unknown field "tier" in io.k8s.api.core.v1.ServiceSpec; if you choose to ignore these errors, turn validation off with --validate=false


thor@jump_host ~$ vi mysql_pv.yml
thor@jump_host ~$ cat mysql_pv.yml
		---
		apiVersion: apps/v1 
		kind: PersistentVolume            
		metadata:
		  name: mysql-pv
		  labels: 
		  type: local 
		spec:
		  storageClassName: standard      
		  capacity:
		    storage: 250Mi
		  accessModes: 
		    - ReadWriteOnce
		  hostPath:                       
		  path: "/mnt/data" 
		  persistentVolumeReclaimPolicy:  
		  -  Retain  

thor@jump_host ~$ kubectl create -f mysql_pv.yml 
		error: unable to recognize "mysql_pv.yml": no matches for kind "PersistentVolume" in version "apps/v1"

thor@jump_host ~$ vi mysql_pv.yml
thor@jump_host ~$ cat mysql_pv.yml
		---
		apiVersion: v1 
		kind: PersistentVolume            
		metadata:
		  name: mysql-pv
		  labels: 
		  type: local 
		spec:
		  storageClassName: standard      
		  capacity:
		    storage: 250Mi
		  accessModes: 
		  - ReadWriteOnce
		  hostPath:                       
		    path: "/mnt/data" 
		  persistentVolumeReclaimPolicy: Retain  

thor@jump_host ~$ kubectl create -f mysql_pv.yml 
		error: error validating "mysql_pv.yml": error validating data: ValidationError(PersistentVolume.metadata): unknown field "type" in io.k8s.apimachinery.pkg.apis.meta.v1.ObjectMeta; if you choose to ignore these errors, turn validation off with --validate=false

thor@jump_host ~$ vi mysql_pv.yml
thor@jump_host ~$ cat mysql_pv.yml
		---
		apiVersion: v1 																																										<---------------
		kind: PersistentVolume            
		metadata:
		  name: mysql-pv
		  labels: 
		    type: local 																																									<---------------
		spec:
		  storageClassName: standard      
		  capacity:
		    storage: 250Mi
		  accessModes: 
		  - ReadWriteOnce																																									<---------------
		  hostPath:                       
		    path: "/mnt/data" 
		  persistentVolumeReclaimPolicy: Retain 																													<--------------- 

thor@jump_host ~$ kubectl create -f mysql_pv.yml 
		persistentvolume/mysql-pv created
 
thor@jump_host ~$ vi mysql_pvc.yml
thor@jump_host ~$ cat mysql_pvc.yml
		---    
		apiVersion: apps/v1 
		kind: PersistentVolumeClaim 
		metadata:                          
		  name: mysql-pv-claim
		  labels:
		  app: mysql-app 
		spec:                              
		  storageClassName: standard       
		  accessModes:
		    - ReadWriteOnce                
		  resources:
		    requests: 
		      storage: 250MB 

thor@jump_host ~$ kubectl create -f  mysql_pvc.yml
		error: unable to recognize "mysql_pvc.yml": no matches for kind "PersistentVolumeClaim" in version "apps/v1"

thor@jump_host ~$ vi mysql_pvc.yml
thor@jump_host ~$ cat mysql_pvc.yml
		---    
		apiVersion: v1 																																									<---------------
		kind: PersistentVolumeClaim 
		metadata:                          
		  name: mysql-pv-claim
		  labels:
		    app: mysql-app 																																							<---------------
		spec:                              
		  storageClassName: standard       
		  accessModes:
		  - ReadWriteOnce 																																							<---------------               
		  resources:
		    requests: 
		      storage: 250Mi 																																						<---------------

thor@jump_host ~$ kubectl create -f  mysql_pvc.yml
		persistentvolumeclaim/mysql-pv-claim created
 
thor@jump_host ~$ vi mysql_service.yml
thor@jump_host ~$ cat mysql_service.yml
		---
		apiVersion: v1                    
		kind: Service                      
		metadata:
		  name: mysql         
		  labels:             
		    app: mysql-app
		spec:
		  type: NodePort
		  ports:
		    - targetPort: 3306
		      port: 3306
		      nodePort: 30011
		  selector:    
		    app: mysql-app
		  tier: mysql

thor@jump_host ~$ kubectl create -f mysql_service.yml
		error: error validating "mysql_service.yml": error validating data: ValidationError(Service.spec): unknown field "tier" in io.k8s.api.core.v1.ServiceSpec; if you choose to ignore these errors, turn validation off with --validate=false

thor@jump_host ~$ vi mysql_service.yml
thor@jump_host ~$ cat mysql_service.yml
		---
		apiVersion: v1                    
		kind: Service                      
		metadata:
		  name: mysql         
		  labels:             
		    app: mysql-app
		spec:
		  type: NodePort
		  ports:
		    - targetPort: 3306
		      port: 3306
		      nodePort: 30011
		  selector:    
		    app: mysql-app
		    tier: mysql 																																							<---------------																						
thor@jump_host ~$ kubectl create -f mysql_service.yml
		service/mysql created

thor@jump_host ~$ vi mysql_deploy.yml
thor@jump_host ~$ cat mysql_deploy.yml
		---
		apiVersion: v1 
		kind: Deployment            
		metadata:
		  name: mysql-deployment       
		  labels:                       
		    app: mysql-app 
		spec:
		  selector:
		    matchLabels:
		      app: mysql-app
		    tier: mysql 
		  strategy:
		    type: Recreate
		  template:                    
		    metadata:
		      labels:                  
		        app: mysql-app
		      tier: mysql 
		    spec:                       
		      containers: 
		      - images: mysql:5.6 
		        name: mysql
		        env:                        
		        - name: MYSQL_ROOT_PASSWORD 
		          valueFrom:                
		          secretKeyRef: 
		            name: mysql-root-pass 
		              key: password 
		        - name: MYSQL_DATABASE
		          valueFrom:
		          secretKeyRef: 
		            name: mysql-db-url 
		              key: database 
		        - name: MYSQL_USER
		          valueFrom:
		            secretKeyRef:
		              name: mysql-user-pass
		              key: username
		        - name: MYSQL_PASSWORD
		          valueFrom:
		            secretKeyRef:
		              name: mysql-user-pass
		              key: password
		        ports:
		        - containerPort: 3306        
		          name: mysql
		        volumeMounts:
		        - name: mysql-persistent-storage 
		          mountPath: /var/lib/mysql
		      volumes:                       
		      - name: mysql-persistent-storage
		          persistentVolumeClaim:
		          claimName: mysql-pv-claim

thor@jump_host ~$ kubectl create -f mysql_deploy.yml
		error: error parsing mysql_deploy.yml: error converting YAML to JSON: yaml: line 29: mapping values are not allowed in this context

thor@jump_host ~$ vi mysql_deploy.yml
thor@jump_host ~$ cat mysql_deploy.yml
		---
		apiVersion: v1 
		kind: Deployment            
		metadata:
		  name: mysql-deployment       
		  labels:                       
		    app: mysql-app 
		spec:
		  selector:
		    matchLabels:
		      app: mysql-app
		      tier: mysql 
		  strategy:
		    type: Recreate
		  template:                    
		    metadata:
		      labels:                  
		        app: mysql-app
		        tier: mysql 
		    spec:                       
		      containers: 
		      - images: mysql:5.6 
		        name: mysql
		        env:                        
		        - name: MYSQL_ROOT_PASSWORD 
		          valueFrom:                
		            secretKeyRef: 
		              name: mysql-root-pass 
		              key: password 
		        - name: MYSQL_DATABASE
		          valueFrom:
		            secretKeyRef: 
		              name: mysql-db-url 
		              key: database 
		        - name: MYSQL_USER
		          valueFrom:
		            secretKeyRef:
		              name: mysql-user-pass
		              key: username
		        - name: MYSQL_PASSWORD
		          valueFrom:
		            secretKeyRef:
		              name: mysql-user-pass
		              key: password
		        ports:
		        - containerPort: 3306        
		          name: mysql
		        volumeMounts:
		        - name: mysql-persistent-storage 
		          mountPath: /var/lib/mysql
		      volumes:                       
		      - name: mysql-persistent-storage
		        persistentVolumeClaim:
		          claimName: mysql-pv-claim

thor@jump_host ~$ kubectl create -f mysql_deploy.yml
		error: unable to recognize "mysql_deploy.yml": no matches for kind "Deployment" in version "v1"

thor@jump_host ~$ vi mysql_deploy.yml

thor@jump_host ~$ kubectl create -f mysql_deploy.yml
		error: error validating "mysql_deploy.yml": error validating data: ValidationError(Deployment.spec.template.spec.containers[0]): unknown field "images" in io.k8s.api.core.v1.Container; if you choose to ignore these errors, turn validation off with --validate=false

thor@jump_host ~$ vi mysql_deploy.yml
thor@jump_host ~$ cat mysql_deploy.yml
		---
		apiVersion: apps/v1 																																	<---------------				
		kind: Deployment            
		metadata:
		  name: mysql-deployment       
		  labels:                       
		    app: mysql-app 
		spec:
		  selector:
		    matchLabels:
		      app: mysql-app
		      tier: mysql 																																		<---------------					
		  strategy:
		    type: Recreate
		  template:                    
		    metadata:
		      labels:                  
		        app: mysql-app
		        tier: mysql 
		    spec:                       
		      containers: 
		      - image: mysql:5.6 																															<---------------		
		        name: mysql
		        env:                        
		        - name: MYSQL_ROOT_PASSWORD 
		          valueFrom:                
		            secretKeyRef: 																														<---------------
		              name: mysql-root-pass 																									<---------------		
		              key: password 
		        - name: MYSQL_DATABASE
		          valueFrom:
		            secretKeyRef: 																														<---------------				
		              name: mysql-db-url 																											<---------------
		              key: database 
		        - name: MYSQL_USER
		          valueFrom:
		            secretKeyRef:
		              name: mysql-user-pass
		              key: username
		        - name: MYSQL_PASSWORD
		          valueFrom:
		            secretKeyRef:
		              name: mysql-user-pass
		              key: password
		        ports:
		        - containerPort: 3306        
		          name: mysql
		        volumeMounts:
		        - name: mysql-persistent-storage 
		          mountPath: /var/lib/mysql
		      volumes:                       
		      - name: mysql-persistent-storage
		        persistentVolumeClaim:																												<---------------			
		          claimName: mysql-pv-claim

thor@jump_host ~$ kubectl create -f mysql_deploy.yml
		deployment.apps/mysql-deployment created

 
thor@jump_host ~$ kubectl get all
		NAME                                    READY   STATUS              RESTARTS   AGE
		pod/mysql-deployment-74f5dd5cdf-gq5nd   0/1     ContainerCreating   0          13s

		NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
		service/kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP          6h23m
		service/mysql        NodePort    10.96.153.164   <none>        3306:30011/TCP   10m

		NAME                               READY   UP-TO-DATE   AVAILABLE   AGE
		deployment.apps/mysql-deployment   0/1     1            0           13s

		NAME                                          DESIRED   CURRENT   READY   AGE
		replicaset.apps/mysql-deployment-74f5dd5cdf   1         1         0       13s

thor@jump_host ~$ kubectl get pv
		NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                    STORAGECLASS   REASON   AGE
		mysql-pv   250Mi      RWO            Retain           Bound    default/mysql-pv-claim   standard                13m

thor@jump_host ~$ kubectl get pvc
		NAME             STATUS   VOLUME     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
		mysql-pv-claim   Bound    mysql-pv   250Mi      RWO            standard       12m
 
thor@jump_host ~$ kubectl get all
		NAME                                    READY   STATUS    RESTARTS   AGE
		pod/mysql-deployment-74f5dd5cdf-gq5nd   1/1     Running   0          36s

		NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
		service/kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP          6h24m
		service/mysql        NodePort    10.96.153.164   <none>        3306:30011/TCP   10m

		NAME                               READY   UP-TO-DATE   AVAILABLE   AGE
		deployment.apps/mysql-deployment   1/1     1            1           36s

		NAME                                          DESIRED   CURRENT   READY   AGE
		replicaset.apps/mysql-deployment-74f5dd5cdf   1         1         1       36s
 
thor@jump_host ~$ kubectl delete service mysql
		service "mysql" deleted

thor@jump_host ~$ kubectl get all
		NAME                                    READY   STATUS    RESTARTS   AGE
		pod/mysql-deployment-74f5dd5cdf-gq5nd   1/1     Running   0          69s

		NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
		service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   6h24m

		NAME                               READY   UP-TO-DATE   AVAILABLE   AGE
		deployment.apps/mysql-deployment   1/1     1            1           69s

		NAME                                          DESIRED   CURRENT   READY   AGE
		replicaset.apps/mysql-deployment-74f5dd5cdf   1         1         1       69s

thor@jump_host ~$ kubectl delete deployment mysql-deployment
		deployment.apps "mysql-deployment" deleted

thor@jump_host ~$ kubectl get all
		NAME                                    READY   STATUS        RESTARTS   AGE
		pod/mysql-deployment-74f5dd5cdf-gq5nd   0/1     Terminating   0          106s

		NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
		service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   6h25m

thor@jump_host ~$ kubectl delete pods mysql-deployment-74f5dd5cdf-gq5nd
		Error from server (NotFound): pods "mysql-deployment-74f5dd5cdf-gq5nd" not found
thor@jump_host ~$ kubectl get all
		NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
		service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   6h26m
 
thor@jump_host ~$ kubectl delete pvc mysql-pv-claim
		persistentvolumeclaim "mysql-pv-claim" deleted

thor@jump_host ~$ kubectl delete pv mysql-pv
		persistentvolume "mysql-pv" deleted
 
thor@jump_host ~$ kubectl get pv 
		No resources found
thor@jump_host ~$ kubectl get pvc
		No resources found in default namespace.

thor@jump_host ~$ cp mysql_deployment.yml mysql_deployment_bkp.yml
thor@jump_host ~$ vi mysql_deployment.yml 
thor@jump_host ~$ cat mysql_pv.yml mysql_pvc.yml mysql_service.yml mysql_deploy.yml >> mysql_deployment.yml 
thor@jump_host ~$ cat mysql_deployment.yml 
---
apiVersion: v1 
kind: PersistentVolume            
metadata:
  name: mysql-pv
  labels: 
    type: local 
spec:
  storageClassName: standard      
  capacity:
    storage: 250Mi
  accessModes: 
  - ReadWriteOnce
  hostPath:                       
    path: "/mnt/data" 
  persistentVolumeReclaimPolicy: Retain  
---    
apiVersion: v1 
kind: PersistentVolumeClaim 
metadata:                          
  name: mysql-pv-claim
  labels:
    app: mysql-app 
spec:                              
  storageClassName: standard       
  accessModes:
  - ReadWriteOnce                
  resources:
    requests: 
      storage: 250Mi 
---
apiVersion: v1                    
kind: Service                      
metadata:
  name: mysql         
  labels:             
    app: mysql-app
spec:
  type: NodePort
  ports:
    - targetPort: 3306
      port: 3306
      nodePort: 30011
  selector:    
    app: mysql-app
    tier: mysql
---
apiVersion: apps/v1 
kind: Deployment            
metadata:
  name: mysql-deployment       
  labels:                       
    app: mysql-app 
spec:
  selector:
    matchLabels:
      app: mysql-app
      tier: mysql 
  strategy:
    type: Recreate
  template:                    
    metadata:
      labels:                  
        app: mysql-app
        tier: mysql 
    spec:                       
      containers: 
      - image: mysql:5.6 
        name: mysql
        env:                        
        - name: MYSQL_ROOT_PASSWORD 
          valueFrom:                
            secretKeyRef: 
              name: mysql-root-pass 
              key: password 
        - name: MYSQL_DATABASE
          valueFrom:
            secretKeyRef: 
              name: mysql-db-url 
              key: database 
        - name: MYSQL_USER
          valueFrom:
            secretKeyRef:
              name: mysql-user-pass
              key: username
        - name: MYSQL_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-user-pass
              key: password
        ports:
        - containerPort: 3306        
          name: mysql
        volumeMounts:
        - name: mysql-persistent-storage 
          mountPath: /var/lib/mysql
      volumes:                       
      - name: mysql-persistent-storage
        persistentVolumeClaim:
          claimName: mysql-pv-claim

thor@jump_host ~$ ls -ahl
total 56K
		drwxr----- 1 thor thor 4.0K Nov 28 03:21 .
		drwxr-xr-x 1 root root 4.0K Oct 31  2020 ..
		-rwxrwx--- 1 thor thor   18 Oct 30  2018 .bash_logout
		-rwxrwx--- 1 thor thor  193 Oct 30  2018 .bash_profile
		-rwxrwx--- 1 thor thor  510 Nov 28 02:58 .bashrc
		drwxrwx--- 1 thor thor 4.0K Oct 31  2020 .config
		drwxrwx--- 3 thor thor 4.0K Nov 28 02:59 .kube
		-rw-r--r-- 1 thor thor 2.3K Nov 28 03:20 mysql_deployment_bkp.yml
		-rw-r--r-- 1 thor thor 2.3K Nov 28 03:21 mysql_deployment.yml
		-rw-rw-r-- 1 thor thor 1.4K Nov 28 03:16 mysql_deploy.yml
		-rw-rw-r-- 1 thor thor  313 Nov 28 03:04 mysql_pvc.yml
		-rw-rw-r-- 1 thor thor  316 Nov 28 03:02 mysql_pv.yml
		-rw-rw-r-- 1 thor thor  295 Nov 28 03:06 mysql_service.yml
		drwx------ 2 thor thor 4.0K Nov 28 02:58 .ssh

thor@jump_host ~$ kubectl create -f mysql_deployment.yml 
		persistentvolume/mysql-pv created
		persistentvolumeclaim/mysql-pv-claim created
		service/mysql created
		deployment.apps/mysql-deployment created
 
thor@jump_host ~$ kubectl get all
		NAME                                    READY   STATUS    RESTARTS   AGE
		pod/mysql-deployment-74f5dd5cdf-cqwt6   1/1     Running   0          7s

		NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
		service/kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP          6h29m
		service/mysql        NodePort    10.96.176.41   <none>        3306:30011/TCP   7s

		NAME                               READY   UP-TO-DATE   AVAILABLE   AGE
		deployment.apps/mysql-deployment   1/1     1            1           7s

		NAME                                          DESIRED   CURRENT   READY   AGE
		replicaset.apps/mysql-deployment-74f5dd5cdf   1         1         1       7s

thor@jump_host ~$ kubectl get pv
		NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                    STORAGECLASS   REASON   AGE
		mysql-pv   250Mi      RWO            Retain           Bound    default/mysql-pv-claim   standard                16s

thor@jump_host ~$ kubectl get pvc
		NAME             STATUS   VOLUME     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
		mysql-pv-claim   Bound    mysql-pv   250Mi      RWO            standard       19s
thor@jump_host ~$ 

--------------------------------------------------------------------------------------------------------------------------------
Task 105: 29/Nov/2022

Save, Load and Transfer Docker Image

One of the DevOps team members was working on to create a new custom docker image on App Server 1 in Stratos DC. He is done with his changes and image is saved on same server with name demo:nautilus. Recently a requirement has been raised by a team to use that image for testing, but the team wants to test the same on App Server 3. So we need to provide them that image on App Server 3 in Stratos DC.

a. On App Server 1 save the image demo:nautilus in an archive.

b. Transfer the image archive to App Server 3.

c. Load that image archive on App Server 3 with same name and tag which was used on App Server 1.

Note: Docker is already installed on both servers; however, if its service is down please make sure to start it.

1. Login to App Server and switch to root

thor@jump_host ~$ ssh tony@stapp01
		The authenticity of host 'stapp01 (172.16.238.10)' can't be established.
		ECDSA key fingerprint is SHA256:7/VTPQ2CABk+3zUB7pJzVmT/d4XiNBJuNZ0bl5GCobQ.
		ECDSA key fingerprint is MD5:df:68:3b:89:1a:b0:fd:1f:79:ec:d4:b7:89:e3:e9:13.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'stapp01,172.16.238.10' (ECDSA) to the list of known hosts.
		tony@stapp01's password: 

[tony@stapp01 ~]$ sudo su -

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for tony: 

2. List the existing images in docker which need to archive

[root@stapp01 ~]# docker images
		REPOSITORY   TAG        IMAGE ID       CREATED              SIZE
		demo         nautilus   a0358bd5cf49   About a minute ago   118MB
		ubuntu       latest     a8780b506fa4   3 weeks ago          77.8MB

3. Save the image in a TAR archive

[root@stapp01 ~]# docker save -o /tmp/demo.tar demo:nautilus

[root@stapp01 ~]# ls -ahl /tmp
		total 115M
		drwxrwxrwt 1 root root 4.0K Nov 29 04:40 .
		drwxr-xr-x 1 root root 4.0K Nov 29 04:36 ..
		-rw------- 1 root root 115M Nov 29 04:40 demo.tar
		-rwxr-xr-x 1 root root  239 Nov 29 04:37 docker_container.sh
		drwxrwxrwt 1 root root 4.0K Aug  1  2019 .font-unix
		drwxrwxrwt 1 root root 4.0K Aug  1  2019 .ICE-unix
		-rwx------ 1 root root  836 Aug  1  2019 ks-script-rnBCJB
		drwxrwxrwt 1 root root 4.0K Aug  1  2019 .Test-unix
		drwxrwxrwt 1 root root 4.0K Aug  1  2019 .X11-unix
		drwxrwxrwt 1 root root 4.0K Aug  1  2019 .XIM-unix
		-rw------- 1 root root    0 Aug  1  2019 yum.log

4. Copy(SCP) the archive file to mentioned server

[root@stapp01 ~]# scp /tmp/demo.tar banner@stapp03:/tmp
		The authenticity of host 'stapp03 (172.16.238.12)' can't be established.
		ECDSA key fingerprint is SHA256:RveOEHTizfGCH1Dwm9qbs1/OPlyAGQuSYS3JGfpEwL0.
		ECDSA key fingerprint is MD5:13:5d:0c:a2:b4:72:84:a9:15:41:8e:35:a6:f3:7f:22.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'stapp03,172.16.238.12' (ECDSA) to the list of known hosts.
		banner@stapp03's password: 
		demo.tar                                                                                                   100%  115MB  45.8MB/s   00:02    

5. Login to mentioned App server and switch to root user

[root@stapp01 ~]# ssh banner@stapp03
		banner@stapp03's password: 
 
[banner@stapp03 ~]$ sudo su  -

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for banner: 

6. Verify the copied archive file

[root@stapp03 ~]# ls -ahl /tmp/
		total 115M
		drwxrwxrwt 1 root   root   4.0K Nov 29 04:41 .
		drwxr-xr-x 1 root   root   4.0K Nov 29 04:36 ..
		-rw------- 1 banner banner 115M Nov 29 04:41 demo.tar
		drwxrwxrwt 1 root   root   4.0K Aug  1  2019 .font-unix
		drwxrwxrwt 1 root   root   4.0K Aug  1  2019 .ICE-unix
		-rwx------ 1 root   root    836 Aug  1  2019 ks-script-rnBCJB
		drwxrwxrwt 1 root   root   4.0K Aug  1  2019 .Test-unix
		drwxrwxrwt 1 root   root   4.0K Aug  1  2019 .X11-unix
		drwxrwxrwt 1 root   root   4.0K Aug  1  2019 .XIM-unix
		-rw------- 1 root   root      0 Aug  1  2019 yum.log

7. Check current docker images
[root@stapp03 ~]# docker images
		Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?

8. Start docker daemon if required 

[root@stapp03 ~]# systemctl start docker

[root@stapp03 ~]# systemctl status docker
		● docker.service - Docker Application Container Engine
		   Loaded: loaded (/usr/lib/systemd/system/docker.service; disabled; vendor preset: disabled)
		   Active: active (running) since Tue 2022-11-29 04:42:59 UTC; 7s ago
		     Docs: https://docs.docker.com
		 Main PID: 583 (dockerd)
		    Tasks: 23
		   Memory: 49.2M
		   CGroup: /docker/dcc4ae04d8c1008c63da5958d22da0d318642b6438e34301f6634af45e159a3c/system.slice/docker.service
		           └─583 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock

		Nov 29 04:42:57 stapp03.stratos.xfusioncorp.com dockerd[583]: time="2022-11-29T04:42:57.326955764Z" level=warning msg="Your kernel do...imit"
		Nov 29 04:42:57 stapp03.stratos.xfusioncorp.com dockerd[583]: time="2022-11-29T04:42:57.327031003Z" level=warning msg="Your kernel do...uler"
		Nov 29 04:42:57 stapp03.stratos.xfusioncorp.com dockerd[583]: time="2022-11-29T04:42:57.327039173Z" level=warning msg="Your kernel do...ight"
		Nov 29 04:42:57 stapp03.stratos.xfusioncorp.com dockerd[583]: time="2022-11-29T04:42:57.327046428Z" level=warning msg="Your kernel do...vice"
		Nov 29 04:42:57 stapp03.stratos.xfusioncorp.com dockerd[583]: time="2022-11-29T04:42:57.330263079Z" level=info msg="Loading container...art."
		Nov 29 04:42:58 stapp03.stratos.xfusioncorp.com dockerd[583]: time="2022-11-29T04:42:58.293270350Z" level=info msg="Loading container...one."
		Nov 29 04:42:58 stapp03.stratos.xfusioncorp.com dockerd[583]: time="2022-11-29T04:42:58.986622869Z" level=info msg="Docker daemon" co....10.7
		Nov 29 04:42:58 stapp03.stratos.xfusioncorp.com dockerd[583]: time="2022-11-29T04:42:58.986895666Z" level=info msg="Daemon has comple...tion"
		Nov 29 04:42:59 stapp03.stratos.xfusioncorp.com systemd[1]: Started Docker Application Container Engine.
		Nov 29 04:42:59 stapp03.stratos.xfusioncorp.com dockerd[583]: time="2022-11-29T04:42:59.681252769Z" level=info msg="API listen on /va...sock"
		Hint: Some lines were ellipsized, use -l to show in full.
 
[root@stapp03 ~]# docker images
		REPOSITORY   TAG       IMAGE ID   CREATED   SIZE

9. Load the image archive 

[root@stapp03 ~]# docker load -i /tmp/demo.tar 
		f4a670ac65b6: Loading layer [==================================================>]  80.31MB/80.31MB
		f1e0d34b5503: Loading layer [==================================================>]   39.8MB/39.8MB
		Loaded image: demo:nautilus

10. Validate the task by checking docker images

[root@stapp03 ~]# docker images
		REPOSITORY   TAG        IMAGE ID       CREATED         SIZE
		demo         nautilus   a0358bd5cf49   4 minutes ago   118MB

--------------------------------------------------------------------------------------------------------------------------------
Task 106: 30/Nov/2022

Create Deployments in Kubernetes Cluster

The Nautilus DevOps team has started practicing some pods, and services deployment on Kubernetes platform as they are planning to migrate most of their applications on Kubernetes platform. Recently one of the team members has been assigned a task to create a deploymnt as per details mentioned below:

Create a deployment named httpd to deploy the application httpd using the image httpd: latest (remember to mention the tag as well)

1. Check current kubectl utility and configuration

thor@jump_host ~$ kubectl get all
		NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
		service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   71m

thor@jump_host ~$ kubectl get namespace
		NAME                 STATUS   AGE
		default              Active   71m
		kube-node-lease      Active   71m
		kube-public          Active   71m
		kube-system          Active   71m
		local-path-storage   Active   71m

thor@jump_host ~$ kubectl get deploy
		No resources found in default namespace.

2. Create the deploy as per given rrequirements

thor@jump_host ~$ kubectl create deploy httpd --image httpd:latest
		deployment.apps/httpd created

3. Validate the task by checking running pod and its description 

thor@jump_host ~$ kubectl get deploy
		NAME    READY   UP-TO-DATE   AVAILABLE   AGE
		httpd   0/1     1            0           4s

thor@jump_host ~$ kubectl get pods
		NAME                    READY   STATUS              RESTARTS   AGE
		httpd-84898796c-cxk8c   0/1     ContainerCreating   0          11s
 
thor@jump_host ~$ kubectl get pods
		NAME                    READY   STATUS    RESTARTS   AGE
		httpd-84898796c-cxk8c   1/1     Running   0          15s

thor@jump_host ~$ kubectl describe pod httpd-84898796c-cxk8c
		Name:         httpd-84898796c-cxk8c
		Namespace:    default
		Priority:     0
		Node:         kodekloud-control-plane/172.17.0.2
		Start Time:   Wed, 30 Nov 2022 14:49:07 +0000
		Labels:       app=httpd
		              pod-template-hash=84898796c
		Annotations:  <none>
		Status:       Running
		IP:           10.244.0.5
		IPs:
		  IP:           10.244.0.5
		Controlled By:  ReplicaSet/httpd-84898796c
		Containers:
		  httpd:
		    Container ID:   containerd://bd2e7069563795ec06f923fb9ade47569a6b88fc12e65a671c6df382c1c4770f
		    Image:          httpd:latest 																														<-------------------
		    Image ID:       docker.io/library/httpd@sha256:f2e89def4c032b02c83e162c1819ccfcbd4ea6bdbc5ff784bbc68cba940a9046
		    Port:           <none>
		    Host Port:      <none>
		    State:          Running
		      Started:      Wed, 30 Nov 2022 14:49:21 +0000
		    Ready:          True
		    Restart Count:  0
		    Environment:    <none>
		    Mounts:
		      /var/run/secrets/kubernetes.io/serviceaccount from default-token-btvbx (ro)
		Conditions:
		  Type              Status
		  Initialized       True 
		  Ready             True 
		  ContainersReady   True 
		  PodScheduled      True 
		Volumes:
		  default-token-btvbx:
		    Type:        Secret (a volume populated by a Secret)
		    SecretName:  default-token-btvbx
		    Optional:    false
		QoS Class:       BestEffort
		Node-Selectors:  <none>
		Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
		                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
		Events:
		  Type    Reason     Age   From               Message
		  ----    ------     ----  ----               -------
		  Normal  Scheduled  42s   default-scheduler  Successfully assigned default/httpd-84898796c-cxk8c to kodekloud-control-plane
		  Normal  Pulling    41s   kubelet            Pulling image "httpd:latest"
		  Normal  Pulled     28s   kubelet            Successfully pulled image "httpd:latest" in 12.615345253s
		  Normal  Created    28s   kubelet            Created container httpd
		  Normal  Started    28s   kubelet            Started container httpd
thor@jump_host ~$

--------------------------------------------------------------------------------------------------------------------------------
Task 107: 02/Dec/2022

Manage Git Repositories

A new developer just joined the Nautilus development team and has been assigned a new project for which he needs to create a new repository under his account on Gitea server. Additionally, there is some existing data that need to be added to the repo. Below you can find more details about the task:

Click on the Gitea UI button on the top bar. You should be able to access the Gitea UI. Login to Gitea server using username max and password Max_pass123.

a. Create a new git repository story_ecommerce under max user.

b. SSH into storage server using user max and password Max_pass123 and clone this newly created repository under user max home directory i.e /home/max.

c. Copy all files from location /usr/sysops to the repository and commit/push your changes to the master branch. The commit message must be "add stories" (must be done in single commit).

d. Create a new branch max_apps from master.

e. Copy a file story-index-max.txt from location /tmp/stories/ to the repository. This file has a typo, which you can fix by changing the word Mooose to Mouse. Commit and push the changes to the newly created branch. Commit message must be "typo fixed for Mooose" (must be done in single commit).

1. Login to Gitea UI using given credentials .

2. Create a new repository 

3. SSH to storage server 

thor@jump_host ~$ ssh max@ststor01
		The authenticity of host 'ststor01 (172.16.238.15)' can't be established.
		ECDSA key fingerprint is SHA256:0z85j/k+4Nf8WKbHJzxo1AOv4FeRA8LPET2N3BEkYyo.
		ECDSA key fingerprint is MD5:74:e6:4d:c4:b3:80:07:be:03:30:0a:bf:1e:eb:e6:82.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'ststor01,172.16.238.15' (ECDSA) to the list of known hosts.
		max@ststor01's password: 
		Welcome to xFusionCorp Storage server.

4. clone the newly created repository

max $ git clone http://git.stratos.xfusioncorp.com/max/story_ecommerce.git
		Cloning into 'story_ecommerce'...
		warning: You appear to have cloned an empty repository.
		Checking connectivity... done.

max $ ls -ahl
		total 32
		drwxr-sr-x    1 max      max         4.0K Dec  2 12:07 .
		drwxr-xr-x    1 root     root        4.0K Oct 26  2020 ..
		-rw-r--r--    1 max      max          202 Oct 26  2020 .bash_profile
		-rw-r--r--    1 max      max          202 Oct 26  2020 .bashrc
		-rw-r--r--    1 max      max           50 Oct 26  2020 .vimrc
		drwxr-sr-x    3 max      max         4.0K Dec  2 12:07 story_ecommerce

 
max $ cd story_ecommerce/

max $ ls -ahl
		total 16
		drwxr-sr-x    3 max      max         4.0K Dec  2 12:07 .
		drwxr-sr-x    1 max      max         4.0K Dec  2 12:07 ..
		drwxr-sr-x    7 max      max         4.0K Dec  2 12:07 .git
5. Copy the file 

max $ cp /usr/sysops/*.* ./

max $ ls -ahl
		total 24
		drwxr-sr-x    3 max      max         4.0K Dec  2 12:08 .
		drwxr-sr-x    1 max      max         4.0K Dec  2 12:07 ..
		drwxr-sr-x    7 max      max         4.0K Dec  2 12:07 .git
		-rw-r--r--    1 max      max          792 Dec  2 12:08 frogs-and-ox.txt
		-rw-r--r--    1 max      max         1.1K Dec  2 12:08 lion-and-mouse.txt

6. Add the files to git , commit the changes and then push the master branch

max $ git add .

max $ git commit "add stories"^C

max $ git status
		On branch master

		Initial commit

		Changes to be committed:
		  (use "git rm --cached <file>..." to unstage)

		        new file:   frogs-and-ox.txt
		        new file:   lion-and-mouse.txt

max $ git commit -m "add stories"
		[master (root-commit) 66c4fcd] add stories
		 Committer: Linux User <max@ststor01.stratos.xfusioncorp.com>
		Your name and email address were configured automatically based
		on your username and hostname. Please check that they are accurate.
		You can suppress this message by setting them explicitly. Run the
		following command and follow the instructions in your editor to edit
		your configuration file:

		    git config --global --edit

		After doing this, you may fix the identity used for this commit with:

		    git commit --amend --reset-author

		 2 files changed, 42 insertions(+)
		 create mode 100644 frogs-and-ox.txt
		 create mode 100644 lion-and-mouse.txt

max (master)$ git status
		On branch master
		Your branch is based on 'origin/master', but the upstream is gone.
		  (use "git branch --unset-upstream" to fixup)
		nothing to commit, working directory clean

max (master)$ git push origin master
		Username for 'http://git.stratos.xfusioncorp.com': max
		Password for 'http://max@git.stratos.xfusioncorp.com': 
		Counting objects: 4, done.
		Delta compression using up to 36 threads.
		Compressing objects: 100% (4/4), done.
		Writing objects: 100% (4/4), 1.19 KiB | 0 bytes/s, done.
		Total 4 (delta 0), reused 0 (delta 0)
		remote: . Processing 1 references
		remote: Processed 1 references in total
		To http://git.stratos.xfusioncorp.com/max/story_ecommerce.git
		 * [new branch]      master -> master

7. Create the new branch and checkout to newly created branch 

max (master)$ git branch max_apps
 
max (master)$ git branch -a
		* master
		  max_apps
		  remotes/origin/master

max (master)$ git checkout max_apps
		Switched to branch 'max_apps'
		max (max_apps)$ git branch -a
		  master
		* max_apps
		  remotes/origin/master

max (max_apps)$ ls -ahl
		total 24
		drwxr-sr-x    3 max      max         4.0K Dec  2 12:08 .
		drwxr-sr-x    1 max      max         4.0K Dec  2 12:07 ..
		drwxr-sr-x    8 max      max         4.0K Dec  2 12:10 .git
		-rw-r--r--    1 max      max          792 Dec  2 12:08 frogs-and-ox.txt
		-rw-r--r--    1 max      max         1.1K Dec  2 12:08 lion-and-mouse.txt

8. copy the new file make the changes 

max (max_apps)$ cp /tmp/stories/story-index-max.txt ./

max (max_apps)$ ls -ahl
		total 28
		drwxr-sr-x    3 max      max         4.0K Dec  2 12:11 .
		drwxr-sr-x    1 max      max         4.0K Dec  2 12:07 ..
		drwxr-sr-x    8 max      max         4.0K Dec  2 12:10 .git
		-rw-r--r--    1 max      max          792 Dec  2 12:08 frogs-and-ox.txt
		-rw-r--r--    1 max      max         1.1K Dec  2 12:08 lion-and-mouse.txt
		-rw-r--r--    1 max      max          102 Dec  2 12:11 story-index-max.txt

max (max_apps)$ vi story-index-max.txt 

max (max_apps)$ cat story-index-max.txt 
		1. The Lion and the Mouse
		2. The Frogs and the Ox
		3. The Fox and the Grapes
		4. The Donkey and the Dog


max (max_apps)$ git status
		On branch max_apps
		Untracked files:
		  (use "git add <file>..." to include in what will be committed)

		        story-index-max.txt

		nothing added to commit but untracked files present (use "git add" to track)

9. Add the files to git , commit the changes and then push the max_apps branch

max (max_apps)$ git add .

max (max_apps)$ git  commit -m "typo fixed for Mooose"
		[max_apps 54d2209] typo fixed for Mooose
		 Committer: Linux User <max@ststor01.stratos.xfusioncorp.com>
		Your name and email address were configured automatically based
		on your username and hostname. Please check that they are accurate.
		You can suppress this message by setting them explicitly. Run the
		following command and follow the instructions in your editor to edit
		your configuration file:

		    git config --global --edit

		After doing this, you may fix the identity used for this commit with:

		    git commit --amend --reset-author

		 1 file changed, 4 insertions(+)
		 create mode 100644 story-index-max.txt

max (max_apps)$ git status
		On branch max_apps
		nothing to commit, working directory clean

max (max_apps)$ git push origin max_apps
		Username for 'http://git.stratos.xfusioncorp.com': max
		Password for 'http://max@git.stratos.xfusioncorp.com': 
		Counting objects: 3, done.
		Delta compression using up to 36 threads.
		Compressing objects: 100% (3/3), done.
		Writing objects: 100% (3/3), 412 bytes | 0 bytes/s, done.
		Total 3 (delta 0), reused 0 (delta 0)
		remote: 
		remote: Create a new pull request for 'max_apps':
		remote:   http://git.stratos.xfusioncorp.com/max/story_ecommerce/compare/master...max_apps
		remote: 
		remote: . Processing 1 references
		remote: Processed 1 references in total
		To http://git.stratos.xfusioncorp.com/max/story_ecommerce.git
		 * [new branch]      max_apps -> max_apps

max (max_apps)$ git status
		On branch max_apps
		nothing to commit, working directory clean

--------------------------------------------------------------------------------------------------------------------------------
Task 108: 03/Dec/2022

Puppet Create a File

The Puppet master and Puppet agent nodes have been set up by the Nautilus DevOps team so they can perform testing. In Stratos DC all app servers have been configured as Puppet agent nodes. Below are details about the testing scenario they want to proceed with.

Use Puppet file resource and perform the below given task:

    Create a Puppet programming file apps.pp under /etc/puppetlabs/code/environments/production/manifests directory on master node i.e Jump Server.

    Using /etc/puppetlabs/code/environments/production/manifests/apps.pp create a file news.txt under /opt/sysops directory on App Server 1.



1. Switch to root user on jump server / puppet master and got to mentione folder location

thor@jump_host ~$ sudo su -

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for thor: 
 
root@jump_host ~# cd /etc/puppetlabs/code/environments/production/manifests/

root@jump_host /etc/puppetlabs/code/environments/production/manifests# ls -ahl
		total 8.0K
		drwxr-xr-x 1 puppet puppet 4.0K Jul 13  2021 .
		drwxr-xr-x 1 puppet puppet 4.0K Aug  9  2021 ..

2. Create puppet programming file as per given requirement

root@jump_host /etc/puppetlabs/code/environments/production/manifests# vi apps.pp

root@jump_host /etc/puppetlabs/code/environments/production/manifests# cat apps.pp
		class file_creator {

		  # Now create news.txt under /opt/sysops

		  file { '/opt/sysops/news.txt':

		    ensure => 'present',

		  }

		}

		node 'stapp01.stratos.xfusioncorp.com' {

		  include file_creator

		}

root@jump_host /etc/puppetlabs/code/environments/production/manifests# ls -ahl
		total 12K
		drwxr-xr-x 1 puppet puppet 4.0K Dec  3 16:13 .
		drwxr-xr-x 1 puppet puppet 4.0K Aug  9  2021 ..
		-rw-r--r-- 1 root   root    201 Dec  3 16:13 apps.pp

3. Validate the puppet file

root@jump_host /etc/puppetlabs/code/environments/production/manifests# puppet parser validate apps.pp 

4. Login to App server 1 and swithc to root user 

root@jump_host /etc/puppetlabs/code/environments/production/manifests# ssh tony@stapp01
		The authenticity of host 'stapp01 (172.16.238.10)' can't be established.
		ECDSA key fingerprint is SHA256:NLXvXSL0mV+6PF2+l8fyhaVp40+uZEWYpT6hSzNh8fw.
		ECDSA key fingerprint is MD5:04:0d:f9:32:97:24:7d:cf:63:ce:a3:c4:0b:a9:0c:40.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'stapp01,172.16.238.10' (ECDSA) to the list of known hosts.
		tony@stapp01's password: 
 
[tony@stapp01 ~]$ sudo su -

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for tony:

5. Run puppet agent to pull the configuration from puppet server

[root@stapp01 ~]# puppet agent -tv
		Info: Using configured environment 'production'
		Info: Retrieving pluginfacts
		Info: Retrieving plugin
		Info: Retrieving locales
		Info: Caching catalog for stapp01.stratos.xfusioncorp.com
		Info: Applying configuration version '1670084110'
		Notice: /Stage[main]/File_creator/File[/opt/sysops/news.txt]/ensure: created
		Notice: Applied catalog in 0.01 seconds

6. Validate the task by checking file created 

[root@stapp01 ~]# cd /opt/sysops/

[root@stapp01 sysops]# ls -ahl
		total 8.0K
		drwxr-xr-x 2 root root 4.0K Dec  3 16:15 .
		drwxr-xr-x 1 root root 4.0K Dec  3 16:10 ..
		-rw-r--r-- 1 root root    0 Dec  3 16:15 news.txt

--------------------------------------------------------------------------------------------------------------------------------
Task 109: 05/Dec/2022

Ansible Blockinfile Module

The Nautilus DevOps team wants to install and set up a simple httpd web server on all app servers in Stratos DC. Additionally, they want to deploy a sample web page for now using Ansible only. Therefore, write the required playbook to complete this task. Find more details about the task below.

We already have an inventory file under /home/thor/ansible directory on jump host. Create a playbook.yml under /home/thor/ansible directory on jump host itself.

    Using the playbook, install httpd web server on all app servers. Additionally, make sure its service should up and running.

    Using blockinfile Ansible module add some content in /var/www/html/index.html file. Below is the content:

    Welcome to XfusionCorp!

    This is Nautilus sample file, created using Ansible!

    Please do not modify this file manually!

    The /var/www/html/index.html file's user and group owner should be apache on all app servers.

    The /var/www/html/index.html file's permissions should be 0644 on all app servers.

Note:

i. Validation will try to run the playbook using command ansible-playbook -i inventory playbook.yml so please make sure the playbook works this way without passing any extra arguments.

ii. Do not use any custom or empty marker for blockinfile module.


1. Go to mentioned folder and check inventory file.

thor@jump_host ~$ cd /home/thor/ansible/

thor@jump_host ~/ansible$ ls -ahl
		total 16K
		drwxr-xr-x 2 thor thor 4.0K Dec  5 16:26 .
		drwxr----- 1 thor thor 4.0K Dec  5 16:26 ..
		-rw-r--r-- 1 thor thor   36 Dec  5 16:26 ansible.cfg
		-rw-r--r-- 1 thor thor  237 Dec  5 16:26 inventory

thor@jump_host ~/ansible$ cat inventory 
		stapp01 ansible_host=172.16.238.10 ansible_ssh_pass=Ir0nM@n ansible_user=tony
		stapp02 ansible_host=172.16.238.11 ansible_ssh_pass=Am3ric@ ansible_user=steve
		stapp03 ansible_host=172.16.238.12 ansible_ssh_pass=BigGr33n ansible_user=banner

thor@jump_host ~/ansible$ cat  ansible.cfg 
		[defaults]
		host_key_checking = False

2. Create playbook as per given requirements		

thor@jump_host ~/ansible$ vi playbook.yml
thor@jump_host ~/ansible$ cat playbook.yml 
		---
		- name: Install httpd and setup index.html
		  hosts: stapp01, stapp02, stapp03
		  become: yes

		  tasks:
		    - name: Install httpd
		      package:
		        name: httpd
		        state: present
		    - name: Start service httpd, if not started
		      service:
		        name: httpd
		        state: started
		    - name: Add content block in index.html and set permissions
		      blockinfile:
		        path: /var/www/html/index.html
		        create: yes
		        block: 
		          Welcome to XfusionCorp!
		          This is Nautilus sample file, created using Ansible!
		          Please do not modify this file manually!
		        owner: apache
		        group: apache
		        mode: "0644"

3. Execute the playbook

thor@jump_host ~/ansible$ ansible-playbook -i inventory playbook.yml 

		PLAY [Install httpd and setup index.html] ***************************************************************************************************

		TASK [Gathering Facts] **********************************************************************************************************************
		ok: [stapp02]
		ok: [stapp03]
		ok: [stapp01]

		TASK [Install httpd] ************************************************************************************************************************
		changed: [stapp02]
		changed: [stapp01]
		changed: [stapp03]

		TASK [Start service httpd, if not started] **************************************************************************************************
		changed: [stapp02]
		changed: [stapp03]
		changed: [stapp01]

		TASK [Add content block in index.html and set permissions] **********************************************************************************
		changed: [stapp03]
		changed: [stapp01]
		changed: [stapp02]

		PLAY RECAP **********************************************************************************************************************************
		stapp01                    : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
		stapp02                    : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
		stapp03                    : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

4. Validate the task using inventory file and url

thor@jump_host ~/ansible$ ansible -i inventory all -a 'ls -ahl /var/www/html/'
		stapp02 | CHANGED | rc=0 >>
		total 12K
		drwxr-xr-x 2 root   root   4.0K Dec  5 16:31 .
		drwxr-xr-x 4 root   root   4.0K Dec  5 16:31 ..
		-rw-r--r-- 1 apache apache  176 Dec  5 16:31 index.html
		stapp03 | CHANGED | rc=0 >>
		total 12K
		drwxr-xr-x 2 root   root   4.0K Dec  5 16:31 .
		drwxr-xr-x 4 root   root   4.0K Dec  5 16:31 ..
		-rw-r--r-- 1 apache apache  176 Dec  5 16:31 index.html
		stapp01 | CHANGED | rc=0 >>
		total 12K
		drwxr-xr-x 2 root   root   4.0K Dec  5 16:31 .
		drwxr-xr-x 4 root   root   4.0K Dec  5 16:31 ..
		-rw-r--r-- 1 apache apache  176 Dec  5 16:31 index.html

thor@jump_host ~/ansible$ curl http://stapp01
		# BEGIN ANSIBLE MANAGED BLOCK
		Welcome to XfusionCorp! This is Nautilus sample file, created using Ansible! Please do not modify this file manually!
		# END ANSIBLE MANAGED BLOCK

thor@jump_host ~/ansible$ curl http://stapp02
		# BEGIN ANSIBLE MANAGED BLOCK
		Welcome to XfusionCorp! This is Nautilus sample file, created using Ansible! Please do not modify this file manually!
		# END ANSIBLE MANAGED BLOCK

thor@jump_host ~/ansible$ curl http://stapp03
		# BEGIN ANSIBLE MANAGED BLOCK
		Welcome to XfusionCorp! This is Nautilus sample file, created using Ansible! Please do not modify this file manually!
		# END ANSIBLE MANAGED BLOCK
thor@jump_host ~/ansible$

--------------------------------------------------------------------------------------------------------------------------------
Task 110: 07/Dec/2022

Docker Volumes Mapping

The Nautilus DevOps team is testing applications containerization, which issupposed to be migrated on docker container-based environments soon. In today's stand-up meeting one of the team members has been assigned a task to create and test a docker container with certain requirements. Below are more details:

a. On App Server 2 in Stratos DC pull nginx image (preferably latest tag but others should work too).

b. Create a new container with name demo from the image you just pulled.

c. Map the host volume /opt/dba with container volume /tmp. There is an sample.txt file present on same server under /tmp; copy that file to /opt/dba. Also please keep the container in running state.

1. Login to App Server and switch to root user

thor@jump_host ~$ ssh steve@stapp02
		The authenticity of host 'stapp02 (172.16.238.11)' can't be established.
		ECDSA key fingerprint is SHA256:7rKUo4xCK3UkRzI5NbGbEjCT2Czto4RqXcEi60iyGBs.
		ECDSA key fingerprint is MD5:72:4b:e2:d6:29:d3:c6:e2:4b:23:04:ac:4f:5d:84:90.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'stapp02,172.16.238.11' (ECDSA) to the list of known hosts.
		steve@stapp02's password: 

[steve@stapp02 ~]$ sudo su -

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for steve: 

2. check existing docker images

[root@stapp02 ~]# docker images 
		REPOSITORY   TAG       IMAGE ID   CREATED   SIZE

3. Pull the mentioned image from server 

[root@stapp02 ~]# docker pull nginx:latest
		latest: Pulling from library/nginx
		025c56f98b67: Pull complete 
		ca9c7f45d396: Pull complete 
		ed6bd111fc08: Pull complete 
		e25b13a5f70d: Pull complete 
		9bbabac55ab6: Pull complete 
		e5c9ba265ded: Pull complete 
		Digest: sha256:ab589a3c466e347b1c0573be23356676df90cd7ce2dbf6ec332a5f0a8b5e59db
		Status: Downloaded newer image for nginx:latest
		docker.io/library/nginx:latest
 
[root@stapp02 ~]# docker images 
		REPOSITORY   TAG       IMAGE ID       CREATED        SIZE
		nginx        latest    ac8efec875ce   30 hours ago   142MB

4. Copy mentioned file to given folder  

[root@stapp02 ~]# cp /tmp/sample.txt /opt/dba

[root@stapp02 ~]# ls -ahl /opt/dba/
		total 12K
		drwxr-xr-x 2 root root 4.0K Dec  7 11:05 .
		drwxr-xr-x 1 root root 4.0K Dec  7 11:02 ..
		-rw-r--r-- 1 root root   23 Dec  7 11:05 sample.txt

5. Run the docker image with mentioned volume mount

[root@stapp02 ~]# docker run --name demo  -v /opt/dba:/tmp -d -it  nginx:latest
		695a584a84cecf83f4b50ad70d803ef41b8ef91a6f35dffa79e85d04cffc5c0e

[root@stapp02 ~]# docker ps
		CONTAINER ID   IMAGE          COMMAND                  CREATED          STATUS          PORTS     NAMES
		695a584a84ce   nginx:latest   "/docker-entrypoint.…"   16 seconds ago   Up 13 seconds   80/tcp    demo

6. Verify the mount by login to container and ls on /tmp

[root@stapp02 ~]# docker exec -it 695a584a84ce /bin/bash

root@695a584a84ce:/# ls -ahl /tmp/
		total 12K
		drwxr-xr-x  2 root root 4.0K Dec  7 11:05 .
		drwxr-xr-x 22 root root 4.0K Dec  7 11:06 ..
		-rw-r--r--  1 root root   23 Dec  7 11:05 sample.txt

root@695a584a84ce:/# 

--------------------------------------------------------------------------------------------------------------------------------
Task 111: 08/Dec/2022

Ansible Replace Module

There is some data on all app servers in Stratos DC. The Nautilus development team shared some requirement with the DevOps team to alter some of the data as per recent changes they made. The DevOps team is working to prepare an Ansible playbook to accomplish the same. Below you can find more details about the task.

Write a playbook.yml under /home/thor/ansible on jump host, an inventory is already present under /home/thor/ansible directory on Jump host itself. Perform below given tasks using this playbook:

    We have a file /opt/devops/blog.txt on app server 1. Using Ansible replace module replace string xFusionCorp to Nautilus in that file.

    We have a file /opt/devops/story.txt on app server 2. Using Ansiblereplace module replace the string Nautilus to KodeKloud in that file.

    We have a file /opt/devops/media.txt on app server 3. Using Ansible replace module replace string KodeKloud to xFusionCorp Industries in that file.

Note: Validation will try to run the playbook using command ansible-playbook -i inventory playbook.yml so please make sure the playbook works this way without passing any extra arguments.

1. Go to mentioned folder and check ansible inventory file 

thor@jump_host ~$ cd /home/thor/ansible/

thor@jump_host ~/ansible$ ls -ahl
		total 16K
		drwxr-xr-x 2 thor thor 4.0K Dec  8 14:18 .
		drwxr----- 1 thor thor 4.0K Dec  8 14:18 ..
		-rw-r--r-- 1 thor thor   36 Dec  8 14:18 ansible.cfg
		-rw-r--r-- 1 thor thor  237 Dec  8 14:18 inventory

thor@jump_host ~/ansible$ cat inventory 
		stapp01 ansible_host=172.16.238.10 ansible_ssh_pass=Ir0nM@n ansible_user=tony
		stapp02 ansible_host=172.16.238.11 ansible_ssh_pass=Am3ric@ ansible_user=steve
		stapp03 ansible_host=172.16.238.12 ansible_ssh_pass=BigGr33n ansible_user=banner

thor@jump_host ~/ansible$ cat ansible.cfg 
		[defaults]
		host_key_checking = False

2.Create playbook.yml as per given requirements 

thor@jump_host ~/ansible$ vi playbook.yml
 
thor@jump_host ~/ansible$ cat playbook.yml 
		- name: Ansible replace
		  hosts: stapp01,stapp02,stapp03
		  become: yes
		  tasks:
		    - name: blog.txt replacement
		      replace:
		        path: /opt/devops/blog.txt
		        regexp: "xFusionCorp"
		        replace: "Nautilus"
		      when: inventory_hostname == "stapp01"
		    - name: story.txt replacement
		      replace:
		        path: /opt/devops/story.txt
		        regexp: "Nautilus"
		        replace: "KodeKloud"
		      when: inventory_hostname == "stapp02"
		    - name: media.txt replacement
		      replace:
		        path: /opt/devops/media.txt
		        regexp: "KodeKloud"
		        replace: "xFusionCorp Industries"
		      when: inventory_hostname == "stapp03"

3. Run playbook to make the changes  

thor@jump_host ~/ansible$ ansible-playbook -i inventory playbook.yml 

		PLAY [Ansible replace] **********************************************************************************************************************

		TASK [Gathering Facts] **********************************************************************************************************************
		ok: [stapp01]
		ok: [stapp02]
		ok: [stapp03]

		TASK [blog.txt replacement] *****************************************************************************************************************
		skipping: [stapp03]
		skipping: [stapp02]
		changed: [stapp01]

		TASK [story.txt replacement] ****************************************************************************************************************
		skipping: [stapp01]
		skipping: [stapp03]
		changed: [stapp02]

		TASK [media.txt replacement] ****************************************************************************************************************
		skipping: [stapp01]
		skipping: [stapp02]
		changed: [stapp03]

		PLAY RECAP **********************************************************************************************************************************
		stapp01                    : ok=2    changed=1    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0   
		stapp02                    : ok=2    changed=1    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0   
		stapp03                    : ok=2    changed=1    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0   

4. Validate the task by checking the files on each server 

thor@jump_host ~/ansible$ ansible -i inventory stapp01 -a 'cat /opt/devops/blog.txt'
		stapp01 | CHANGED | rc=0 >>
		Welcome to Nautilus Industries !

thor@jump_host ~/ansible$ ansible -i inventory stapp02 -a 'cat /opt/devops/story.txt'
		stapp02 | CHANGED | rc=0 >>
		Welcome to KodeKloud Group !

thor@jump_host ~/ansible$ ansible -i inventory stapp03 -a 'cat /opt/devops/media.txt'
		stapp03 | CHANGED | rc=0 >>
		Welcome to xFusionCorp Industries !

--------------------------------------------------------------------------------------------------------------------------------
Task 112: 10/Dec/2022

Pull Docker Image

Nautilus project developers are planning to start testing on a new project. As per their meeting with the DevOps team, they want to test containerized environment application features. As per details shared with DevOps team, we need to accomplish the following task:

a. Pull busybox:musl image on App Server 1 in Stratos DC and re-tag (create new tag) this image as busybox:news


1. Login to App Server and switch to root user 

thor@jump_host ~$ ssh tony@stapp01
		The authenticity of host 'stapp01 (172.16.238.10)' can't be established.
		ECDSA key fingerprint is SHA256:JJuCHauappFu+vIIL627WKXRst9vHaF4I+p/4JibJ8E.
		ECDSA key fingerprint is MD5:ca:3c:ea:4e:7c:90:2f:27:c0:40:66:0e:7c:ff:a0:b7.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'stapp01,172.16.238.10' (ECDSA) to the list of known hosts.
		tony@stapp01's password: 
 
[tony@stapp01 ~]$ sudo su - 

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for tony: 

2. Check current docker images on server

[root@stapp01 ~]# docker images
		REPOSITORY   TAG       IMAGE ID   CREATED   SIZE

3. Pull the mentioned docker image

[root@stapp01 ~]# docker pull busybox:musl
		musl: Pulling from library/busybox
		4c50f639dbf8: Pull complete 
		Digest: sha256:7d1702cfd71b9eb2ceea92bcadc293cd1ac17e321542ef9551d58f77e8b798e4
		Status: Downloaded newer image for busybox:musl
		docker.io/library/busybox:musl
 
[root@stapp01 ~]# docker images
		REPOSITORY   TAG       IMAGE ID       CREATED      SIZE
		busybox      musl      24d4c1fb43e8   3 days ago   1.41MB

4. Create new tag/ re-tag the pulled docker image and verify

[root@stapp01 ~]# docker image tag busybox:musl busybox:news
 
[root@stapp01 ~]# docker images
		REPOSITORY   TAG       IMAGE ID       CREATED      SIZE
		busybox      musl      24d4c1fb43e8   3 days ago   1.41MB
		busybox      news      24d4c1fb43e8   3 days ago   1.41MB

--------------------------------------------------------------------------------------------------------------------------------
Task 113: 11/Dec/2022

Managing ACLs Using Ansible

There are some files that need to be created on all app servers in Stratos DC. The Nautilus DevOps team want these files to be owned by user root only however, they also want that the app specific user to have a set of permissions on these files. All tasks must be done using Ansible only, so they need to create a playbook. Below you can find more information about the task.

Create a playbook.yml under /home/thor/ansible on jump host, an inventory file is already present under /home/thor/ansible directory on Jump Server itself.

    Create an empty file blog.txt under /opt/sysops/ directory on app server 1. Set some acl properties for this file. Using acl provide read '(r)' permissions to group tony (i.e entity is tony and etype is group).

    Create an empty file story.txt under /opt/sysops/ directory on app server 2. Set some acl properties for this file. Using acl provide read + write '(rw)' permissions to user steve (i.e entity is steve and etype is user).

    Create an empty file media.txt under /opt/sysops/ on app server 3. Set some acl properties for this file. Using acl provide read + write '(rw)' permissions to group banner (i.e entity is banner and etype is group).

Note: Validation will try to run the playbook using command ansible-playbook -i inventory playbook.yml so please make sure the playbook works this way, without passing any extra arguments.

1. Go to mentioned folder and check ansible inventory file 

thor@jump_host ~$ cd /home/thor/ansible/

thor@jump_host ~/ansible$ ls -ahl
		total 16K
		drwxr-xr-x 2 thor thor 4.0K Dec 11 05:22 .
		drwxr----- 1 thor thor 4.0K Dec 11 05:22 ..
		-rw-r--r-- 1 thor thor   36 Dec 11 05:22 ansible.cfg
		-rw-r--r-- 1 thor thor  237 Dec 11 05:22 inventory

thor@jump_host ~/ansible$ cat inventory 
		stapp01 ansible_host=172.16.238.10 ansible_ssh_pass=Ir0nM@n ansible_user=tony
		stapp02 ansible_host=172.16.238.11 ansible_ssh_pass=Am3ric@ ansible_user=steve
		stapp03 ansible_host=172.16.238.12 ansible_ssh_pass=BigGr33n ansible_user=banner

thor@jump_host ~/ansible$ cat ansible.cfg 
		[defaults]
		host_key_checking = False

thor@jump_host ~/ansible$ ansible -i inventory all -a "ls -ahl /opt/sysops/"
		stapp02 | CHANGED | rc=0 >>
		total 8.0K
		drwxr-xr-x 2 root root 4.0K Dec 11 05:22 .
		drwxr-xr-x 1 root root 4.0K Dec 11 05:22 ..
		stapp03 | CHANGED | rc=0 >>
		total 8.0K
		drwxr-xr-x 2 root root 4.0K Dec 11 05:22 .
		drwxr-xr-x 1 root root 4.0K Dec 11 05:22 ..
		stapp01 | CHANGED | rc=0 >>
		total 8.0K
		drwxr-xr-x 2 root root 4.0K Dec 11 05:22 .
		drwxr-xr-x 1 root root 4.0K Dec 11 05:22 ..

2.Create playbook.yml as per given requirements

thor@jump_host ~/ansible$ vi playbook.yml

thor@jump_host ~/ansible$ cat playbook.yml 
		- name: Create file and set ACL in Host 1
		  hosts: stapp01
		  become: yes
		  tasks:
		    - name: Create the blog.txt on stapp01
		      file:
		        path: /opt/sysops/blog.txt
		        state: touch
		    - name: Set ACL for blog.txt
		      acl:
		        path: /opt/sysops/blog.txt
		        entity: tony
		        etype: group
		        permissions: r
		        state: present
		- name: Create file and set ACL in Host 2
		  hosts: stapp02
		  become: yes
		  tasks:
		    - name: Create the story.txt on stapp02
		      file:
		        path: /opt/sysops/story.txt
		        state: touch
		    - name: Set ACL for story.txt
		      acl:
		        path: /opt/sysops/story.txt
		        entity: steve
		        etype: user
		        permissions: rw
		        state: present
		- name: Create file and set ACL in Host 3
		  hosts: stapp03
		  become: yes
		  tasks:
		    - name: Create the media.txt on stapp03
		      file:
		        path: /opt/sysops/media.txt
		        state: touch
		    - name: Set ACL for media.txt
		      acl:
		        path: /opt/sysops/media.txt
		        entity: banner
		        etype: group
		        permissions: rw
		        state: present

3. Run playbook to make the changes

thor@jump_host ~/ansible$ ansible-playbook -i inventory playbook.yml 

		PLAY [Create file and set ACL in Host 1] ****************************************************************************************************

		TASK [Gathering Facts] **********************************************************************************************************************
		ok: [stapp01]

		TASK [Create the blog.txt on stapp01] *******************************************************************************************************
		changed: [stapp01]

		TASK [Set ACL for blog.txt] *****************************************************************************************************************
		changed: [stapp01]

		PLAY [Create file and set ACL in Host 2] ****************************************************************************************************

		TASK [Gathering Facts] **********************************************************************************************************************
		ok: [stapp02]

		TASK [Create the story.txt on stapp02] ******************************************************************************************************
		changed: [stapp02]

		TASK [Set ACL for story.txt] ****************************************************************************************************************
		changed: [stapp02]

		PLAY [Create file and set ACL in Host 3] ****************************************************************************************************

		TASK [Gathering Facts] **********************************************************************************************************************
		ok: [stapp03]

		TASK [Create the media.txt on stapp03] ******************************************************************************************************
		changed: [stapp03]

		TASK [Set ACL for media.txt] ****************************************************************************************************************
		changed: [stapp03]

		PLAY RECAP **********************************************************************************************************************************
		stapp01                    : ok=3    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
		stapp02                    : ok=3    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
		stapp03                    : ok=3    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   


4. Validate the task by checking the files on each server 

thor@jump_host ~/ansible$ ansible -i inventory all -a "ls -ahl /opt/sysops/"
		stapp02 | CHANGED | rc=0 >>
		total 8.0K
		drwxr-xr-x  2 root root 4.0K Dec 11 05:25 .
		drwxr-xr-x  1 root root 4.0K Dec 11 05:22 ..
		-rw-rw-r--+ 1 root root    0 Dec 11 05:25 story.txt
		stapp01 | CHANGED | rc=0 >>
		total 8.0K
		drwxr-xr-x  2 root root 4.0K Dec 11 05:25 .
		drwxr-xr-x  1 root root 4.0K Dec 11 05:22 ..
		-rw-r--r--+ 1 root root    0 Dec 11 05:25 blog.txt
		stapp03 | CHANGED | rc=0 >>
		total 8.0K
		drwxr-xr-x  2 root root 4.0K Dec 11 05:25 .
		drwxr-xr-x  1 root root 4.0K Dec 11 05:22 ..
		-rw-rw-r--+ 1 root root    0 Dec 11 05:25 media.txt

--------------------------------------------------------------------------------------------------------------------------------
Task 114: 12/Dec/2022

Rollback a Deployment in Kubernetes

This morning the Nautilus DevOps team rolled out a new release for one of the applications. Recently one of the customers logged a complaint which seems to be about a bug related to the recent release. Therefore, the team wants to rollback the recent release.

There is a deployment named nginx-deployment; roll it back to the previous revision.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

1. check existing deployment and pods running status

thor@jump_host ~$ kubectl  get all
		NAME                                    READY   STATUS    RESTARTS   AGE
		pod/nginx-deployment-594c94667b-mvkmr   1/1     Running   0          37s
		pod/nginx-deployment-594c94667b-r7b6k   1/1     Running   0          16s
		pod/nginx-deployment-594c94667b-sfxns   1/1     Running   0          20s

		NAME                    TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
		service/kubernetes      ClusterIP   10.96.0.1      <none>        443/TCP        135m
		service/nginx-service   NodePort    10.96.37.132   <none>        80:30008/TCP   49s

		NAME                               READY   UP-TO-DATE   AVAILABLE   AGE
		deployment.apps/nginx-deployment   3/3     3            3           49s

		NAME                                          DESIRED   CURRENT   READY   AGE
		replicaset.apps/nginx-deployment-594c94667b   3         3         3       38s
		replicaset.apps/nginx-deployment-74fb588559   0         0         0       49s

thor@jump_host ~$ kubectl  get deploy
		NAME               READY   UP-TO-DATE   AVAILABLE   AGE
		nginx-deployment   3/3     3            3           60s

thor@jump_host ~$ kubectl  get pods
		NAME                                READY   STATUS    RESTARTS   AGE
		nginx-deployment-594c94667b-mvkmr   1/1     Running   0          54s
		nginx-deployment-594c94667b-r7b6k   1/1     Running   0          33s
		nginx-deployment-594c94667b-sfxns   1/1     Running   0          37s

2. Rollback/revert last deployment

thor@jump_host ~$ kubectl rollout undo deployment nginx-deployment
		deployment.apps/nginx-deployment rolled back

3. Wait for all pods to get back to running status

thor@jump_host ~$ kubectl  get deploy
		NAME               READY   UP-TO-DATE   AVAILABLE   AGE
		nginx-deployment   3/3     2            3           112s

thor@jump_host ~$ kubectl  get pods
		NAME                                READY   STATUS        RESTARTS   AGE
		nginx-deployment-594c94667b-mvkmr   1/1     Terminating   0          105s
		nginx-deployment-594c94667b-sfxns   0/1     Terminating   0          88s
		nginx-deployment-74fb588559-h7hpg   1/1     Running       0          9s
		nginx-deployment-74fb588559-whhpb   1/1     Running       0          7s
		nginx-deployment-74fb588559-zvjz7   1/1     Running       0          4s

thor@jump_host ~$ kubectl  get pods
		NAME                                READY   STATUS        RESTARTS   AGE
		nginx-deployment-594c94667b-sfxns   0/1     Terminating   0          92s
		nginx-deployment-74fb588559-h7hpg   1/1     Running       0          13s
		nginx-deployment-74fb588559-whhpb   1/1     Running       0          11s
		nginx-deployment-74fb588559-zvjz7   1/1     Running       0          8s

thor@jump_host ~$ kubectl  get pods
		NAME                                READY   STATUS    RESTARTS   AGE
		nginx-deployment-74fb588559-h7hpg   1/1     Running   0          16s
		nginx-deployment-74fb588559-whhpb   1/1     Running   0          14s
		nginx-deployment-74fb588559-zvjz7   1/1     Running   0          11s

4. Validate the task

thor@jump_host ~$ kubectl rollout status deployment nginx-deployment
		deployment "nginx-deployment" successfully rolled out

thor@jump_host ~$ kubectl  get all
		NAME                                    READY   STATUS    RESTARTS   AGE
		pod/nginx-deployment-74fb588559-h7hpg   1/1     Running   0          40s
		pod/nginx-deployment-74fb588559-whhpb   1/1     Running   0          38s
		pod/nginx-deployment-74fb588559-zvjz7   1/1     Running   0          35s

		NAME                    TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
		service/kubernetes      ClusterIP   10.96.0.1      <none>        443/TCP        137m
		service/nginx-service   NodePort    10.96.37.132   <none>        80:30008/TCP   2m28s

		NAME                               READY   UP-TO-DATE   AVAILABLE   AGE
		deployment.apps/nginx-deployment   3/3     3            3           2m28s

		NAME                                          DESIRED   CURRENT   READY   AGE
		replicaset.apps/nginx-deployment-594c94667b   0         0         0       2m17s
		replicaset.apps/nginx-deployment-74fb588559   3         3         3       2m28s
thor@jump_host ~$

--------------------------------------------------------------------------------------------------------------------------------
Task 115: 13/Dec/2022

Ansible Config File Update

To manage all servers within the stack using Ansible, the Nautilus DevOps team is planning to use a common sudo user among all servers. Ansible will be able to use this to perform different tasks on each server. This is not finalized yet, but the team has decided to first perform testing. The DevOps team has already installed Ansible on jump host using yum, and they now have the following requirement:

On jump host make appropriate changes so that Ansible can use kareem as a default ssh user for all hosts. Make changes in Ansible's default configuration only —please do not try to create a new config.


1. Switch to root user on jump_host

		thor@jump_host ~$ sudo su -

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for thor: 

2. check current configuration in ansible config file(/etc/ansible/ansible.cfg) for default ssh user (remote_user) 

[root@jump_host ~]# cat /etc/ansible/ansible.cfg |grep remote_user
		#remote_user = root

3. Change the value of remote_user in config file as per given requirements

[root@jump_host ~]# vi /etc/ansible/ansible.cfg

[root@jump_host ~]# cat /etc/ansible/ansible.cfg |grep remote_user
		remote_user = kareem

[root@jump_host ~]#

--------------------------------------------------------------------------------------------------------------------------------
Task 116: 14/Dec/2022

Git Install and Create Repository

The Nautilus development team shared with the DevOps team requirements for new application development, setting up a Git repository for that project. Create a Git repository on Storage server in Stratos DC as per details given below:

    Install git package using yum on Storage server.

    After that create/init a git repository /opt/apps.git (use the exact name as asked and make sure not to create a bare repository).

1. Login to Storage server and switch to root user

thor@jump_host ~$ ssh natasha@ststor01
		The authenticity of host 'ststor01 (172.16.238.15)' can't be established.
		ECDSA key fingerprint is SHA256:uWWoehA80FSH1JuFHkpfSS/WsONvW8V0Pk5krQQP86Y.
		ECDSA key fingerprint is MD5:a9:cf:a5:29:d8:fa:df:6e:c3:f6:4b:71:8a:c2:dd:cc.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'ststor01,172.16.238.15' (ECDSA) to the list of known hosts.
		natasha@ststor01's password: 
 
[natasha@ststor01 ~]$ sudo su -

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for natasha: 

2. Install git and verify 

[root@ststor01 ~]# rpm -qa|grep git

[root@ststor01 ~]# yum install -y git
		Loaded plugins: fastestmirror, ovl
		Determining fastest mirrors
		 * base: mirror.grid.uchicago.edu
		 * extras: mirror.team-cymru.com
		 * updates: mirror.vacares.com
		base                                                                                                                  | 3.6 kB  00:00:00     
		extras                                                                                                                | 2.9 kB  00:00:00     
		updates                                                                                                               | 2.9 kB  00:00:00     
		(1/4): base/7/x86_64/group_gz                                                                                         | 153 kB  00:00:00     
		(2/4): extras/7/x86_64/primary_db                                                                                     | 249 kB  00:00:00     
		(3/4): base/7/x86_64/primary_db                                                                                       | 6.1 MB  00:00:00     
		(4/4): updates/7/x86_64/primary_db                                                                                    |  18 MB  00:00:05     
		Resolving Dependencies
		--> Running transaction check
		---> Package git.x86_64 0:1.8.3.1-23.el7_8 will be installed
		--> Processing Dependency: perl-Git = 1.8.3.1-23.el7_8 for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl >= 5.008 for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: rsync for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(warnings) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(vars) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(strict) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(lib) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(Term::ReadKey) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(Git) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(Getopt::Long) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(File::stat) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(File::Temp) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(File::Spec) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(File::Path) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(File::Find) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(File::Copy) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(File::Basename) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(Exporter) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(Error) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: openssh-clients for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: less for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: /usr/bin/perl for package: git-1.8.3.1-23.el7_8.x86_64
		--> Running transaction check
		---> Package less.x86_64 0:458-9.el7 will be installed
		--> Processing Dependency: groff-base for package: less-458-9.el7.x86_64
		---> Package openssh-clients.x86_64 0:7.4p1-22.el7_9 will be installed
		--> Processing Dependency: openssh = 7.4p1-22.el7_9 for package: openssh-clients-7.4p1-22.el7_9.x86_64
		--> Processing Dependency: libedit.so.0()(64bit) for package: openssh-clients-7.4p1-22.el7_9.x86_64
		---> Package perl.x86_64 4:5.16.3-299.el7_9 will be installed
		--> Processing Dependency: perl-libs = 4:5.16.3-299.el7_9 for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(Socket) >= 1.3 for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(Scalar::Util) >= 1.10 for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl-macros for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl-libs for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(threads::shared) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(threads) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(constant) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(Time::Local) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(Time::HiRes) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(Storable) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(Socket) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(Scalar::Util) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(Pod::Simple::XHTML) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(Pod::Simple::Search) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(Filter::Util::Call) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(Carp) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: libperl.so()(64bit) for package: 4:perl-5.16.3-299.el7_9.x86_64
		---> Package perl-Error.noarch 1:0.17020-2.el7 will be installed
		---> Package perl-Exporter.noarch 0:5.68-3.el7 will be installed
		---> Package perl-File-Path.noarch 0:2.09-2.el7 will be installed
		---> Package perl-File-Temp.noarch 0:0.23.01-3.el7 will be installed
		---> Package perl-Getopt-Long.noarch 0:2.40-3.el7 will be installed
		--> Processing Dependency: perl(Pod::Usage) >= 1.14 for package: perl-Getopt-Long-2.40-3.el7.noarch
		--> Processing Dependency: perl(Text::ParseWords) for package: perl-Getopt-Long-2.40-3.el7.noarch
		---> Package perl-Git.noarch 0:1.8.3.1-23.el7_8 will be installed
		---> Package perl-PathTools.x86_64 0:3.40-5.el7 will be installed
		---> Package perl-TermReadKey.x86_64 0:2.30-20.el7 will be installed
		---> Package rsync.x86_64 0:3.1.2-11.el7_9 will be installed
		--> Running transaction check
		---> Package groff-base.x86_64 0:1.22.2-8.el7 will be installed
		---> Package libedit.x86_64 0:3.0-12.20121213cvs.el7 will be installed
		---> Package openssh.x86_64 0:7.4p1-21.el7 will be updated
		--> Processing Dependency: openssh = 7.4p1-21.el7 for package: openssh-server-7.4p1-21.el7.x86_64
		---> Package openssh.x86_64 0:7.4p1-22.el7_9 will be an update
		---> Package perl-Carp.noarch 0:1.26-244.el7 will be installed
		---> Package perl-Filter.x86_64 0:1.49-3.el7 will be installed
		---> Package perl-Pod-Simple.noarch 1:3.28-4.el7 will be installed
		--> Processing Dependency: perl(Pod::Escapes) >= 1.04 for package: 1:perl-Pod-Simple-3.28-4.el7.noarch
		--> Processing Dependency: perl(Encode) for package: 1:perl-Pod-Simple-3.28-4.el7.noarch
		---> Package perl-Pod-Usage.noarch 0:1.63-3.el7 will be installed
		--> Processing Dependency: perl(Pod::Text) >= 3.15 for package: perl-Pod-Usage-1.63-3.el7.noarch
		--> Processing Dependency: perl-Pod-Perldoc for package: perl-Pod-Usage-1.63-3.el7.noarch
		---> Package perl-Scalar-List-Utils.x86_64 0:1.27-248.el7 will be installed
		---> Package perl-Socket.x86_64 0:2.010-5.el7 will be installed
		---> Package perl-Storable.x86_64 0:2.45-3.el7 will be installed
		---> Package perl-Text-ParseWords.noarch 0:3.29-4.el7 will be installed
		---> Package perl-Time-HiRes.x86_64 4:1.9725-3.el7 will be installed
		---> Package perl-Time-Local.noarch 0:1.2300-2.el7 will be installed
		---> Package perl-constant.noarch 0:1.27-2.el7 will be installed
		---> Package perl-libs.x86_64 4:5.16.3-299.el7_9 will be installed
		---> Package perl-macros.x86_64 4:5.16.3-299.el7_9 will be installed
		---> Package perl-threads.x86_64 0:1.87-4.el7 will be installed
		---> Package perl-threads-shared.x86_64 0:1.43-6.el7 will be installed
		--> Running transaction check
		---> Package openssh-server.x86_64 0:7.4p1-21.el7 will be updated
		---> Package openssh-server.x86_64 0:7.4p1-22.el7_9 will be an update
		---> Package perl-Encode.x86_64 0:2.51-7.el7 will be installed
		---> Package perl-Pod-Escapes.noarch 1:1.04-299.el7_9 will be installed
		---> Package perl-Pod-Perldoc.noarch 0:3.20-4.el7 will be installed
		--> Processing Dependency: perl(parent) for package: perl-Pod-Perldoc-3.20-4.el7.noarch
		--> Processing Dependency: perl(HTTP::Tiny) for package: perl-Pod-Perldoc-3.20-4.el7.noarch
		---> Package perl-podlators.noarch 0:2.5.1-3.el7 will be installed
		--> Running transaction check
		---> Package perl-HTTP-Tiny.noarch 0:0.033-3.el7 will be installed
		---> Package perl-parent.noarch 1:0.225-244.el7 will be installed
		--> Finished Dependency Resolution

		Dependencies Resolved

		=============================================================================================================================================
		 Package                                  Arch                     Version                                   Repository                 Size
		=============================================================================================================================================
		Installing:
		 git                                      x86_64                   1.8.3.1-23.el7_8                          base                      4.4 M
		Installing for dependencies:
		 groff-base                               x86_64                   1.22.2-8.el7                              base                      942 k
		 less                                     x86_64                   458-9.el7                                 base                      120 k
		 libedit                                  x86_64                   3.0-12.20121213cvs.el7                    base                       92 k
		 openssh-clients                          x86_64                   7.4p1-22.el7_9                            updates                   655 k
		 perl                                     x86_64                   4:5.16.3-299.el7_9                        updates                   8.0 M
		 perl-Carp                                noarch                   1.26-244.el7                              base                       19 k
		 perl-Encode                              x86_64                   2.51-7.el7                                base                      1.5 M
		 perl-Error                               noarch                   1:0.17020-2.el7                           base                       32 k
		 perl-Exporter                            noarch                   5.68-3.el7                                base                       28 k
		 perl-File-Path                           noarch                   2.09-2.el7                                base                       26 k
		 perl-File-Temp                           noarch                   0.23.01-3.el7                             base                       56 k
		 perl-Filter                              x86_64                   1.49-3.el7                                base                       76 k
		 perl-Getopt-Long                         noarch                   2.40-3.el7                                base                       56 k
		 perl-Git                                 noarch                   1.8.3.1-23.el7_8                          base                       56 k
		 perl-HTTP-Tiny                           noarch                   0.033-3.el7                               base                       38 k
		 perl-PathTools                           x86_64                   3.40-5.el7                                base                       82 k
		 perl-Pod-Escapes                         noarch                   1:1.04-299.el7_9                          updates                    52 k
		 perl-Pod-Perldoc                         noarch                   3.20-4.el7                                base                       87 k
		 perl-Pod-Simple                          noarch                   1:3.28-4.el7                              base                      216 k
		 perl-Pod-Usage                           noarch                   1.63-3.el7                                base                       27 k
		 perl-Scalar-List-Utils                   x86_64                   1.27-248.el7                              base                       36 k
		 perl-Socket                              x86_64                   2.010-5.el7                               base                       49 k
		 perl-Storable                            x86_64                   2.45-3.el7                                base                       77 k
		 perl-TermReadKey                         x86_64                   2.30-20.el7                               base                       31 k
		 perl-Text-ParseWords                     noarch                   3.29-4.el7                                base                       14 k
		 perl-Time-HiRes                          x86_64                   4:1.9725-3.el7                            base                       45 k
		 perl-Time-Local                          noarch                   1.2300-2.el7                              base                       24 k
		 perl-constant                            noarch                   1.27-2.el7                                base                       19 k
		 perl-libs                                x86_64                   4:5.16.3-299.el7_9                        updates                   690 k
		 perl-macros                              x86_64                   4:5.16.3-299.el7_9                        updates                    44 k
		 perl-parent                              noarch                   1:0.225-244.el7                           base                       12 k
		 perl-podlators                           noarch                   2.5.1-3.el7                               base                      112 k
		 perl-threads                             x86_64                   1.87-4.el7                                base                       49 k
		 perl-threads-shared                      x86_64                   1.43-6.el7                                base                       39 k
		 rsync                                    x86_64                   3.1.2-11.el7_9                            updates                   408 k
		Updating for dependencies:
		 openssh                                  x86_64                   7.4p1-22.el7_9                            updates                   510 k
		 openssh-server                           x86_64                   7.4p1-22.el7_9                            updates                   459 k

		Transaction Summary
		=============================================================================================================================================
		Install  1 Package  (+35 Dependent packages)
		Upgrade             (  2 Dependent packages)

		Total download size: 19 M
		Downloading packages:
		Delta RPMs disabled because /usr/bin/applydeltarpm not installed.
		(1/38): groff-base-1.22.2-8.el7.x86_64.rpm                                                                            | 942 kB  00:00:00     
		(2/38): less-458-9.el7.x86_64.rpm                                                                                     | 120 kB  00:00:00     
		(3/38): libedit-3.0-12.20121213cvs.el7.x86_64.rpm                                                                     |  92 kB  00:00:00     
		(4/38): git-1.8.3.1-23.el7_8.x86_64.rpm                                                                               | 4.4 MB  00:00:00     
		(5/38): openssh-7.4p1-22.el7_9.x86_64.rpm                                                                             | 510 kB  00:00:00     
		(6/38): openssh-clients-7.4p1-22.el7_9.x86_64.rpm                                                                     | 655 kB  00:00:00     
		(7/38): perl-Carp-1.26-244.el7.noarch.rpm                                                                             |  19 kB  00:00:00     
		(8/38): perl-Error-0.17020-2.el7.noarch.rpm                                                                           |  32 kB  00:00:00     
		(9/38): perl-Exporter-5.68-3.el7.noarch.rpm                                                                           |  28 kB  00:00:00     
		(10/38): perl-File-Path-2.09-2.el7.noarch.rpm                                                                         |  26 kB  00:00:00     
		(11/38): perl-File-Temp-0.23.01-3.el7.noarch.rpm                                                                      |  56 kB  00:00:00     
		(12/38): perl-Filter-1.49-3.el7.x86_64.rpm                                                                            |  76 kB  00:00:00     
		(13/38): openssh-server-7.4p1-22.el7_9.x86_64.rpm                                                                     | 459 kB  00:00:00     
		(14/38): perl-Getopt-Long-2.40-3.el7.noarch.rpm                                                                       |  56 kB  00:00:00     
		(15/38): perl-Git-1.8.3.1-23.el7_8.noarch.rpm                                                                         |  56 kB  00:00:00     
		(16/38): perl-HTTP-Tiny-0.033-3.el7.noarch.rpm                                                                        |  38 kB  00:00:00     
		(17/38): perl-PathTools-3.40-5.el7.x86_64.rpm                                                                         |  82 kB  00:00:00     
		(18/38): perl-Pod-Perldoc-3.20-4.el7.noarch.rpm                                                                       |  87 kB  00:00:00     
		(19/38): perl-Pod-Escapes-1.04-299.el7_9.noarch.rpm                                                                   |  52 kB  00:00:00     
		(20/38): perl-Pod-Simple-3.28-4.el7.noarch.rpm                                                                        | 216 kB  00:00:00     
		(21/38): perl-Pod-Usage-1.63-3.el7.noarch.rpm                                                                         |  27 kB  00:00:00     
		(22/38): perl-Scalar-List-Utils-1.27-248.el7.x86_64.rpm                                                               |  36 kB  00:00:00     
		(23/38): perl-Socket-2.010-5.el7.x86_64.rpm                                                                           |  49 kB  00:00:00     
		(24/38): perl-Storable-2.45-3.el7.x86_64.rpm                                                                          |  77 kB  00:00:00     
		(25/38): perl-TermReadKey-2.30-20.el7.x86_64.rpm                                                                      |  31 kB  00:00:00     
		(26/38): perl-Encode-2.51-7.el7.x86_64.rpm                                                                            | 1.5 MB  00:00:00     
		(27/38): perl-Text-ParseWords-3.29-4.el7.noarch.rpm                                                                   |  14 kB  00:00:00     
		(28/38): perl-Time-HiRes-1.9725-3.el7.x86_64.rpm                                                                      |  45 kB  00:00:00     
		(29/38): perl-Time-Local-1.2300-2.el7.noarch.rpm                                                                      |  24 kB  00:00:00     
		(30/38): perl-constant-1.27-2.el7.noarch.rpm                                                                          |  19 kB  00:00:00     
		(31/38): perl-libs-5.16.3-299.el7_9.x86_64.rpm                                                                        | 690 kB  00:00:00     
		(32/38): perl-parent-0.225-244.el7.noarch.rpm                                                                         |  12 kB  00:00:00     
		(33/38): perl-threads-1.87-4.el7.x86_64.rpm                                                                           |  49 kB  00:00:00     
		(34/38): perl-macros-5.16.3-299.el7_9.x86_64.rpm                                                                      |  44 kB  00:00:00     
		(35/38): perl-threads-shared-1.43-6.el7.x86_64.rpm                                                                    |  39 kB  00:00:00     
		(36/38): perl-podlators-2.5.1-3.el7.noarch.rpm                                                                        | 112 kB  00:00:00     
		(37/38): rsync-3.1.2-11.el7_9.x86_64.rpm                                                                              | 408 kB  00:00:00     
		(38/38): perl-5.16.3-299.el7_9.x86_64.rpm                                                                             | 8.0 MB  00:00:04     
		---------------------------------------------------------------------------------------------------------------------------------------------
		Total                                                                                                        3.2 MB/s |  19 MB  00:00:05     
		Running transaction check
		Running transaction test
		Transaction test succeeded
		Running transaction
		  Installing : groff-base-1.22.2-8.el7.x86_64                                                                                           1/40 
		  Updating   : openssh-7.4p1-22.el7_9.x86_64                                                                                            2/40 
		  Installing : 1:perl-parent-0.225-244.el7.noarch                                                                                       3/40 
		  Installing : perl-HTTP-Tiny-0.033-3.el7.noarch                                                                                        4/40 
		  Installing : perl-podlators-2.5.1-3.el7.noarch                                                                                        5/40 
		  Installing : perl-Pod-Perldoc-3.20-4.el7.noarch                                                                                       6/40 
		  Installing : 1:perl-Pod-Escapes-1.04-299.el7_9.noarch                                                                                 7/40 
		  Installing : perl-Encode-2.51-7.el7.x86_64                                                                                            8/40 
		  Installing : perl-Text-ParseWords-3.29-4.el7.noarch                                                                                   9/40 
		  Installing : perl-Pod-Usage-1.63-3.el7.noarch                                                                                        10/40 
		  Installing : 4:perl-macros-5.16.3-299.el7_9.x86_64                                                                                   11/40 
		  Installing : 4:perl-Time-HiRes-1.9725-3.el7.x86_64                                                                                   12/40 
		  Installing : perl-Exporter-5.68-3.el7.noarch                                                                                         13/40 
		  Installing : perl-constant-1.27-2.el7.noarch                                                                                         14/40 
		  Installing : perl-Socket-2.010-5.el7.x86_64                                                                                          15/40 
		  Installing : perl-Time-Local-1.2300-2.el7.noarch                                                                                     16/40 
		  Installing : perl-Carp-1.26-244.el7.noarch                                                                                           17/40 
		  Installing : perl-Storable-2.45-3.el7.x86_64                                                                                         18/40 
		  Installing : perl-PathTools-3.40-5.el7.x86_64                                                                                        19/40 
		  Installing : perl-Scalar-List-Utils-1.27-248.el7.x86_64                                                                              20/40 
		  Installing : 1:perl-Pod-Simple-3.28-4.el7.noarch                                                                                     21/40 
		  Installing : perl-File-Temp-0.23.01-3.el7.noarch                                                                                     22/40 
		  Installing : perl-File-Path-2.09-2.el7.noarch                                                                                        23/40 
		  Installing : perl-threads-shared-1.43-6.el7.x86_64                                                                                   24/40 
		  Installing : perl-threads-1.87-4.el7.x86_64                                                                                          25/40 
		  Installing : perl-Filter-1.49-3.el7.x86_64                                                                                           26/40 
		  Installing : 4:perl-libs-5.16.3-299.el7_9.x86_64                                                                                     27/40 
		  Installing : perl-Getopt-Long-2.40-3.el7.noarch                                                                                      28/40 
		  Installing : 4:perl-5.16.3-299.el7_9.x86_64                                                                                          29/40 
		  Installing : 1:perl-Error-0.17020-2.el7.noarch                                                                                       30/40 
		  Installing : perl-TermReadKey-2.30-20.el7.x86_64                                                                                     31/40 
		  Installing : less-458-9.el7.x86_64                                                                                                   32/40 
		  Installing : libedit-3.0-12.20121213cvs.el7.x86_64                                                                                   33/40 
		  Installing : openssh-clients-7.4p1-22.el7_9.x86_64                                                                                   34/40 
		  Installing : rsync-3.1.2-11.el7_9.x86_64                                                                                             35/40 
		  Installing : perl-Git-1.8.3.1-23.el7_8.noarch                                                                                        36/40 
		  Installing : git-1.8.3.1-23.el7_8.x86_64                                                                                             37/40 
		  Updating   : openssh-server-7.4p1-22.el7_9.x86_64                                                                                    38/40 
		  Cleanup    : openssh-server-7.4p1-21.el7.x86_64                                                                                      39/40 
		  Cleanup    : openssh-7.4p1-21.el7.x86_64                                                                                             40/40 
		  Verifying  : perl-HTTP-Tiny-0.033-3.el7.noarch                                                                                        1/40 
		  Verifying  : perl-threads-shared-1.43-6.el7.x86_64                                                                                    2/40 
		  Verifying  : 4:perl-Time-HiRes-1.9725-3.el7.x86_64                                                                                    3/40 
		  Verifying  : openssh-clients-7.4p1-22.el7_9.x86_64                                                                                    4/40 
		  Verifying  : perl-Exporter-5.68-3.el7.noarch                                                                                          5/40 
		  Verifying  : perl-constant-1.27-2.el7.noarch                                                                                          6/40 
		  Verifying  : perl-PathTools-3.40-5.el7.x86_64                                                                                         7/40 
		  Verifying  : openssh-7.4p1-22.el7_9.x86_64                                                                                            8/40 
		  Verifying  : 4:perl-macros-5.16.3-299.el7_9.x86_64                                                                                    9/40 
		  Verifying  : git-1.8.3.1-23.el7_8.x86_64                                                                                             10/40 
		  Verifying  : 1:perl-parent-0.225-244.el7.noarch                                                                                      11/40 
		  Verifying  : perl-Socket-2.010-5.el7.x86_64                                                                                          12/40 
		  Verifying  : rsync-3.1.2-11.el7_9.x86_64                                                                                             13/40 
		  Verifying  : perl-TermReadKey-2.30-20.el7.x86_64                                                                                     14/40 
		  Verifying  : groff-base-1.22.2-8.el7.x86_64                                                                                          15/40 
		  Verifying  : perl-File-Temp-0.23.01-3.el7.noarch                                                                                     16/40 
		  Verifying  : 1:perl-Pod-Simple-3.28-4.el7.noarch                                                                                     17/40 
		  Verifying  : perl-Time-Local-1.2300-2.el7.noarch                                                                                     18/40 
		  Verifying  : 1:perl-Pod-Escapes-1.04-299.el7_9.noarch                                                                                19/40 
		  Verifying  : perl-Git-1.8.3.1-23.el7_8.noarch                                                                                        20/40 
		  Verifying  : perl-Carp-1.26-244.el7.noarch                                                                                           21/40 
		  Verifying  : 1:perl-Error-0.17020-2.el7.noarch                                                                                       22/40 
		  Verifying  : perl-Storable-2.45-3.el7.x86_64                                                                                         23/40 
		  Verifying  : perl-Scalar-List-Utils-1.27-248.el7.x86_64                                                                              24/40 
		  Verifying  : perl-Pod-Usage-1.63-3.el7.noarch                                                                                        25/40 
		  Verifying  : perl-Encode-2.51-7.el7.x86_64                                                                                           26/40 
		  Verifying  : perl-Pod-Perldoc-3.20-4.el7.noarch                                                                                      27/40 
		  Verifying  : perl-podlators-2.5.1-3.el7.noarch                                                                                       28/40 
		  Verifying  : 4:perl-5.16.3-299.el7_9.x86_64                                                                                          29/40 
		  Verifying  : perl-File-Path-2.09-2.el7.noarch                                                                                        30/40 
		  Verifying  : libedit-3.0-12.20121213cvs.el7.x86_64                                                                                   31/40 
		  Verifying  : perl-threads-1.87-4.el7.x86_64                                                                                          32/40 
		  Verifying  : openssh-server-7.4p1-22.el7_9.x86_64                                                                                    33/40 
		  Verifying  : perl-Filter-1.49-3.el7.x86_64                                                                                           34/40 
		  Verifying  : perl-Getopt-Long-2.40-3.el7.noarch                                                                                      35/40 
		  Verifying  : perl-Text-ParseWords-3.29-4.el7.noarch                                                                                  36/40 
		  Verifying  : 4:perl-libs-5.16.3-299.el7_9.x86_64                                                                                     37/40 
		  Verifying  : less-458-9.el7.x86_64                                                                                                   38/40 
		  Verifying  : openssh-7.4p1-21.el7.x86_64                                                                                             39/40 
		  Verifying  : openssh-server-7.4p1-21.el7.x86_64                                                                                      40/40 

		Installed:
		  git.x86_64 0:1.8.3.1-23.el7_8                                                                                                              

		Dependency Installed:
		  groff-base.x86_64 0:1.22.2-8.el7             less.x86_64 0:458-9.el7                      libedit.x86_64 0:3.0-12.20121213cvs.el7         
		  openssh-clients.x86_64 0:7.4p1-22.el7_9      perl.x86_64 4:5.16.3-299.el7_9               perl-Carp.noarch 0:1.26-244.el7                 
		  perl-Encode.x86_64 0:2.51-7.el7              perl-Error.noarch 1:0.17020-2.el7            perl-Exporter.noarch 0:5.68-3.el7               
		  perl-File-Path.noarch 0:2.09-2.el7           perl-File-Temp.noarch 0:0.23.01-3.el7        perl-Filter.x86_64 0:1.49-3.el7                 
		  perl-Getopt-Long.noarch 0:2.40-3.el7         perl-Git.noarch 0:1.8.3.1-23.el7_8           perl-HTTP-Tiny.noarch 0:0.033-3.el7             
		  perl-PathTools.x86_64 0:3.40-5.el7           perl-Pod-Escapes.noarch 1:1.04-299.el7_9     perl-Pod-Perldoc.noarch 0:3.20-4.el7            
		  perl-Pod-Simple.noarch 1:3.28-4.el7          perl-Pod-Usage.noarch 0:1.63-3.el7           perl-Scalar-List-Utils.x86_64 0:1.27-248.el7    
		  perl-Socket.x86_64 0:2.010-5.el7             perl-Storable.x86_64 0:2.45-3.el7            perl-TermReadKey.x86_64 0:2.30-20.el7           
		  perl-Text-ParseWords.noarch 0:3.29-4.el7     perl-Time-HiRes.x86_64 4:1.9725-3.el7        perl-Time-Local.noarch 0:1.2300-2.el7           
		  perl-constant.noarch 0:1.27-2.el7            perl-libs.x86_64 4:5.16.3-299.el7_9          perl-macros.x86_64 4:5.16.3-299.el7_9           
		  perl-parent.noarch 1:0.225-244.el7           perl-podlators.noarch 0:2.5.1-3.el7          perl-threads.x86_64 0:1.87-4.el7                
		  perl-threads-shared.x86_64 0:1.43-6.el7      rsync.x86_64 0:3.1.2-11.el7_9               

		Dependency Updated:
		  openssh.x86_64 0:7.4p1-22.el7_9                                   openssh-server.x86_64 0:7.4p1-22.el7_9                                  

		Complete!

[root@ststor01 ~]# rpm -qa|grep git
		git-1.8.3.1-23.el7_8.x86_64

3. Go to mentioned folder and init/create the required git repository 

[root@ststor01 ~]# cd /opt/

[root@ststor01 opt]# git init apps.git
		Initialized empty Git repository in /opt/apps.git/.git/

4. Verify the task  

[root@ststor01 opt]# ls -ahl
		total 12K
		drwxr-xr-x 1 root root 4.0K Dec 14 14:12 .
		drwxr-xr-x 1 root root 4.0K Dec 14 14:09 ..
		drwxr-xr-x 3 root root 4.0K Dec 14 14:12 apps.git

[root@ststor01 opt]# cd apps.git/

[root@ststor01 apps.git]# ls -ahl
		total 12K
		drwxr-xr-x 3 root root 4.0K Dec 14 14:12 .
		drwxr-xr-x 1 root root 4.0K Dec 14 14:12 ..
		drwxr-xr-x 7 root root 4.0K Dec 14 14:12 .git
[root@ststor01 apps.git]#

--------------------------------------------------------------------------------------------------------------------------------
Task 117: 18/Dec/2022

Ansible Lineinfile Module

The Nautilus DevOps team want to install and set up a simple httpd web server on all app servers in Stratos DC. They also want to deploy a sample web page using Ansible. Therefore, write the required playbook to complete this task as per details mentioned below.

We already have an inventory file under /home/thor/ansible directory on jump host. Write a playbook playbook.yml under /home/thor/ansible directory on jump host itself. Using the playbook perform below given tasks:

    Install httpd web server on all app servers, and make sure its service is up and running.

    Create a file /var/www/html/index.html with content:

		This is a Nautilus sample file, created using Ansible!

    Using lineinfile Ansible module add some more content in /var/www/html/index.html file. Below is the content:

		Welcome to Nautilus Group!

Also make sure this new line is added at the top of the file.

    The /var/www/html/index.html file's user and group owner should be apache on all app servers.

    The /var/www/html/index.html file's permissions should be 0744 on all app servers.

Note: Validation will try to run the playbook using command ansible-playbook -i inventory playbook.yml so please make sure the playbook works this way without passing any extra arguments.

1. Go to mentioned  ansible folder and check inventory and confing files

thor@jump_host ~$ cd /home/thor/ansible/

thor@jump_host ~/ansible$ ls -ahl
		total 16K
		drwxr-xr-x 2 thor thor 4.0K Dec 18 14:54 .
		drwxr----- 1 thor thor 4.0K Dec 18 14:54 ..
		-rw-r--r-- 1 thor thor   36 Dec 18 14:54 ansible.cfg
		-rw-r--r-- 1 thor thor  237 Dec 18 14:54 inventory

thor@jump_host ~/ansible$ cat inventory 
		stapp01 ansible_host=172.16.238.10 ansible_ssh_pass=Ir0nM@n ansible_user=tony
		stapp02 ansible_host=172.16.238.11 ansible_ssh_pass=Am3ric@ ansible_user=steve
		stapp03 ansible_host=172.16.238.12 ansible_ssh_pass=BigGr33n ansible_user=banner

thor@jump_host ~/ansible$ cat ansible.cfg 
		[defaults]
		host_key_checking = False

2. Create playbook.yml as per given requirement

thor@jump_host ~/ansible$ vi playbook.yml

thor@jump_host ~/ansible$ cat playbook.yml 
		- name: Install httpd and setup index.html
		  hosts: stapp01, stapp02, stapp03
		  become: yes
		  tasks:
		  - name: Install httpd
		    package:
		      name: httpd
		      state: present
		  - name: Start service httpd, if not started
		    service:
		      name: httpd
		      state: started
		  - name: Add content in index.html. Create file if it does not exist and set file attributes
		    copy:
		      dest: /var/www/html/index.html
		      content: This is a Nautilus sample file, created using Ansible!
		      mode: "0744"
		      owner: apache
		      group: apache
		  - name: Update content in index.html
		    lineinfile:
		      path: /var/www/html/index.html
		      insertbefore: BOF
		      line: Welcome to Nautilus Group!

3. Check current status on app server before running playbook

thor@jump_host ~/ansible$ ansible -i inventory  all -a "cat /var/www/html/index.html"
		stapp03 | CHANGED | rc=0 >>
		This is KodeKloud Ansible Lab !
		stapp01 | CHANGED | rc=0 >>
		This is KodeKloud Ansible Lab !
		stapp02 | CHANGED | rc=0 >>
		This is KodeKloud Ansible Lab !

4. Run the ansible playbook

thor@jump_host ~/ansible$ ansible-playbook -i inventory playbook.yml 

		PLAY [Install httpd and setup index.html] ***************************************************************************************************

		TASK [Gathering Facts] **********************************************************************************************************************
		ok: [stapp01]
		ok: [stapp03]
		ok: [stapp02]

		TASK [Install httpd] ************************************************************************************************************************
		changed: [stapp01]
		changed: [stapp02]
		changed: [stapp03]

		TASK [Start service httpd, if not started] **************************************************************************************************
		changed: [stapp01]
		changed: [stapp03]
		changed: [stapp02]

		TASK [Add content in index.html. Create file if it does not exist and set file attributes] **************************************************
		changed: [stapp03]
		changed: [stapp01]
		changed: [stapp02]

		TASK [Update content in index.html] *********************************************************************************************************
		changed: [stapp03]
		changed: [stapp01]
		changed: [stapp02]

		PLAY RECAP **********************************************************************************************************************************
		stapp01                    : ok=5    changed=4    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
		stapp02                    : ok=5    changed=4    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
		stapp03                    : ok=5    changed=4    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   


5. Validate the task by checking changed index file and curl on each app server

thor@jump_host ~/ansible$ ansible -i inventory  all -a "cat /var/www/html/index.html"
		stapp02 | CHANGED | rc=0 >>
		Welcome to Nautilus Group!
		This is a Nautilus sample file, created using Ansible!
		stapp03 | CHANGED | rc=0 >>
		Welcome to Nautilus Group!
		This is a Nautilus sample file, created using Ansible!
		stapp01 | CHANGED | rc=0 >>
		Welcome to Nautilus Group!
		This is a Nautilus sample file, created using Ansible!

thor@jump_host ~/ansible$ curl http://stapp01
		Welcome to Nautilus Group!
		This is a Nautilus sample file, created using Ansible!

thor@jump_host ~/ansible$ curl http://stapp02
		Welcome to Nautilus Group!
		This is a Nautilus sample file, created using Ansible!

thor@jump_host ~/ansible$ curl http://stapp03
		Welcome to Nautilus Group!
		This is a Nautilus sample file, created using Ansible!
thor@jump_host ~/ansible$ 

--------------------------------------------------------------------------------------------------------------------------------
Task 118: 20/Dec/2022

 Git Setup from Scratch

 Some new developers have joined xFusionCorp Industries and have been assigned Nautilus project. They are going to start development on a new application, and some pre-requisites have been shared with the DevOps team to proceed with. Please note that all tasks need to be performed on storage server in Stratos DC.

a. Install git, set up any values for user.email and user.name globally and create a bare repository /opt/beta.git.

b. There is an update hook (to block direct pushes to master branch) under /tmp on storage server itself; use the same to block direct pushes to master branch in /opt/beta.git repo.

c. Clone /opt/beta.git repo in /usr/src/kodekloudrepos/beta directory.

d. Create a new branch xfusioncorp_beta in repo that you cloned in /usr/src/kodekloudrepos.

e. There is a readme.md file in /tmp on storage server itself; copy that to repo, add/commit in the new branch you created, and finally push your branch to origin.

f. Also create master branch from your branch and remember you should not be able to push to master as per hook you have set up.

1. Login to Storage server and swithc to root user 

thor@jump_host ~$ ssh natasha@ststor01
			The authenticity of host 'ststor01 (172.16.238.15)' can't be established.
			ECDSA key fingerprint is SHA256:xI9jm4UPQLaCbWnPSdAYxPs5mhviGmA6fT/Xvk5TFAQ.
			ECDSA key fingerprint is MD5:fb:41:2b:5d:c6:59:f6:f5:09:9f:48:c4:73:fc:60:49.
			Are you sure you want to continue connecting (yes/no)? yes
			Warning: Permanently added 'ststor01,172.16.238.15' (ECDSA) to the list of known hosts.
			natasha@ststor01's password: 

[natasha@ststor01 ~]$ sudo su -

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for natasha: 

2. Install git 

[root@ststor01 ~]# rpm -qa|grep git 

[root@ststor01 ~]# yum install -y git
		Loaded plugins: fastestmirror, ovl
		Determining fastest mirrors
		 * base: repos.forethought.net
		 * extras: mirror.us.oneandone.net
		 * updates: mirror.dal.nexril.net
		base                                                                                                                  | 3.6 kB  00:00:00     
		extras                                                                                                                | 2.9 kB  00:00:00     
		updates                                                                                                               | 2.9 kB  00:00:00     
		(1/4): base/7/x86_64/group_gz                                                                                         | 153 kB  00:00:00     
		(2/4): extras/7/x86_64/primary_db                                                                                     | 249 kB  00:00:00     
		(3/4): base/7/x86_64/primary_db                                                                                       | 6.1 MB  00:00:00     
		(4/4): updates/7/x86_64/primary_db                                                                                    |  19 MB  00:00:00     
		Resolving Dependencies
		--> Running transaction check
		---> Package git.x86_64 0:1.8.3.1-23.el7_8 will be installed
		--> Processing Dependency: perl-Git = 1.8.3.1-23.el7_8 for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl >= 5.008 for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: rsync for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(warnings) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(vars) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(strict) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(lib) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(Term::ReadKey) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(Git) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(Getopt::Long) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(File::stat) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(File::Temp) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(File::Spec) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(File::Path) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(File::Find) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(File::Copy) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(File::Basename) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(Exporter) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: perl(Error) for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: openssh-clients for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: less for package: git-1.8.3.1-23.el7_8.x86_64
		--> Processing Dependency: /usr/bin/perl for package: git-1.8.3.1-23.el7_8.x86_64
		--> Running transaction check
		---> Package less.x86_64 0:458-9.el7 will be installed
		--> Processing Dependency: groff-base for package: less-458-9.el7.x86_64
		---> Package openssh-clients.x86_64 0:7.4p1-22.el7_9 will be installed
		--> Processing Dependency: openssh = 7.4p1-22.el7_9 for package: openssh-clients-7.4p1-22.el7_9.x86_64
		--> Processing Dependency: libedit.so.0()(64bit) for package: openssh-clients-7.4p1-22.el7_9.x86_64
		---> Package perl.x86_64 4:5.16.3-299.el7_9 will be installed
		--> Processing Dependency: perl-libs = 4:5.16.3-299.el7_9 for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(Socket) >= 1.3 for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(Scalar::Util) >= 1.10 for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl-macros for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl-libs for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(threads::shared) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(threads) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(constant) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(Time::Local) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(Time::HiRes) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(Storable) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(Socket) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(Scalar::Util) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(Pod::Simple::XHTML) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(Pod::Simple::Search) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(Filter::Util::Call) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: perl(Carp) for package: 4:perl-5.16.3-299.el7_9.x86_64
		--> Processing Dependency: libperl.so()(64bit) for package: 4:perl-5.16.3-299.el7_9.x86_64
		---> Package perl-Error.noarch 1:0.17020-2.el7 will be installed
		---> Package perl-Exporter.noarch 0:5.68-3.el7 will be installed
		---> Package perl-File-Path.noarch 0:2.09-2.el7 will be installed
		---> Package perl-File-Temp.noarch 0:0.23.01-3.el7 will be installed
		---> Package perl-Getopt-Long.noarch 0:2.40-3.el7 will be installed
		--> Processing Dependency: perl(Pod::Usage) >= 1.14 for package: perl-Getopt-Long-2.40-3.el7.noarch
		--> Processing Dependency: perl(Text::ParseWords) for package: perl-Getopt-Long-2.40-3.el7.noarch
		---> Package perl-Git.noarch 0:1.8.3.1-23.el7_8 will be installed
		---> Package perl-PathTools.x86_64 0:3.40-5.el7 will be installed
		---> Package perl-TermReadKey.x86_64 0:2.30-20.el7 will be installed
		---> Package rsync.x86_64 0:3.1.2-12.el7_9 will be installed
		--> Running transaction check
		---> Package groff-base.x86_64 0:1.22.2-8.el7 will be installed
		---> Package libedit.x86_64 0:3.0-12.20121213cvs.el7 will be installed
		---> Package openssh.x86_64 0:7.4p1-21.el7 will be updated
		--> Processing Dependency: openssh = 7.4p1-21.el7 for package: openssh-server-7.4p1-21.el7.x86_64
		---> Package openssh.x86_64 0:7.4p1-22.el7_9 will be an update
		---> Package perl-Carp.noarch 0:1.26-244.el7 will be installed
		---> Package perl-Filter.x86_64 0:1.49-3.el7 will be installed
		---> Package perl-Pod-Simple.noarch 1:3.28-4.el7 will be installed
		--> Processing Dependency: perl(Pod::Escapes) >= 1.04 for package: 1:perl-Pod-Simple-3.28-4.el7.noarch
		--> Processing Dependency: perl(Encode) for package: 1:perl-Pod-Simple-3.28-4.el7.noarch
		---> Package perl-Pod-Usage.noarch 0:1.63-3.el7 will be installed
		--> Processing Dependency: perl(Pod::Text) >= 3.15 for package: perl-Pod-Usage-1.63-3.el7.noarch
		--> Processing Dependency: perl-Pod-Perldoc for package: perl-Pod-Usage-1.63-3.el7.noarch
		---> Package perl-Scalar-List-Utils.x86_64 0:1.27-248.el7 will be installed
		---> Package perl-Socket.x86_64 0:2.010-5.el7 will be installed
		---> Package perl-Storable.x86_64 0:2.45-3.el7 will be installed
		---> Package perl-Text-ParseWords.noarch 0:3.29-4.el7 will be installed
		---> Package perl-Time-HiRes.x86_64 4:1.9725-3.el7 will be installed
		---> Package perl-Time-Local.noarch 0:1.2300-2.el7 will be installed
		---> Package perl-constant.noarch 0:1.27-2.el7 will be installed
		---> Package perl-libs.x86_64 4:5.16.3-299.el7_9 will be installed
		---> Package perl-macros.x86_64 4:5.16.3-299.el7_9 will be installed
		---> Package perl-threads.x86_64 0:1.87-4.el7 will be installed
		---> Package perl-threads-shared.x86_64 0:1.43-6.el7 will be installed
		--> Running transaction check
		---> Package openssh-server.x86_64 0:7.4p1-21.el7 will be updated
		---> Package openssh-server.x86_64 0:7.4p1-22.el7_9 will be an update
		---> Package perl-Encode.x86_64 0:2.51-7.el7 will be installed
		---> Package perl-Pod-Escapes.noarch 1:1.04-299.el7_9 will be installed
		---> Package perl-Pod-Perldoc.noarch 0:3.20-4.el7 will be installed
		--> Processing Dependency: perl(parent) for package: perl-Pod-Perldoc-3.20-4.el7.noarch
		--> Processing Dependency: perl(HTTP::Tiny) for package: perl-Pod-Perldoc-3.20-4.el7.noarch
		---> Package perl-podlators.noarch 0:2.5.1-3.el7 will be installed
		--> Running transaction check
		---> Package perl-HTTP-Tiny.noarch 0:0.033-3.el7 will be installed
		---> Package perl-parent.noarch 1:0.225-244.el7 will be installed
		--> Finished Dependency Resolution

		Dependencies Resolved

		=============================================================================================================================================
		 Package                                  Arch                     Version                                   Repository                 Size
		=============================================================================================================================================
		Installing:
		 git                                      x86_64                   1.8.3.1-23.el7_8                          base                      4.4 M
		Installing for dependencies:
		 groff-base                               x86_64                   1.22.2-8.el7                              base                      942 k
		 less                                     x86_64                   458-9.el7                                 base                      120 k
		 libedit                                  x86_64                   3.0-12.20121213cvs.el7                    base                       92 k
		 openssh-clients                          x86_64                   7.4p1-22.el7_9                            updates                   655 k
		 perl                                     x86_64                   4:5.16.3-299.el7_9                        updates                   8.0 M
		 perl-Carp                                noarch                   1.26-244.el7                              base                       19 k
		 perl-Encode                              x86_64                   2.51-7.el7                                base                      1.5 M
		 perl-Error                               noarch                   1:0.17020-2.el7                           base                       32 k
		 perl-Exporter                            noarch                   5.68-3.el7                                base                       28 k
		 perl-File-Path                           noarch                   2.09-2.el7                                base                       26 k
		 perl-File-Temp                           noarch                   0.23.01-3.el7                             base                       56 k
		 perl-Filter                              x86_64                   1.49-3.el7                                base                       76 k
		 perl-Getopt-Long                         noarch                   2.40-3.el7                                base                       56 k
		 perl-Git                                 noarch                   1.8.3.1-23.el7_8                          base                       56 k
		 perl-HTTP-Tiny                           noarch                   0.033-3.el7                               base                       38 k
		 perl-PathTools                           x86_64                   3.40-5.el7                                base                       82 k
		 perl-Pod-Escapes                         noarch                   1:1.04-299.el7_9                          updates                    52 k
		 perl-Pod-Perldoc                         noarch                   3.20-4.el7                                base                       87 k
		 perl-Pod-Simple                          noarch                   1:3.28-4.el7                              base                      216 k
		 perl-Pod-Usage                           noarch                   1.63-3.el7                                base                       27 k
		 perl-Scalar-List-Utils                   x86_64                   1.27-248.el7                              base                       36 k
		 perl-Socket                              x86_64                   2.010-5.el7                               base                       49 k
		 perl-Storable                            x86_64                   2.45-3.el7                                base                       77 k
		 perl-TermReadKey                         x86_64                   2.30-20.el7                               base                       31 k
		 perl-Text-ParseWords                     noarch                   3.29-4.el7                                base                       14 k
		 perl-Time-HiRes                          x86_64                   4:1.9725-3.el7                            base                       45 k
		 perl-Time-Local                          noarch                   1.2300-2.el7                              base                       24 k
		 perl-constant                            noarch                   1.27-2.el7                                base                       19 k
		 perl-libs                                x86_64                   4:5.16.3-299.el7_9                        updates                   690 k
		 perl-macros                              x86_64                   4:5.16.3-299.el7_9                        updates                    44 k
		 perl-parent                              noarch                   1:0.225-244.el7                           base                       12 k
		 perl-podlators                           noarch                   2.5.1-3.el7                               base                      112 k
		 perl-threads                             x86_64                   1.87-4.el7                                base                       49 k
		 perl-threads-shared                      x86_64                   1.43-6.el7                                base                       39 k
		 rsync                                    x86_64                   3.1.2-12.el7_9                            updates                   408 k
		Updating for dependencies:
		 openssh                                  x86_64                   7.4p1-22.el7_9                            updates                   510 k
		 openssh-server                           x86_64                   7.4p1-22.el7_9                            updates                   459 k

		Transaction Summary
		=============================================================================================================================================
		Install  1 Package  (+35 Dependent packages)
		Upgrade             (  2 Dependent packages)

		Total download size: 19 M
		Downloading packages:
		Delta RPMs disabled because /usr/bin/applydeltarpm not installed.
		(1/38): groff-base-1.22.2-8.el7.x86_64.rpm                                                                            | 942 kB  00:00:00     
		(2/38): less-458-9.el7.x86_64.rpm                                                                                     | 120 kB  00:00:00     
		(3/38): git-1.8.3.1-23.el7_8.x86_64.rpm                                                                               | 4.4 MB  00:00:00     
		(4/38): libedit-3.0-12.20121213cvs.el7.x86_64.rpm                                                                     |  92 kB  00:00:00     
		(5/38): openssh-clients-7.4p1-22.el7_9.x86_64.rpm                                                                     | 655 kB  00:00:00     
		(6/38): openssh-7.4p1-22.el7_9.x86_64.rpm                                                                             | 510 kB  00:00:00     
		(7/38): openssh-server-7.4p1-22.el7_9.x86_64.rpm                                                                      | 459 kB  00:00:00     
		(8/38): perl-Carp-1.26-244.el7.noarch.rpm                                                                             |  19 kB  00:00:00     
		(9/38): perl-Error-0.17020-2.el7.noarch.rpm                                                                           |  32 kB  00:00:00     
		(10/38): perl-Exporter-5.68-3.el7.noarch.rpm                                                                          |  28 kB  00:00:00     
		(11/38): perl-File-Path-2.09-2.el7.noarch.rpm                                                                         |  26 kB  00:00:00     
		(12/38): perl-File-Temp-0.23.01-3.el7.noarch.rpm                                                                      |  56 kB  00:00:00     
		(13/38): perl-Filter-1.49-3.el7.x86_64.rpm                                                                            |  76 kB  00:00:00     
		(14/38): perl-Getopt-Long-2.40-3.el7.noarch.rpm                                                                       |  56 kB  00:00:00     
		(15/38): perl-Git-1.8.3.1-23.el7_8.noarch.rpm                                                                         |  56 kB  00:00:00     
		(16/38): perl-HTTP-Tiny-0.033-3.el7.noarch.rpm                                                                        |  38 kB  00:00:00     
		(17/38): perl-Pod-Escapes-1.04-299.el7_9.noarch.rpm                                                                   |  52 kB  00:00:00     
		(18/38): perl-Encode-2.51-7.el7.x86_64.rpm                                                                            | 1.5 MB  00:00:00     
		(19/38): perl-PathTools-3.40-5.el7.x86_64.rpm                                                                         |  82 kB  00:00:00     
		(20/38): perl-Pod-Perldoc-3.20-4.el7.noarch.rpm                                                                       |  87 kB  00:00:00     
		(21/38): perl-Pod-Simple-3.28-4.el7.noarch.rpm                                                                        | 216 kB  00:00:00     
		(22/38): perl-Pod-Usage-1.63-3.el7.noarch.rpm                                                                         |  27 kB  00:00:00     
		(23/38): perl-Scalar-List-Utils-1.27-248.el7.x86_64.rpm                                                               |  36 kB  00:00:00     
		(24/38): perl-Socket-2.010-5.el7.x86_64.rpm                                                                           |  49 kB  00:00:00     
		(25/38): perl-Storable-2.45-3.el7.x86_64.rpm                                                                          |  77 kB  00:00:00     
		(26/38): perl-TermReadKey-2.30-20.el7.x86_64.rpm                                                                      |  31 kB  00:00:00     
		(27/38): perl-Text-ParseWords-3.29-4.el7.noarch.rpm                                                                   |  14 kB  00:00:00     
		(28/38): perl-Time-HiRes-1.9725-3.el7.x86_64.rpm                                                                      |  45 kB  00:00:00     
		(29/38): perl-Time-Local-1.2300-2.el7.noarch.rpm                                                                      |  24 kB  00:00:00     
		(30/38): perl-constant-1.27-2.el7.noarch.rpm                                                                          |  19 kB  00:00:00     
		(31/38): perl-libs-5.16.3-299.el7_9.x86_64.rpm                                                                        | 690 kB  00:00:00     
		(32/38): perl-macros-5.16.3-299.el7_9.x86_64.rpm                                                                      |  44 kB  00:00:00     
		(33/38): perl-parent-0.225-244.el7.noarch.rpm                                                                         |  12 kB  00:00:00     
		(34/38): perl-threads-1.87-4.el7.x86_64.rpm                                                                           |  49 kB  00:00:00     
		(35/38): perl-threads-shared-1.43-6.el7.x86_64.rpm                                                                    |  39 kB  00:00:00     
		(36/38): rsync-3.1.2-12.el7_9.x86_64.rpm                                                                              | 408 kB  00:00:00     
		(37/38): perl-podlators-2.5.1-3.el7.noarch.rpm                                                                        | 112 kB  00:00:00     
		(38/38): perl-5.16.3-299.el7_9.x86_64.rpm                                                                             | 8.0 MB  00:00:00     
		---------------------------------------------------------------------------------------------------------------------------------------------
		Total                                                                                                         15 MB/s |  19 MB  00:00:01     
		Running transaction check
		Running transaction test
		Transaction test succeeded
		Running transaction
		  Installing : groff-base-1.22.2-8.el7.x86_64                                                                                           1/40 
		  Updating   : openssh-7.4p1-22.el7_9.x86_64                                                                                            2/40 
		  Installing : 1:perl-parent-0.225-244.el7.noarch                                                                                       3/40 
		  Installing : perl-HTTP-Tiny-0.033-3.el7.noarch                                                                                        4/40 
		  Installing : perl-podlators-2.5.1-3.el7.noarch                                                                                        5/40 
		  Installing : perl-Pod-Perldoc-3.20-4.el7.noarch                                                                                       6/40 
		  Installing : 1:perl-Pod-Escapes-1.04-299.el7_9.noarch                                                                                 7/40 
		  Installing : perl-Encode-2.51-7.el7.x86_64                                                                                            8/40 
		  Installing : perl-Text-ParseWords-3.29-4.el7.noarch                                                                                   9/40 
		  Installing : perl-Pod-Usage-1.63-3.el7.noarch                                                                                        10/40 
		  Installing : 4:perl-macros-5.16.3-299.el7_9.x86_64                                                                                   11/40 
		  Installing : 4:perl-Time-HiRes-1.9725-3.el7.x86_64                                                                                   12/40 
		  Installing : perl-Exporter-5.68-3.el7.noarch                                                                                         13/40 
		  Installing : perl-constant-1.27-2.el7.noarch                                                                                         14/40 
		  Installing : perl-Socket-2.010-5.el7.x86_64                                                                                          15/40 
		  Installing : perl-Time-Local-1.2300-2.el7.noarch                                                                                     16/40 
		  Installing : perl-Carp-1.26-244.el7.noarch                                                                                           17/40 
		  Installing : perl-Storable-2.45-3.el7.x86_64                                                                                         18/40 
		  Installing : perl-PathTools-3.40-5.el7.x86_64                                                                                        19/40 
		  Installing : perl-Scalar-List-Utils-1.27-248.el7.x86_64                                                                              20/40 
		  Installing : 1:perl-Pod-Simple-3.28-4.el7.noarch                                                                                     21/40 
		  Installing : perl-File-Temp-0.23.01-3.el7.noarch                                                                                     22/40 
		  Installing : perl-File-Path-2.09-2.el7.noarch                                                                                        23/40 
		  Installing : perl-threads-shared-1.43-6.el7.x86_64                                                                                   24/40 
		  Installing : perl-threads-1.87-4.el7.x86_64                                                                                          25/40 
		  Installing : perl-Filter-1.49-3.el7.x86_64                                                                                           26/40 
		  Installing : 4:perl-libs-5.16.3-299.el7_9.x86_64                                                                                     27/40 
		  Installing : perl-Getopt-Long-2.40-3.el7.noarch                                                                                      28/40 
		  Installing : 4:perl-5.16.3-299.el7_9.x86_64                                                                                          29/40 
		  Installing : 1:perl-Error-0.17020-2.el7.noarch                                                                                       30/40 
		  Installing : perl-TermReadKey-2.30-20.el7.x86_64                                                                                     31/40 
		  Installing : less-458-9.el7.x86_64                                                                                                   32/40 
		  Installing : libedit-3.0-12.20121213cvs.el7.x86_64                                                                                   33/40 
		  Installing : openssh-clients-7.4p1-22.el7_9.x86_64                                                                                   34/40 
		  Installing : rsync-3.1.2-12.el7_9.x86_64                                                                                             35/40 
		  Installing : perl-Git-1.8.3.1-23.el7_8.noarch                                                                                        36/40 
		  Installing : git-1.8.3.1-23.el7_8.x86_64                                                                                             37/40 
		  Updating   : openssh-server-7.4p1-22.el7_9.x86_64                                                                                    38/40 
		  Cleanup    : openssh-server-7.4p1-21.el7.x86_64                                                                                      39/40 
		  Cleanup    : openssh-7.4p1-21.el7.x86_64                                                                                             40/40 
		  Verifying  : perl-HTTP-Tiny-0.033-3.el7.noarch                                                                                        1/40 
		  Verifying  : rsync-3.1.2-12.el7_9.x86_64                                                                                              2/40 
		  Verifying  : perl-threads-shared-1.43-6.el7.x86_64                                                                                    3/40 
		  Verifying  : 4:perl-Time-HiRes-1.9725-3.el7.x86_64                                                                                    4/40 
		  Verifying  : openssh-clients-7.4p1-22.el7_9.x86_64                                                                                    5/40 
		  Verifying  : perl-Exporter-5.68-3.el7.noarch                                                                                          6/40 
		  Verifying  : perl-constant-1.27-2.el7.noarch                                                                                          7/40 
		  Verifying  : perl-PathTools-3.40-5.el7.x86_64                                                                                         8/40 
		  Verifying  : openssh-7.4p1-22.el7_9.x86_64                                                                                            9/40 
		  Verifying  : 4:perl-macros-5.16.3-299.el7_9.x86_64                                                                                   10/40 
		  Verifying  : git-1.8.3.1-23.el7_8.x86_64                                                                                             11/40 
		  Verifying  : 1:perl-parent-0.225-244.el7.noarch                                                                                      12/40 
		  Verifying  : perl-Socket-2.010-5.el7.x86_64                                                                                          13/40 
		  Verifying  : perl-TermReadKey-2.30-20.el7.x86_64                                                                                     14/40 
		  Verifying  : groff-base-1.22.2-8.el7.x86_64                                                                                          15/40 
		  Verifying  : perl-File-Temp-0.23.01-3.el7.noarch                                                                                     16/40 
		  Verifying  : 1:perl-Pod-Simple-3.28-4.el7.noarch                                                                                     17/40 
		  Verifying  : perl-Time-Local-1.2300-2.el7.noarch                                                                                     18/40 
		  Verifying  : 1:perl-Pod-Escapes-1.04-299.el7_9.noarch                                                                                19/40 
		  Verifying  : perl-Git-1.8.3.1-23.el7_8.noarch                                                                                        20/40 
		  Verifying  : perl-Carp-1.26-244.el7.noarch                                                                                           21/40 
		  Verifying  : 1:perl-Error-0.17020-2.el7.noarch                                                                                       22/40 
		  Verifying  : perl-Storable-2.45-3.el7.x86_64                                                                                         23/40 
		  Verifying  : perl-Scalar-List-Utils-1.27-248.el7.x86_64                                                                              24/40 
		  Verifying  : perl-Pod-Usage-1.63-3.el7.noarch                                                                                        25/40 
		  Verifying  : perl-Encode-2.51-7.el7.x86_64                                                                                           26/40 
		  Verifying  : perl-Pod-Perldoc-3.20-4.el7.noarch                                                                                      27/40 
		  Verifying  : perl-podlators-2.5.1-3.el7.noarch                                                                                       28/40 
		  Verifying  : 4:perl-5.16.3-299.el7_9.x86_64                                                                                          29/40 
		  Verifying  : perl-File-Path-2.09-2.el7.noarch                                                                                        30/40 
		  Verifying  : libedit-3.0-12.20121213cvs.el7.x86_64                                                                                   31/40 
		  Verifying  : perl-threads-1.87-4.el7.x86_64                                                                                          32/40 
		  Verifying  : openssh-server-7.4p1-22.el7_9.x86_64                                                                                    33/40 
		  Verifying  : perl-Filter-1.49-3.el7.x86_64                                                                                           34/40 
		  Verifying  : perl-Getopt-Long-2.40-3.el7.noarch                                                                                      35/40 
		  Verifying  : perl-Text-ParseWords-3.29-4.el7.noarch                                                                                  36/40 
		  Verifying  : 4:perl-libs-5.16.3-299.el7_9.x86_64                                                                                     37/40 
		  Verifying  : less-458-9.el7.x86_64                                                                                                   38/40 
		  Verifying  : openssh-7.4p1-21.el7.x86_64                                                                                             39/40 
		  Verifying  : openssh-server-7.4p1-21.el7.x86_64                                                                                      40/40 

		Installed:
		  git.x86_64 0:1.8.3.1-23.el7_8                                                                                                              

		Dependency Installed:
		  groff-base.x86_64 0:1.22.2-8.el7             less.x86_64 0:458-9.el7                      libedit.x86_64 0:3.0-12.20121213cvs.el7         
		  openssh-clients.x86_64 0:7.4p1-22.el7_9      perl.x86_64 4:5.16.3-299.el7_9               perl-Carp.noarch 0:1.26-244.el7                 
		  perl-Encode.x86_64 0:2.51-7.el7              perl-Error.noarch 1:0.17020-2.el7            perl-Exporter.noarch 0:5.68-3.el7               
		  perl-File-Path.noarch 0:2.09-2.el7           perl-File-Temp.noarch 0:0.23.01-3.el7        perl-Filter.x86_64 0:1.49-3.el7                 
		  perl-Getopt-Long.noarch 0:2.40-3.el7         perl-Git.noarch 0:1.8.3.1-23.el7_8           perl-HTTP-Tiny.noarch 0:0.033-3.el7             
		  perl-PathTools.x86_64 0:3.40-5.el7           perl-Pod-Escapes.noarch 1:1.04-299.el7_9     perl-Pod-Perldoc.noarch 0:3.20-4.el7            
		  perl-Pod-Simple.noarch 1:3.28-4.el7          perl-Pod-Usage.noarch 0:1.63-3.el7           perl-Scalar-List-Utils.x86_64 0:1.27-248.el7    
		  perl-Socket.x86_64 0:2.010-5.el7             perl-Storable.x86_64 0:2.45-3.el7            perl-TermReadKey.x86_64 0:2.30-20.el7           
		  perl-Text-ParseWords.noarch 0:3.29-4.el7     perl-Time-HiRes.x86_64 4:1.9725-3.el7        perl-Time-Local.noarch 0:1.2300-2.el7           
		  perl-constant.noarch 0:1.27-2.el7            perl-libs.x86_64 4:5.16.3-299.el7_9          perl-macros.x86_64 4:5.16.3-299.el7_9           
		  perl-parent.noarch 1:0.225-244.el7           perl-podlators.noarch 0:2.5.1-3.el7          perl-threads.x86_64 0:1.87-4.el7                
		  perl-threads-shared.x86_64 0:1.43-6.el7      rsync.x86_64 0:3.1.2-12.el7_9               

		Dependency Updated:
		  openssh.x86_64 0:7.4p1-22.el7_9                                   openssh-server.x86_64 0:7.4p1-22.el7_9                                  

		Complete!

[root@ststor01 ~]# rpm -qa|grep git 
		git-1.8.3.1-23.el7_8.x86_64

3. Add git global variables values 

[root@ststor01 ~]# git config --global --add user.name natasha

[root@ststor01 ~]# git config --global --add user.email natasha@stratos.xfusioncorp.com

[root@ststor01 ~]# git config --global -l
		user.name=natasha
		user.email=natasha@stratos.xfusioncorp.com

4. Create git bare repo

[root@ststor01 ~]# git init --bare /opt/beta.git
		Initialized empty Git repository in /opt/beta.git/

[root@ststor01 ~]# ls -ahl /opt/beta.git/
		total 40K
		drwxr-xr-x 7 root root 4.0K Dec 21 04:05 .
		drwxr-xr-x 1 root root 4.0K Dec 21 04:05 ..
		drwxr-xr-x 2 root root 4.0K Dec 21 04:05 branches
		-rw-r--r-- 1 root root   66 Dec 21 04:05 config
		-rw-r--r-- 1 root root   73 Dec 21 04:05 description
		-rw-r--r-- 1 root root   23 Dec 21 04:05 HEAD
		drwxr-xr-x 2 root root 4.0K Dec 21 04:05 hooks
		drwxr-xr-x 2 root root 4.0K Dec 21 04:05 info
		drwxr-xr-x 4 root root 4.0K Dec 21 04:05 objects
		drwxr-xr-x 4 root root 4.0K Dec 21 04:05 refs

5. copy the hook file from given location to hhoks folder under git repo 

[root@ststor01 ~]# cat /tmp/update 
		#!/bin/sh
		if [ "$1" == refs/heads/master ];
		then
		  echo "Manual pushing to this repo's master branch is restricted"
		  exit 1
		fi

[root@ststor01 ~]# cp /tmp/update /opt/beta.git/hooks/

[root@ststor01 ~]# cat /opt/beta.git/hooks/update
		#!/bin/sh
		if [ "$1" == refs/heads/master ];
		then
		  echo "Manual pushing to this repo's master branch is restricted"
		  exit 1
		fi

6. Clone the repo under given directory location

[root@ststor01 ~]# cd /usr/src/kodekloudrepos

[root@ststor01 kodekloudrepos]# git clone /opt/beta.git
		Cloning into 'beta'...
		warning: You appear to have cloned an empty repository.
		done.
[root@ststor01 kodekloudrepos]# ls -ahl
		total 12K
		drwxr-xr-x 3 root root 4.0K Dec 21 04:07 .
		drwxr-xr-x 1 root root 4.0K Dec 21 04:02 ..
		drwxr-xr-x 3 root root 4.0K Dec 21 04:07 beta

[root@ststor01 kodekloudrepos]# cd beta/

[root@ststor01 beta]# ls -ahl
		total 12K
		drwxr-xr-x 3 root root 4.0K Dec 21 04:07 .
		drwxr-xr-x 3 root root 4.0K Dec 21 04:07 ..
		drwxr-xr-x 7 root root 4.0K Dec 21 04:07 .git

7. Create new branch

[root@ststor01 beta]# git checkout -b xfusioncorp_beta
		Switched to a new branch 'xfusioncorp_beta'

8. Copy readme.md file

[root@ststor01 beta]# cp /tmp/readme.md ./

[root@ststor01 beta]# ls -ahl
		total 16K
		drwxr-xr-x 3 root root 4.0K Dec 21 04:08 .
		drwxr-xr-x 3 root root 4.0K Dec 21 04:07 ..
		drwxr-xr-x 7 root root 4.0K Dec 21 04:08 .git
		-rw-r--r-- 1 root root   33 Dec 21 04:08 readme.md

9. Commit and push the changes in ementioned branch

[root@ststor01 beta]# git status
		# On branch xfusioncorp_beta
		#
		# Initial commit
		#
		# Untracked files:
		#   (use "git add <file>..." to include in what will be committed)
		#
		#       readme.md
		nothing added to commit but untracked files present (use "git add" to track)

[root@ststor01 beta]# git add readme.md 

[root@ststor01 beta]# git commit -m "Added readme.md files"
		[xfusioncorp_beta (root-commit) 24839e3] Added readme.md files
		 1 file changed, 1 insertion(+)
		 create mode 100644 readme.md

[root@ststor01 beta]# git push origin xfusioncorp_beta
		Counting objects: 3, done.
		Writing objects: 100% (3/3), 256 bytes | 0 bytes/s, done.
		Total 3 (delta 0), reused 0 (delta 0)
		To /opt/beta.git
		 * [new branch]      xfusioncorp_beta -> xfusioncorp_beta

[root@ststor01 beta]# git status
		# On branch xfusioncorp_beta
		nothing to commit, working directory clean

10. Move to master branch and try to push changes (push should fail as per added hook)

[root@ststor01 beta]# git checkout -b master
		Switched to a new branch 'master'

[root@ststor01 beta]# git branch -a
		* master
		  xfusioncorp_beta
		  remotes/origin/xfusioncorp_beta

[root@ststor01 beta]# git push origin master
		Total 0 (delta 0), reused 0 (delta 0)
		remote: Manual pushing to this repo's master branch is restricted
		remote: error: hook declined to update refs/heads/master
		To /opt/beta.git
		 ! [remote rejected] master -> master (hook declined)
		error: failed to push some refs to '/opt/beta.git'
[root@ststor01 beta]#

--------------------------------------------------------------------------------------------------------------------------------
Task 119: 23/Dec/2022

Puppet Install a Package


Some new packages need to be installed on app server 3 in Stratos Datacenter. The Nautilus DevOps team has decided to install the same using Puppet. Since jump host is already configured to run as Puppet master server and all app servers are already configured to work as the puppet agent nodes, we need to create required manifests on the Puppet master server so that the same can be applied on all Puppet agent nodes. Please find more details about the task below.

Create a Puppet programming file apps.pp under /etc/puppetlabs/code/environments/production/manifests directory on master node i.e Jump Server and using puppet package resource perform the tasks given below.

    Install package vsftpd through Puppet package resource only on App server 3i.epuppet agent node 3`.

Notes: :- Please make sure to run the puppet agent test using sudo on agent nodes, otherwise you can face certificate issues. In that case you will have to clean the certificates first and then you will be able to run the puppet agent test.

:- Before clicking on the Check button please make sure to verify puppet server and puppet agent services are up and running on the respective servers, also please make sure to run puppet agent test to apply/test the changes manually first.

:- Please note that once lab is loaded, the puppet server service should start automatically on puppet master server, however it can take upto 2-3 minutes to start.


1. Switch to root user on puppet master / jump host server

thor@jump_host ~$ sudo su -

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for thor: 

2. Go to Puppet folder and create mentioned puppet file as per requirements 

root@jump_host ~# cd /etc/puppetlabs/code/environments/production/manifests

root@jump_host /etc/puppetlabs/code/environments/production/manifests# ls -ahl
		total 8.0K
		drwxr-xr-x 1 puppet puppet 4.0K Jul 13  2021 .
		drwxr-xr-x 1 puppet puppet 4.0K Aug  9  2021 ..

root@jump_host /etc/puppetlabs/code/environments/production/manifests# vi apps.pp

root@jump_host /etc/puppetlabs/code/environments/production/manifests# cat apps.pp 
		class vsftpd_installer {
		    package {'vsftpd':
		        ensure => installed
		    }
		}

		node 'stapp03.stratos.xfusioncorp.com' {
		  include vsftpd_installer
		}

3. Validate the puppet file created  

root@jump_host /etc/puppetlabs/code/environments/production/manifests# puppet parser validate apps.pp 

4. Login to App Server 3 and switch to root 

root@jump_host /etc/puppetlabs/code/environments/production/manifests# ssh banner@stapp03
		The authenticity of host 'stapp03 (172.16.238.12)' can't be established.
		ECDSA key fingerprint is SHA256:FH25URE1iUQRs9xBcOo91DcHsaLqxniAcWvLCFxadwk.
		ECDSA key fingerprint is MD5:6d:62:95:a8:cb:d3:73:41:13:f0:99:1b:4a:7b:ec:64.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'stapp03,172.16.238.12' (ECDSA) to the list of known hosts.
		banner@stapp03's password: 

[banner@stapp03 ~]$ sudo su -

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for banner: 

[root@stapp03 ~]# rpm -qa |grep vsftpd

5. Run Puppet agent to pull the configuration from puppet server

[root@stapp03 ~]# puppet agent -tv
		Info: Using configured environment 'production'
		Info: Retrieving pluginfacts
		Info: Retrieving plugin
		Info: Retrieving locales
		Info: Caching catalog for stapp03.stratos.xfusioncorp.com
		Info: Applying configuration version '1671769126'
		Notice: Applied catalog in 0.12 seconds

6. Validate the task by checking installed package

[root@stapp03 ~]# rpm -qa |grep vsftpd
		vsftpd-3.0.2-29.el7_9.x86_64

[root@stapp03 ~]# 

--------------------------------------------------------------------------------------------------------------------------------
Task 120: 24/Dec/2022

Fix Issue with VolumeMounts in Kubernetes

We deployed a Nginx and PHPFPM based setup on Kubernetes cluster last week and it had been working fine. This morning one of the team members made a change somewhere which caused some issues, and it stopped working. Please look into the issue and fix it:

The pod name is nginx-phpfpm and configmap name is nginx-config. Figure out the issue and fix the same.

Once issue is fixed, copy /home/thor/index.php file from jump host into nginx-container under nginx document root and you should be able to access the website using Website button on top bar.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

1. Check existing running pods

thor@jump_host ~$ kubectl get pods
		NAME           READY   STATUS    RESTARTS   AGE
		nginx-phpfpm   2/2     Running   0          87s

2. Check existing running configmap for shared volume map

thor@jump_host ~$ kubectl get configmap
		NAME               DATA   AGE
		kube-root-ca.crt   1      26m
		nginx-config       1      96s

thor@jump_host ~$ kubectl describe configmap nginx-config
		Name:         nginx-config
		Namespace:    default
		Labels:       <none>
		Annotations:  <none>

		Data
		====
		nginx.conf:
		----
		events {
		}
		http {
		  server {
		    listen 8099 default_server;
		    listen [::]:8099 default_server;

		    # Set nginx to serve files from the shared volume!
		    root /var/www/html;                                                               <--------------------------------------
		    index  index.html index.htm index.php;
		    server_name _;
		    location / {
		      try_files $uri $uri/ =404;
		    }
		    location ~ \.php$ {
		      include fastcgi_params;
		      fastcgi_param REQUEST_METHOD $request_method;
		      fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
		      fastcgi_pass 127.0.0.1:9000;
		    }
		  }
		}

		Events:  <none>

3. Get running pod configuration into a YAML file and look for mountPath value

thor@jump_host ~$ kubectl get pod nginx-phpfpm -o yaml  > /tmp/nginx.yaml

thor@jump_host ~$ cat /tmp/nginx.yaml |grep mountPath
      {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"labels":{"app":"php-app"},"name":"nginx-phpfpm","namespace":"default"},"spec":{"containers":[{"image":"php:7.2-fpm","name":"php-fpm-container","volumeMounts":[{"mountPath":"/var/www/html","name":"shared-files"}]},{"image":"nginx:latest","name":"nginx-container","volumeMounts":[{"mountPath":"/usr/share/nginx/html","name":"shared-files"},{"mountPath":"/etc/nginx/nginx.conf","name":"nginx-config-volume","subPath":"nginx.conf"}]}],"volumes":[{"emptyDir":{},"name":"shared-files"},{"configMap":{"name":"nginx-config"},"name":"nginx-config-volume"}]}}
              k:{"mountPath":"/etc/nginx/nginx.conf"}:
                f:mountPath: {}
              k:{"mountPath":"/usr/share/nginx/html"}:
                f:mountPath: {}
              k:{"mountPath":"/var/www/html"}:
                f:mountPath: {}
    - mountPath: /var/www/html
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
    - mountPath: /usr/share/nginx/html
    - mountPath: /etc/nginx/nginx.conf
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount

4. Edit the nginx.yaml file and change mountPath value from ‘/usr/share/nginx/html’ to ‘/var/www/html’ 

thor@jump_host ~$ vi /tmp/nginx.yaml 

thor@jump_host ~$ cat /tmp/nginx.yaml |grep mountPath
      {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"labels":{"app":"php-app"},"name":"nginx-phpfpm","namespace":"default"},"spec":{"containers":[{"image":"php:7.2-fpm","name":"php-fpm-container","volumeMounts":[{"mountPath":"/var/www/html","name":"shared-files"}]},{"image":"nginx:latest","name":"nginx-container","volumeMounts":[{"mountPath":"/var/www/html","name":"shared-files"},{"mountPath":"/etc/nginx/nginx.conf","name":"nginx-config-volume","subPath":"nginx.conf"}]}],"volumes":[{"emptyDir":{},"name":"shared-files"},{"configMap":{"name":"nginx-config"},"name":"nginx-config-volume"}]}}
              k:{"mountPath":"/etc/nginx/nginx.conf"}:
                f:mountPath: {}
              k:{"mountPath":"/var/www/html"}:
                f:mountPath: {}
              k:{"mountPath":"/var/www/html"}:
                f:mountPath: {}
    - mountPath: /var/www/html
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
    - mountPath: /var/www/html
    - mountPath: /etc/nginx/nginx.conf
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount

5. Replace the running pods with new edited/changed config

thor@jump_host ~$ kubectl replace -f /tmp/nginx.yaml --force
		pod "nginx-phpfpm" deleted
		pod/nginx-phpfpm replaced

thor@jump_host ~$ kubectl get pods
		NAME           READY   STATUS    RESTARTS   AGE
		nginx-phpfpm   2/2     Running   0          10s

6. Copy the mentioned index.php file into pods

thor@jump_host ~$ kubectl cp  /home/thor/index.php  nginx-phpfpm:/var/www/html -c nginx-container

7. Validate the the task by curl on running nginx instance / check Website as mentioned in task

thor@jump_host ~$ kubectl exec -it nginx-phpfpm -c nginx-container  -- curl -I  http://localhost:8099
		HTTP/1.1 200 OK
		Server: nginx/1.23.3
		Date: Sat, 24 Dec 2022 13:34:46 GMT
		Content-Type: text/html; charset=UTF-8
		Connection: keep-alive
		X-Powered-By: PHP/7.2.34

thor@jump_host ~$

--------------------------------------------------------------------------------------------------------------------------------
Task 121: 25/Dec/2022

Create Pods in Kubernetes Cluster

The Nautilus DevOps team has started practicing some pods and services deployment on Kubernetes platform as they are planning to migrate most of their applications on Kubernetes platform. Recently one of the team members has been assigned a task to create a pod as per details mentioned below:

    Create a pod named pod-httpd using httpd image with latest tag only and remember to mention the tag i.e httpd:latest.

    Labels app should be set to httpd_app, also container should be named as httpd-container.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.


1. Check kubectl utility config

thor@jump_host ~$ kubectl get all
		NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
		service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   25m

thor@jump_host ~$ kubectl get services
		NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
		kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   25m

thor@jump_host ~$ kubectl get namespaces
		NAME                 STATUS   AGE
		default              Active   25m
		kube-node-lease      Active   25m
		kube-public          Active   25m
		kube-system          Active   25m
		local-path-storage   Active   25m

thor@jump_host ~$ kubectl get pods
	No resources found in default namespace.

2. Create YAML file as per requirements

thor@jump_host ~$ vi /tmp/pods.yml

thor@jump_host ~$ cat /tmp/pods.yml
		apiVersion: v1
		kind: Pod
		metadata:
		    name: pod-httpd
		    labels:
		      app: httpd_app
		spec:
		    containers:
		    - name: httpd-container
		      image: httpd:latest

3. Create pods by using the yaml file created

thor@jump_host ~$ kubectl create -f /tmp/pods.yml 
		pod/pod-httpd created

4. Check the status and wait for pods to get running

thor@jump_host ~$ kubectl get all
		NAME            READY   STATUS              RESTARTS   AGE
		pod/pod-httpd   0/1     ContainerCreating   0          8s

		NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
		service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   27m

thor@jump_host ~$ kubectl get pods
		NAME        READY   STATUS    RESTARTS   AGE
		pod-httpd   1/1     Running   0          15s

thor@jump_host ~$ kubectl get pods -o wide
		NAME        READY   STATUS    RESTARTS   AGE   IP           NODE                      NOMINATED NODE   READINESS GATES
		pod-httpd   1/1     Running   0          22s   10.244.0.5   kodekloud-control-plane   <none>           <none>
thor@jump_host ~$

--------------------------------------------------------------------------------------------------------------------------------
Task 122: 26/Dec/2022

Git Repository Update

The Nautilus development team started with new project development. They have created different Git repositories to manage respective project's source code. One of the repo /opt/blog.git was created recently. The team has given us a sample index.html file that is currently present on jump host under /tmp. The repository has been cloned at /usr/src/kodekloudrepos on storage server in Stratos DC.

Copy sample index.html file from jump host to storage server under cloned repository at /usr/src/kodekloudrepos, add/commit the file and push to master branch.


1. Copy/SCP the index.html file from jump host to storage server

thor@jump_host ~$ sudo scp /tmp/index.html natasha@ststor01:/tmp/

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for thor: 

		The authenticity of host 'ststor01 (172.16.238.15)' can't be established.
		ECDSA key fingerprint is SHA256:NTUGOk5/tEHb4NSstyQtMlCVIii+1RuDwMw14+CgndM.
		ECDSA key fingerprint is MD5:a7:27:52:36:c5:db:6f:16:47:31:ed:2f:10:6c:b7:70.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'ststor01,172.16.238.15' (ECDSA) to the list of known hosts.
		natasha@ststor01's password: 
		
		index.html                                                                                                 100%   27    82.1KB/s   00:00    

2. Login/SSH to storage server and switch to root user 

thor@jump_host ~$ ssh natasha@ststor01
		The authenticity of host 'ststor01 (172.16.238.15)' can't be established.
		ECDSA key fingerprint is SHA256:NTUGOk5/tEHb4NSstyQtMlCVIii+1RuDwMw14+CgndM.
		ECDSA key fingerprint is MD5:a7:27:52:36:c5:db:6f:16:47:31:ed:2f:10:6c:b7:70.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'ststor01,172.16.238.15' (ECDSA) to the list of known hosts.
		natasha@ststor01's password: 

[natasha@ststor01 ~]$ sudo su -

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for natasha: 

3. Move to local git repo and copy the index.html file 

[root@ststor01 ~]# cd /usr/src/kodekloudrepos

[root@ststor01 kodekloudrepos]# ls -ahl
		total 12K
		drwxr-xr-x 3 root root 4.0K Dec 26 16:22 .
		drwxr-xr-x 1 root root 4.0K Dec 26 16:22 ..
		drwxr-xr-x 3 root root 4.0K Dec 26 16:22 blog

[root@ststor01 kodekloudrepos]# cd blog/

[root@ststor01 blog]# ls -ahl
		total 20K
		drwxr-xr-x 3 root root 4.0K Dec 26 16:22 .
		drwxr-xr-x 3 root root 4.0K Dec 26 16:22 ..
		drwxr-xr-x 8 root root 4.0K Dec 26 16:22 .git
		-rw-r--r-- 1 root root   34 Dec 26 16:22 info.txt
		-rw-r--r-- 1 root root   26 Dec 26 16:22 welcome.txt

[root@ststor01 blog]# git status
		# On branch master
		nothing to commit, working directory clean

[root@ststor01 blog]# cp /tmp/index.html ./

[root@ststor01 blog]# ls -ahl
		total 24K
		drwxr-xr-x 3 root root 4.0K Dec 26 16:25 .
		drwxr-xr-x 3 root root 4.0K Dec 26 16:22 ..
		drwxr-xr-x 8 root root 4.0K Dec 26 16:25 .git
		-rw-r--r-- 1 root root   27 Dec 26 16:25 index.html
		-rw-r--r-- 1 root root   34 Dec 26 16:22 info.txt
		-rw-r--r-- 1 root root   26 Dec 26 16:22 welcome.txt

4. Add and commit the index.html file

[root@ststor01 blog]# git status
		# On branch master
		# Untracked files:
		#   (use "git add <file>..." to include in what will be committed)
		#
		#       index.html
		nothing added to commit but untracked files present (use "git add" to track)

[root@ststor01 blog]# git add index.html 

[root@ststor01 blog]# git commit -m "Added index.html"
		[master 0b92f5c] Added index.html
		 1 file changed, 1 insertion(+)
		 create mode 100644 index.html

5. Push the master branch to origin

[root@ststor01 blog]# git push -u origin master
		Counting objects: 4, done.
		Delta compression using up to 36 threads.
		Compressing objects: 100% (2/2), done.
		Writing objects: 100% (3/3), 332 bytes | 0 bytes/s, done.
		Total 3 (delta 0), reused 0 (delta 0)
		To /opt/blog.git
		   f9b5024..0b92f5c  master -> master
		Branch master set up to track remote branch master from origin.

[root@ststor01 blog]# git status
		# On branch master
		nothing to commit, working directory clean

[root@ststor01 blog]# git remote -v
		origin  /opt/blog.git (fetch)
		origin  /opt/blog.git (push)

[root@ststor01 blog]#

--------------------------------------------------------------------------------------------------------------------------------
Task 123: 28/Dec/2022

Ansible Unarchive Module

One of the DevOps team members has created an ZIP archive on jump host in Stratos DC that needs to be extracted and copied over to all app servers in Stratos DC itself. Because this is a routine task, the Nautilus DevOps team has suggested automating it. We can use Ansible since we have been using it for other automation tasks. Below you can find more details about the task:

We have an inventory file under /home/thor/ansible directory on jump host, which should have all the app servers added already.

There is a ZIP archive /usr/src/devops/xfusion.zip on jump host.

Create a playbook.yml under /home/thor/ansible/ directory on jump host itself to perform the below given tasks.

    Unzip /usr/src/devops/xfusion.zip archive in /opt/devops/ location on all app servers.

    Make sure the extracted data must has the respective sudo user as their user and group owner, i.e tony for app server 1, steve for app server 2, banner for app server 3.

    The extracted data permissions must be 0755

Note: Validation will try to run the playbook using command ansible-playbook -i inventory playbook.yml so please make sure playbook works this way, without passing any extra arguments.


1. Check current config and inventory file on jump host

thor@jump_host ~$ cd /home/thor/ansible/

thor@jump_host ~/ansible$ ls -ahl
		total 16K
		drwxr-xr-x 2 thor thor 4.0K Dec 28 03:24 .
		drwxr----- 1 thor thor 4.0K Dec 28 03:24 ..
		-rw-r--r-- 1 thor thor   36 Dec 28 03:24 ansible.cfg
		-rw-r--r-- 1 thor thor  237 Dec 28 03:24 inventory

thor@jump_host ~/ansible$ cat inventory 
		stapp01 ansible_host=172.16.238.10 ansible_ssh_pass=Ir0nM@n ansible_user=tony
		stapp02 ansible_host=172.16.238.11 ansible_ssh_pass=Am3ric@ ansible_user=steve
		stapp03 ansible_host=172.16.238.12 ansible_ssh_pass=BigGr33n ansible_user=banner

thor@jump_host ~/ansible$ cat ansible.cfg 
		[defaults]
		host_key_checking = False

2. Check existing files on app servers as well as jump host and verify inventory file 

thor@jump_host ~/ansible$ ansible -i inventory all -a "ls -ahl /opt/devops/"
		stapp02 | CHANGED | rc=0 >>
		total 8.0K
		drwxr-xr-x 2 root root 4.0K Dec 28 03:26 .
		drwxr-xr-x 1 root root 4.0K Dec 28 03:26 ..
		stapp03 | CHANGED | rc=0 >>
		total 8.0K
		drwxr-xr-x 2 root root 4.0K Dec 28 03:26 .
		drwxr-xr-x 1 root root 4.0K Dec 28 03:26 ..
		stapp01 | CHANGED | rc=0 >>
		total 8.0K
		drwxr-xr-x 2 root root 4.0K Dec 28 03:26 .
		drwxr-xr-x 1 root root 4.0K Dec 28 03:26 ..

thor@jump_host ~/ansible$ ls -ahl /usr/src/devops
		total 12K
		drwxr-xr-x 2 root root 4.0K Dec 28 03:26 .
		drwxr-xr-x 1 root root 4.0K Dec 28 03:26 ..
		-rw-r--r-- 1 root root  367 Dec 28 03:26 xfusion.zip

3. Create playbook as per requirements 

thor@jump_host ~/ansible$ vi playbook.yml

thor@jump_host ~/ansible$ cat playbook.yml
		- name: Extract archive
		  hosts: stapp01, stapp02, stapp03
		  become: yes
		  tasks:
		    - name: Extract the archive and set the owner/permissions
		      unarchive:
		        src: /usr/src/devops/xfusion.zip
		        dest: /opt/devops/
		        owner: "{{ ansible_user }}"
		        group: "{{ ansible_user }}"
		        mode: "0755"

4. Run the playbook
thor@jump_host ~/ansible$ ansible-playbook -i inventory playbook.yml 

		PLAY [Extract archive] **********************************************************************************************************************

		TASK [Gathering Facts] **********************************************************************************************************************
		ok: [stapp02]
		ok: [stapp03]
		ok: [stapp01]

		TASK [Extract the archive and set the owner/permissions] ************************************************************************************
		changed: [stapp02]
		changed: [stapp01]
		changed: [stapp03]

		PLAY RECAP **********************************************************************************************************************************
		stapp01                    : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
		stapp02                    : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
		stapp03                    : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

5. Validate the task by checking files on app servers

thor@jump_host ~/ansible$ ansible -i inventory all -a "ls -ahl /opt/devops/"
		stapp03 | CHANGED | rc=0 >>
		total 12K
		drwxr-xr-x 3 root   root   4.0K Dec 28 03:31 .
		drwxr-xr-x 1 root   root   4.0K Dec 28 03:26 ..
		drwxr-xr-x 2 banner banner 4.0K Dec 28 03:26 unarchive
		stapp01 | CHANGED | rc=0 >>
		total 12K
		drwxr-xr-x 3 root root 4.0K Dec 28 03:31 .
		drwxr-xr-x 1 root root 4.0K Dec 28 03:26 ..
		drwxr-xr-x 2 tony tony 4.0K Dec 28 03:26 unarchive
		stapp02 | CHANGED | rc=0 >>
		total 12K
		drwxr-xr-x 3 root  root  4.0K Dec 28 03:31 .
		drwxr-xr-x 1 root  root  4.0K Dec 28 03:26 ..
		drwxr-xr-x 2 steve steve 4.0K Dec 28 03:26 unarchive

--------------------------------------------------------------------------------------------------------------------------------
Task 124: 29/Dec/2022

Update an Existing Deployment in Kubernetes

There is an application deployed on Kubernetes cluster. Recently, the Nautilus application development team developed a new version of the application that needs to be deployed now. As per new updates some new changes need to be made in this existing setup. So update the deployment and service as per details mentioned below:

We already have a deployment named nginx-deployment and service named nginx-service. Some changes need to be made in this deployment and service, make sure not to delete the deployment and service.

1.) Change the service nodeport from 30008 to 32165

2.) Change the replicas count from 1 to 5

3.) Change the image from nginx:1.17 to nginx:latest

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

1. Check current runnning / existing setup

thor@jump_host ~$ kubectl get all
		NAME                                    READY   STATUS    RESTARTS   AGE
		pod/nginx-deployment-576b8f48fb-9prm8   1/1     Running   0          24s

		NAME                    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
		service/kubernetes      ClusterIP   10.96.0.1       <none>        443/TCP        92m
		service/nginx-service   NodePort    10.96.249.254   <none>        80:30008/TCP   25s

		NAME                               READY   UP-TO-DATE   AVAILABLE   AGE
		deployment.apps/nginx-deployment   1/1     1            1           25s

		NAME                                          DESIRED   CURRENT   READY   AGE
		replicaset.apps/nginx-deployment-576b8f48fb   1         1         1       25s

thor@jump_host ~$ kubectl get services 
		NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
		kubernetes      ClusterIP   10.96.0.1       <none>        443/TCP        92m
		nginx-service   NodePort    10.96.249.254   <none>        80:30008/TCP   39s

thor@jump_host ~$ kubectl get deployment
		NAME               READY   UP-TO-DATE   AVAILABLE   AGE
		nginx-deployment   1/1     1            1           49s

2. Edit the service to change the node port 

thor@jump_host ~$ kubectl edit service nginx-service
		service/nginx-service edited

thor@jump_host ~$ kubectl get services 
NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
		kubernetes      ClusterIP   10.96.0.1       <none>        443/TCP        94m
		nginx-service   NodePort    10.96.249.254   <none>        80:32165/TCP   2m21s

3. Edit the deployment to change number of replicas and container image 

thor@jump_host ~$ kubectl get deployment
		NAME               READY   UP-TO-DATE   AVAILABLE   AGE
		nginx-deployment   1/1     1            1           2m33s

thor@jump_host ~$ kubectl edit deployment nginx-deployment
		deployment.apps/nginx-deployment edited

4. Wait for all pods to get to running status and deployment get available

thor@jump_host ~$ kubectl get deployment 
		NAME               READY   UP-TO-DATE   AVAILABLE   AGE
		nginx-deployment   4/5     3            4           4m1s

thor@jump_host ~$ kubectl get pods
		NAME                                READY   STATUS              RESTARTS   AGE
		nginx-deployment-576b8f48fb-2m4j2   0/1     Terminating         0          24s
		nginx-deployment-576b8f48fb-9prm8   1/1     Running             0          4m11s
		nginx-deployment-576b8f48fb-f8gl7   1/1     Terminating         0          24s
		nginx-deployment-576b8f48fb-nbxdz   1/1     Terminating         0          24s
		nginx-deployment-c7ff8fb84-2btjm    0/1     ContainerCreating   0          6s
		nginx-deployment-c7ff8fb84-62wqb    1/1     Running             0          24s
		nginx-deployment-c7ff8fb84-n4t6s    0/1     ContainerCreating   0          5s
		nginx-deployment-c7ff8fb84-vwmps    1/1     Running             0          24s
		nginx-deployment-c7ff8fb84-zjbjf    1/1     Running             0          23s

thor@jump_host ~$ kubectl get pods
		NAME                                READY   STATUS        RESTARTS   AGE
		nginx-deployment-576b8f48fb-9prm8   0/1     Terminating   0          4m26s
		nginx-deployment-c7ff8fb84-2btjm    1/1     Running       0          21s
		nginx-deployment-c7ff8fb84-62wqb    1/1     Running       0          39s
		nginx-deployment-c7ff8fb84-n4t6s    1/1     Running       0          20s
		nginx-deployment-c7ff8fb84-vwmps    1/1     Running       0          39s
		nginx-deployment-c7ff8fb84-zjbjf    1/1     Running       0          38s

thor@jump_host ~$ kubectl get pods
		NAME                               READY   STATUS    RESTARTS   AGE
		nginx-deployment-c7ff8fb84-2btjm   1/1     Running   0          27s
		nginx-deployment-c7ff8fb84-62wqb   1/1     Running   0          45s
		nginx-deployment-c7ff8fb84-n4t6s   1/1     Running   0          26s
		nginx-deployment-c7ff8fb84-vwmps   1/1     Running   0          45s
		nginx-deployment-c7ff8fb84-zjbjf   1/1     Running   0          44s

thor@jump_host ~$ kubectl get deployment 
		NAME               READY   UP-TO-DATE   AVAILABLE   AGE
		nginx-deployment   5/5     5            5           4m37s
thor@jump_host ~$

--------------------------------------------------------------------------------------------------------------------------------
Task 125: 30/Dec/2022

Puppet Setup File Permissions

The Nautilus DevOps team has put some data on all app servers in Stratos DC. jump host is configured as Puppet master server, and all app servers are already been configured as Puppet agent nodes. The team needs to update the content of some of the exiting files, as well as need to update their permissions etc. Please find below more details about the task:

Create a Puppet programming file cluster.pp under /etc/puppetlabs/code/environments/production/manifests directory on the master node i.e Jump Server. Using puppet file resource, perform the below mentioned tasks.

    A file named beta.txt already exists under /opt/devops directory on App Server 2.

    Add content Welcome to xFusionCorp Industries! in beta.txt file on App Server 2.

    Set its permissions to 0777.

Notes: :- Please make sure to run the puppet agent test using sudo on agent nodes, otherwise you can face certificate issues. In that case you will have to clean the certificates first and then you will be able to run the puppet agent test.

:- Before clicking on the Check button please make sure to verify puppet server and puppet agent services are up and running on the respective servers, also please make sure to run puppet agent test to apply/test the changes manually first.

:- Please note that once lab is loaded, the puppet server service should start automatically on puppet master server, however it can take upto 2-3 minutes to start.


1. Switch to root on jump host/ puppet master server

thor@jump_host ~$ sudo su -

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for thor: 

2. Create puppet programming file as per requirements

root@jump_host ~# cd /etc/puppetlabs/code/environments/production/manifests

root@jump_host /etc/puppetlabs/code/environments/production/manifests# ls -ahl
		total 8.0K
		drwxr-xr-x 1 puppet puppet 4.0K Jul 13  2021 .
		drwxr-xr-x 1 puppet puppet 4.0K Aug  9  2021 ..

root@jump_host /etc/puppetlabs/code/environments/production/manifests# vi cluster.pp

root@jump_host /etc/puppetlabs/code/environments/production/manifests# cat cluster.pp 
		class file_updater {
		 # Update beta.txt under /opt/devops
		 file { '/opt/devops/beta.txt':
		   ensure => 'present',
		   content => 'Welcome to xFusionCorp Industries!',
		   mode => '0777',
		 }
		}

		node 'stapp02.stratos.xfusioncorp.com' {
		 include file_updater
		}

3. Parse the cluster.pp file created

root@jump_host /etc/puppetlabs/code/environments/production/manifests# puppet parser validate cluster.pp 

4. Login to app server 2 and switch to root user

root@jump_host /etc/puppetlabs/code/environments/production/manifests# ssh steve@stapp02
		The authenticity of host 'stapp02 (172.16.238.11)' can't be established.
		ECDSA key fingerprint is SHA256:y+jxiyrcebtFq5E1tWUCPBvqr4JIpQzsMbyOE6GTxE4.
		ECDSA key fingerprint is MD5:b4:dd:d5:ca:7d:77:7c:be:6d:ec:33:e6:15:73:9a:0c.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'stapp02,172.16.238.11' (ECDSA) to the list of known hosts.
		steve@stapp02's password: 

[steve@stapp02 ~]$ sudo su -

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for steve: 

5. Run puppet agent to pull the changes  

[root@stapp02 ~]# puppet agent -tv
		Info: Using configured environment 'production'
		Info: Retrieving pluginfacts
		Info: Retrieving plugin
		Info: Retrieving locales
		Info: Caching catalog for stapp02.stratos.xfusioncorp.com
		Info: Applying configuration version '1672396253'
		Notice: Applied catalog in 0.05 seconds

6. Validate the task by checing file content and permissions

[root@stapp02 ~]# cat /opt/devops/beta.txt 
		Welcome to xFusionCorp Industries!

[root@stapp02 ~]# ls -ahl /opt/devops/
		total 12K
		drwxr-xr-x 2 root root 4.0K Dec 30 10:30 .
		drwxr-xr-x 1 root root 4.0K Dec 30 10:26 ..
		-rwxrwxrwx 1 root root   34 Dec 30 10:30 beta.txt
[root@stapp02 ~]#

--------------------------------------------------------------------------------------------------------------------------------
Task 126: 31/Dec/2022

Fix Python App Deployed on Kubernetes Cluster


One of the DevOps engineers was trying to deploy a python app on Kubernetes cluster. Unfortunately, due to some mis-configuration, the application is not coming up. Please take a look into it and fix the issues. Application should be accessible on the specified nodePort.

The deployment name is python-deployment-nautilus, its using poroko/flask-demo-appimage. The deployment and service of this app is already deployed.

    nodePort should be 32345 and targetPort should be python flask app's default port.

Note: The kubectl on jump_host has been configured to work with the kubernetes cluster.


1. Get current running status and configuration

thor@jump_host ~$ kubectl get all
		NAME                                              READY   STATUS             RESTARTS   AGE
		pod/python-deployment-nautilus-5dbcd5f8b4-dflt7   0/1     ImagePullBackOff   0          28s

		NAME                              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
		service/kubernetes                ClusterIP   10.96.0.1       <none>        443/TCP          71m
		service/python-service-nautilus   NodePort    10.96.200.222   <none>        8080:32345/TCP   28s

		NAME                                         READY   UP-TO-DATE   AVAILABLE   AGE
		deployment.apps/python-deployment-nautilus   0/1     1            0           28s

		NAME                                                    DESIRED   CURRENT   READY   AGE
		replicaset.apps/python-deployment-nautilus-5dbcd5f8b4   1         1         0       28s

2. Check the pod logs to know the error

thor@jump_host ~$ kubectl logs python-deployment-nautilus-5dbcd5f8b4-dflt7
		Error from server (BadRequest): container "python-container-nautilus" in pod "python-deployment-nautilus-5dbcd5f8b4-dflt7" is waiting to start: trying and failing to pull image

3. The error is that its uable to pull image, so lets check the deploy configured

thor@jump_host ~$ kubectl get deploy
		NAME                         READY   UP-TO-DATE   AVAILABLE   AGE
		python-deployment-nautilus   0/1     1            0           64s

thor@jump_host ~$ kubectl describe deploy python-deployment-nautilus
			Name:                   python-deployment-nautilus
			Namespace:              default
			CreationTimestamp:      Sat, 31 Dec 2022 14:12:25 +0000
			Labels:                 <none>
			Annotations:            deployment.kubernetes.io/revision: 1
			Selector:               app=python_app
			Replicas:               1 desired | 1 updated | 1 total | 0 available | 1 unavailable
			StrategyType:           RollingUpdate
			MinReadySeconds:        0
			RollingUpdateStrategy:  25% max unavailable, 25% max surge
			Pod Template:
			  Labels:  app=python_app
			  Containers:
			   python-container-nautilus:
			    Image:        poroko/flask-app-demo                                 <--------Wrong name of image
			    Port:         5000/TCP                                              <--------Also image running on port 5000 which is flask default port
			    Host Port:    0/TCP
			    Environment:  <none>
			    Mounts:       <none>
			  Volumes:        <none>
			Conditions:
			  Type           Status  Reason
			  ----           ------  ------
			  Available      False   MinimumReplicasUnavailable
			  Progressing    True    ReplicaSetUpdated
			OldReplicaSets:  <none>
			NewReplicaSet:   python-deployment-nautilus-5dbcd5f8b4 (1/1 replicas created)
			Events:
			  Type    Reason             Age   From                   Message
			  ----    ------             ----  ----                   -------
			  Normal  ScalingReplicaSet  93s   deployment-controller  Scaled up replica set python-deployment-nautilus-5dbcd5f8b4 to 1

4. Edit deployment to update the image name. 

thor@jump_host ~$ kubectl edit deploy python-deployment-nautilus
		deployment.apps/python-deployment-nautilus edited

5. Wait for pod to get to runnning status 

thor@jump_host ~$ kubectl get all
			NAME                                              READY   STATUS              RESTARTS   AGE
			pod/python-deployment-nautilus-5dbcd5f8b4-dflt7   0/1     ImagePullBackOff    0          3m
			pod/python-deployment-nautilus-767b6d4c9b-t4jzx   0/1     ContainerCreating   0          9s

			NAME                              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
			service/kubernetes                ClusterIP   10.96.0.1       <none>        443/TCP          74m
			service/python-service-nautilus   NodePort    10.96.200.222   <none>        8080:32345/TCP   3m

			NAME                                         READY   UP-TO-DATE   AVAILABLE   AGE
			deployment.apps/python-deployment-nautilus   0/1     1            0           3m

			NAME                                                    DESIRED   CURRENT   READY   AGE
			replicaset.apps/python-deployment-nautilus-5dbcd5f8b4   1         1         0       3m
			replicaset.apps/python-deployment-nautilus-767b6d4c9b   1         1         0       9s

thor@jump_host ~$ kubectl get all
		NAME                                              READY   STATUS              RESTARTS   AGE
		pod/python-deployment-nautilus-5dbcd5f8b4-dflt7   0/1     ImagePullBackOff    0          3m7s
		pod/python-deployment-nautilus-767b6d4c9b-t4jzx   0/1     ContainerCreating   0          16s

		NAME                              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
		service/kubernetes                ClusterIP   10.96.0.1       <none>        443/TCP          74m
		service/python-service-nautilus   NodePort    10.96.200.222   <none>        8080:32345/TCP   3m7s

		NAME                                         READY   UP-TO-DATE   AVAILABLE   AGE
		deployment.apps/python-deployment-nautilus   0/1     1            0           3m7s

		NAME                                                    DESIRED   CURRENT   READY   AGE
		replicaset.apps/python-deployment-nautilus-5dbcd5f8b4   1         1         0       3m7s
		replicaset.apps/python-deployment-nautilus-767b6d4c9b   1         1         0       16s

6. Check service configuration as it is running on port 8080 where as it should run on flask default port 5000

thor@jump_host ~$ kubectl get service
NAME                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
kubernetes                ClusterIP   10.96.0.1       <none>        443/TCP          74m
python-service-nautilus   NodePort    10.96.200.222   <none>        8080:32345/TCP   3m19s

thor@jump_host ~$ kubectl describe service python-service-nautilus
		Name:                     python-service-nautilus
		Namespace:                default
		Labels:                   <none>
		Annotations:              <none>
		Selector:                 app=python_app
		Type:                     NodePort
		IP:                       10.96.200.222
		Port:                     <unset>  8080/TCP
		TargetPort:               8080/TCP
		NodePort:                 <unset>  32345/TCP
		Endpoints:                10.244.0.6:8080
		Session Affinity:         None
		External Traffic Policy:  Cluster
		Events:                   <none>

7. Edit serive configuration to update port to 5000

thor@jump_host ~$ kubectl edit service python-service-nautilus
		service/python-service-nautilus edited

thor@jump_host ~$ kubectl get service
		NAME                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
		kubernetes                ClusterIP   10.96.0.1       <none>        443/TCP          76m
		python-service-nautilus   NodePort    10.96.200.222   <none>        5000:32345/TCP   4m55s

8. Validate the task by checking updated config and logs of pod. Also run Application from top right corner

thor@jump_host ~$ kubectl get all
		NAME                                              READY   STATUS    RESTARTS   AGE
		pod/python-deployment-nautilus-767b6d4c9b-t4jzx   1/1     Running   0          2m11s

		NAME                              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
		service/kubernetes                ClusterIP   10.96.0.1       <none>        443/TCP          76m
		service/python-service-nautilus   NodePort    10.96.200.222   <none>        5000:32345/TCP   5m2s

		NAME                                         READY   UP-TO-DATE   AVAILABLE   AGE
		deployment.apps/python-deployment-nautilus   1/1     1            1           5m2s

		NAME                                                    DESIRED   CURRENT   READY   AGE
		replicaset.apps/python-deployment-nautilus-5dbcd5f8b4   0         0         0       5m2s
		replicaset.apps/python-deployment-nautilus-767b6d4c9b   1         1         1       2m11s

thor@jump_host ~$ kubectl logs python-deployment-nautilus-767b6d4c9b-t4jzx
		 * Serving Flask app "app.py"
		 * Environment: production
		   WARNING: Do not use the development server in a production environment.
		   Use a production WSGI server instead.
		 * Debug mode: off
		 * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)
thor@jump_host ~$

--------------------------------------------------------------------------------------------------------------------------------
Task 127: 02/Jan/2023

Fix issue with PhpFpm Application Deployed on Kubernetes

We deployed a Nginx and PHPFPM based application on Kubernetes cluster last week and it had been working fine. This morning one of the team members was troubleshooting an issue with this stack and he was supposed to run Nginx welcome page for now on this stack till issue with phpfpm is fixed but he made a change somewhere which caused some issue and the application stopped working. Please look into the issue and fix the same:

The deployment name is nginx-phpfpm-dp and service name is nginx-service. Figure out the issues and fix them. FYI Nginx is configured to use default http port, node port is 30008 and copy index.php under /tmp/index.php to deployment under /var/www/html. Please do not try to delete/modify any other existing components like deployment name, service name etc.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

1. Get current running status and config

thor@jump_host ~$ kubectl get all
			NAME                                   READY   STATUS    RESTARTS   AGE
			pod/nginx-phpfpm-dp-5cccd45499-52sfq   2/2     Running   0          72s

			NAME                    TYPE           CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE
			service/kubernetes      ClusterIP      10.96.0.1     <none>        443/TCP          43m
			service/nginx-service   LoadBalancer   10.96.21.86   <pending>     8094:30008/TCP   72s

			NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
			deployment.apps/nginx-phpfpm-dp   1/1     1            1           72s

			NAME                                         DESIRED   CURRENT   READY   AGE
			replicaset.apps/nginx-phpfpm-dp-5cccd45499   1         1         1       72s

thor@jump_host ~$ kubectl get deploy
		NAME              READY   UP-TO-DATE   AVAILABLE   AGE
		nginx-phpfpm-dp   1/1     1            1           83s

thor@jump_host ~$ kubectl describe deploy nginx-phpfpm-dp
			Name:               nginx-phpfpm-dp
			Namespace:          default
			CreationTimestamp:  Mon, 02 Jan 2023 07:57:43 +0000
			Labels:             app=nginx-fpm
			Annotations:        deployment.kubernetes.io/revision: 1
			Selector:           app=nginx-fpm,tier=frontend
			Replicas:           1 desired | 1 updated | 1 total | 1 available | 0 unavailable
			StrategyType:       Recreate
			MinReadySeconds:    0
			Pod Template:
			  Labels:  app=nginx-fpm
			           tier=frontend
			  Containers:
			   nginx-container:
			    Image:        nginx:latest
			    Port:         <none>
			    Host Port:    <none>
			    Environment:  <none>
			    Mounts:
			      /etc/nginx/nginx.conf from nginx-config-volume (rw,path="nginx.conf")
			      /var/www/html from shared-files (rw)
			   php-fpm-container:
			    Image:        php:7.3-fpm
			    Port:         <none>
			    Host Port:    <none>
			    Environment:  <none>
			    Mounts:
			      /var/www/html from shared-files (rw)
			  Volumes:
			   nginx-persistent-storage:
			    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
			    ClaimName:  nginx-pv-claim
			    ReadOnly:   false
			   shared-files:
			    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
			    Medium:     
			    SizeLimit:  <unset>
			   nginx-config-volume:
			    Type:      ConfigMap (a volume populated by a ConfigMap)
			    Name:      nginx-config
			    Optional:  false
			Conditions:
			  Type           Status  Reason
			  ----           ------  ------
			  Available      True    MinimumReplicasAvailable
			  Progressing    True    NewReplicaSetAvailable
			OldReplicaSets:  nginx-phpfpm-dp-5cccd45499 (1/1 replicas created)
			NewReplicaSet:   <none>
			Events:
			  Type    Reason             Age   From                   Message
			  ----    ------             ----  ----                   -------
			  Normal  ScalingReplicaSet  104s  deployment-controller  Scaled up replica set nginx-phpfpm-dp-5cccd45499 to 1

thor@jump_host ~$ kubectl get pods
		NAME                               READY   STATUS    RESTARTS   AGE
		nginx-phpfpm-dp-5cccd45499-52sfq   2/2     Running   0          2m14s

thor@jump_host ~$ kubectl describe pod nginx-phpfpm-dp-5cccd45499-52sfq
		Name:         nginx-phpfpm-dp-5cccd45499-52sfq
		Namespace:    default
		Priority:     0
		Node:         kodekloud-control-plane/172.17.0.2
		Start Time:   Mon, 02 Jan 2023 07:57:44 +0000
		Labels:       app=nginx-fpm
		              pod-template-hash=5cccd45499
		              tier=frontend
		Annotations:  <none>
		Status:       Running
		IP:           10.244.0.5
		IPs:
		  IP:           10.244.0.5
		Controlled By:  ReplicaSet/nginx-phpfpm-dp-5cccd45499
		Containers:
		  nginx-container:
		    Container ID:   containerd://e1f25836dc6bae575137ffc0a4bc333ee8ee6814f9781d07312c6a076a1aac0b
		    Image:          nginx:latest
		    Image ID:       docker.io/library/nginx@sha256:0047b729188a15da49380d9506d65959cce6d40291ccfb4e039f5dc7efd33286
		    Port:           <none>
		    Host Port:      <none>
		    State:          Running
		      Started:      Mon, 02 Jan 2023 07:57:56 +0000
		    Ready:          True
		    Restart Count:  0
		    Environment:    <none>
		    Mounts:
		      /etc/nginx/nginx.conf from nginx-config-volume (rw,path="nginx.conf")
		      /var/run/secrets/kubernetes.io/serviceaccount from default-token-lx7b4 (ro)
		      /var/www/html from shared-files (rw)
		  php-fpm-container:
		    Container ID:   containerd://949818ca41eff60942027eba6b89522df41a2702e05cab4b19bf7f887604be87
		    Image:          php:7.3-fpm
		    Image ID:       docker.io/library/php@sha256:2d68e401d2d3b9f8a6572791cd7a25062450c43ff52e58391146809741ad0885
		    Port:           <none>
		    Host Port:      <none>
		    State:          Running
		      Started:      Mon, 02 Jan 2023 07:58:25 +0000
		    Ready:          True
		    Restart Count:  0
		    Environment:    <none>
		    Mounts:
		      /var/run/secrets/kubernetes.io/serviceaccount from default-token-lx7b4 (ro)
		      /var/www/html from shared-files (rw)
		Conditions:
		  Type              Status
		  Initialized       True 
		  Ready             True 
		  ContainersReady   True 
		  PodScheduled      True 
		Volumes:
		  nginx-persistent-storage:
		    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
		    ClaimName:  nginx-pv-claim
		    ReadOnly:   false
		  shared-files:
		    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
		    Medium:     
		    SizeLimit:  <unset>
		  nginx-config-volume:
		    Type:      ConfigMap (a volume populated by a ConfigMap)
		    Name:      nginx-config
		    Optional:  false
		  default-token-lx7b4:
		    Type:        Secret (a volume populated by a Secret)
		    SecretName:  default-token-lx7b4
		    Optional:    false
		QoS Class:       BestEffort
		Node-Selectors:  <none>
		Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
		                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
		Events:
		  Type    Reason     Age    From               Message
		  ----    ------     ----   ----               -------
		  Normal  Scheduled  2m40s  default-scheduler  Successfully assigned default/nginx-phpfpm-dp-5cccd45499-52sfq to kodekloud-control-plane
		  Normal  Pulling    2m40s  kubelet            Pulling image "nginx:latest"
		  Normal  Pulled     2m29s  kubelet            Successfully pulled image "nginx:latest" in 10.902914584s
		  Normal  Created    2m29s  kubelet            Created container nginx-container
		  Normal  Started    2m29s  kubelet            Started container nginx-container
		  Normal  Pulling    2m29s  kubelet            Pulling image "php:7.3-fpm"
		  Normal  Pulled     2m1s   kubelet            Successfully pulled image "php:7.3-fpm" in 28.293999265s
		  Normal  Created    2m     kubelet            Created container php-fpm-container
		  Normal  Started    2m     kubelet            Started container php-fpm-container

2. Check pod logs for error

thor@jump_host ~$ kubectl logs nginx-phpfpm-dp-5cccd45499-52sfq
	error: a container name must be specified for pod nginx-phpfpm-dp-5cccd45499-52sfq, choose one of: [nginx-container php-fpm-container]

thor@jump_host ~$ kubectl logs nginx-phpfpm-dp-5cccd45499-52sfq -c nginx-container
		/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
		/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
		/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
		10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf
		10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf
		/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
		/docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh
		/docker-entrypoint.sh: Configuration complete; ready for start up

thor@jump_host ~$ kubectl logs nginx-phpfpm-dp-5cccd45499-52sfq -c php-fpm-container
		[02-Jan-2023 07:58:25] NOTICE: fpm is running, pid 1
		[02-Jan-2023 07:58:25] NOTICE: ready to handle connections

3. Check the configmap for port used 
 
thor@jump_host ~$ kubectl get configmap
NAME               DATA   AGE
kube-root-ca.crt   1      45m
nginx-config       1      3m47s
thor@jump_host ~$ kubectl describe configmap nginx-config
Name:         nginx-config
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
nginx.conf:
----
events {
}
http {
  server {
    listen 80 default_server;                                                      <----- Default nginx port 80
    listen [::]:80 default_server;

    # Set nginx to serve files from the shared volume!
    root /var/www/html;
    index  index.html index.ph p index.htm;                                         <------- error in index file name
    server_name _;
    location / {
      try_files $uri $uri/ =404;
    }
    location ~ \.php$ {
      include fastcgi_params;
      fastcgi_param REQUEST_METHOD $request_method;
      fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
      fastcgi_pass 127.0.0.1:9000;
    }
  }
}

Events:  <none>

4. Edit the index file name in configmap (2 changes)

thor@jump_host ~$ kubectl edit configmap nginx-config
		configmap/nginx-config edited

thor@jump_host ~$ kubectl describe configmap nginx-config
			Name:         nginx-config
			Namespace:    default
			Labels:       <none>
			Annotations:  <none>

			Data
			====
			nginx.conf:
			----
			events {
			}
			http {
			  server {
			    listen 80 default_server;
			    listen [::]:80 default_server;

			    # Set nginx to serve files from the shared volume!
			    root /var/www/html;
			    index  index.html index.php index.htm;
			    server_name _;
			    location / {
			      try_files $uri $uri/ =404;
			    }
			    location ~ \.php$ {
			      include fastcgi_params;
			      fastcgi_param REQUEST_METHOD $request_method;
			      fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
			      fastcgi_pass 127.0.0.1:9000;
			    }
			  }
			}

			Events:  <none>

5. Check the service			

thor@jump_host ~$ kubectl get service
		NAME            TYPE           CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE
		kubernetes      ClusterIP      10.96.0.1     <none>        443/TCP          47m
		nginx-service   LoadBalancer   10.96.21.86   <pending>     8094:30008/TCP   5m32s                <----Port is different than 80

thor@jump_host ~$ kubectl describe service nginx-service
			Name:                     nginx-service
			Namespace:                default
			Labels:                   app=nginx-fpm
			Annotations:              <none>
			Selector:                 app=nginx-fpm,tier=frontend
			Type:                     LoadBalancer
			IP:                       10.96.21.86
			Port:                     <unset>  8094/TCP                                                     <------Port is different than 80
			TargetPort:               8094/TCP
			NodePort:                 <unset>  30008/TCP
			Endpoints:                10.244.0.5:8094
			Session Affinity:         None
			External Traffic Policy:  Cluster
			Events:                   <none>

6. Edit service to configure port to 80 (3 changes)

thor@jump_host ~$ kubectl edit service nginx-service
		service/nginx-service edited
 
thor@jump_host ~$ kubectl get service
		NAME            TYPE           CLUSTER-IP    EXTERNAL-IP   PORT(S)        AGE
		kubernetes      ClusterIP      10.96.0.1     <none>        443/TCP        48m
		nginx-service   LoadBalancer   10.96.21.86   <pending>     80:30008/TCP   6m35s                  <-----Port Changed

7. Restart deployment in order to restart pods (if pods dont restart on own)

thor@jump_host ~$ kubectl rollout restart deploy nginx-phpfpm-dp
		deployment.apps/nginx-phpfpm-dp restarted

thor@jump_host ~$ kubectl get deploy
		NAME              READY   UP-TO-DATE   AVAILABLE   AGE
		nginx-phpfpm-dp   0/1     1            0           13m

thor@jump_host ~$ kubectl get deploy
		NAME              READY   UP-TO-DATE   AVAILABLE   AGE
		nginx-phpfpm-dp   1/1     1            1           13m

thor@jump_host ~$ kubectl get pods
		NAME                               READY   STATUS    RESTARTS   AGE
		nginx-phpfpm-dp-6b5c45c6c7-zq6nr   2/2     Running   0          12s

thor@jump_host ~$ kubectl get pods
		NAME                               READY   STATUS    RESTARTS   AGE
		nginx-phpfpm-dp-6b5c45c6c7-zq6nr   2/2     Running   0          57s

8. Copy index.php from jump host server to nginx_container  in the running pod and verify

thor@jump_host ~$ kubectl cp /tmp/index.php nginx-phpfpm-dp-6b5c45c6c7-zq6nr:/var/www/html -c nginx-container

thor@jump_host ~$ kubectl exec -it nginx-phpfpm-dp-6b5c45c6c7-zq6nr -c nginx-container -- bash

root@nginx-phpfpm-dp-6b5c45c6c7-zq6nr:/# ls -ahl /var/www/html
		total 12K
		drwxrwxrwx 2 root root 4.0K Jan  2 08:12 .
		drwxr-xr-x 3 root root 4.0K Jan  2 08:10 ..
		-rw-r--r-- 1 root root  168 Jan  2 08:12 index.php
root@nginx-phpfpm-dp-6b5c45c6c7-zq6nr:/# 

9. Validate by clicking on the view port top right side side of the terminal and select port 30008. If the container is running then we see nginx page and the task is done.

--------------------------------------------------------------------------------------------------------------------------------
Task 128: 03/Jan/2023

Rolling Updates And Rolling Back Deployments in Kubernetes


There is a production deployment planned for next week. The Nautilus DevOps team wants to test the deployment update and rollback on Dev environment first so that they can identify the risks in advance. Below you can find more details about the plan they want to execute.

    Create a namespace nautilus. Create a deployment called httpd-deploy under this new namespace, It should have one container called httpd, use httpd:2.4.27 image and 2 replicas. The deployment should use RollingUpdate strategy with maxSurge=1, and maxUnavailable=2. Also create a NodePort type service named httpd-service and expose the deployment on nodePort: 30008.

    Now upgrade the deployment to version httpd:2.4.43 using a rolling update.

    Finally, once all pods are updated undo the recent update and roll back to the previous/original version.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

1. Get current configuration

thor@jump_host ~$ kubectl get namespace
	NAME                 STATUS   AGE
	default              Active   27m
	kube-node-lease      Active   27m
	kube-public          Active   27m
	kube-system          Active   27m
	local-path-storage   Active   27m

2. Create mentioned namespace and verify

thor@jump_host ~$ kubectl create namespace nautilus
	namespace/nautilus created

thor@jump_host ~$ kubectl get namespace
		NAME                 STATUS   AGE
		default              Active   28m
		kube-node-lease      Active   28m
		kube-public          Active   28m
		kube-system          Active   28m
		local-path-storage   Active   27m
		nautilus             Active   3s

3. Create YAML file to implement the requirements of deployment and service

thor@jump_host ~$ vi /tmp/httpd_deploy.yaml

thor@jump_host ~$ cat /tmp/httpd_deploy.yaml
		---
		apiVersion: apps/v1
		kind: Deployment
		metadata:
		  name: httpd-deploy
		  namespace: nautilus
		spec:
		  replicas: 2
		  strategy:
		    type: RollingUpdate
		    rollingUpdate:
		      maxSurge: 1
		      maxUnavailable: 2
		  selector:
		    matchLabels:
		      app: httpd
		  template:
		    metadata:
		      labels:
		        app: httpd
		    spec:
		      containers:
		        - name: httpd
		          image: httpd:2.4.27
		---                                                                                                           
		apiVersion: v1                                                                                                
		kind: Service                                                                                                 
		metadata:                                                                                                     
		  name: httpd-service                                                                                         
		spec:                                                                                                         
		  type: NodePort                                                                                             
		  selector:                                                                                                  
		    app: httpd                                                                                     
		  ports:                                                                                                     
		    - port: 80                                                                                               
		      targetPort: 80                                                                                         
		      nodePort: 30008

4. Apply the YAML file created 
thor@jump_host ~$ kubectl apply -f /tmp/httpd_deploy.yaml --namespace=nautilus
		deployment.apps/httpd-deploy created
		service/httpd-service created

5. Verify the changes/creation of deployment, pods and service. Wait till pods get into running status.

thor@jump_host ~$ kubectl get all
		NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
		service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   29m

thor@jump_host ~$ kubectl get all -n nautilus
		NAME                                READY   STATUS    RESTARTS   AGE
		pod/httpd-deploy-5c68f9f7b7-bbczb   1/1     Running   0          38s
		pod/httpd-deploy-5c68f9f7b7-txl6h   1/1     Running   0          38s

		NAME                    TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
		service/httpd-service   NodePort   10.96.225.85   <none>        80:30008/TCP   38s

		NAME                           READY   UP-TO-DATE   AVAILABLE   AGE
		deployment.apps/httpd-deploy   2/2     2            2           38s

		NAME                                      DESIRED   CURRENT   READY   AGE
		replicaset.apps/httpd-deploy-5c68f9f7b7   2         2         2       38s

thor@jump_host ~$ kubectl get deployment -n nautilus -o wide
		NAME           READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES         SELECTOR
		httpd-deploy   2/2     2            2           60s   httpd        httpd:2.4.27   app=httpd

thor@jump_host ~$ kubectl get pods -n nautilus
		NAME                            READY   STATUS    RESTARTS   AGE
		httpd-deploy-5c68f9f7b7-bbczb   1/1     Running   0          73s
		httpd-deploy-5c68f9f7b7-txl6h   1/1     Running   0          73s

6. Update the container image as mentioned and wait till pods get into running status 

thor@jump_host ~$ kubectl set image deployment/httpd-deploy httpd=httpd:2.4.43 --namespace=nautilus --record=true
		deployment.apps/httpd-deploy image updated

thor@jump_host ~$ kubectl get deployment -n nautilus -o wide
		NAME           READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS   IMAGES         SELECTOR
		httpd-deploy   0/2     2            0           101s   httpd        httpd:2.4.43   app=httpd

thor@jump_host ~$ kubectl get pods -n nautilus
		NAME                            READY   STATUS              RESTARTS   AGE
		httpd-deploy-7bb4d96457-7pg4l   0/1     ContainerCreating   0          19s
		httpd-deploy-7bb4d96457-kjbn4   0/1     ContainerCreating   0          18s

thor@jump_host ~$ kubectl get pods -n nautilus
		NAME                            READY   STATUS    RESTARTS   AGE
		httpd-deploy-7bb4d96457-7pg4l   1/1     Running   0          28s
		httpd-deploy-7bb4d96457-kjbn4   1/1     Running   0          27s

thor@jump_host ~$ kubectl get deployment -n nautilus -o wide
		NAME           READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS   IMAGES         SELECTOR
		httpd-deploy   2/2     2            2           119s   httpd        httpd:2.4.43   app=httpd


7. Check rollout status and history for deployment

thor@jump_host ~$ kubectl rollout status deployment httpd-deploy -n nautilus
		deployment "httpd-deploy" successfully rolled out
 
thor@jump_host ~$ kubectl rollout history deployment httpd-deploy -n nautilus
		deployment.apps/httpd-deploy 
		REVISION  CHANGE-CAUSE
		1         <none>
		2         kubectl set image deployment/httpd-deploy httpd=httpd:2.4.43 --namespace=nautilus --record=true

8. Undo latest rollout to revert back the changes and wait for pods to get back to running status again. Check status and history of deployment

thor@jump_host ~$ kubectl rollout undo deployment httpd-deploy -n nautilus
		deployment.apps/httpd-deploy rolled back

thor@jump_host ~$ kubectl rollout status deployment httpd-deploy -n nautilus
		Waiting for deployment "httpd-deploy" rollout to finish: 1 of 2 updated replicas are available...
		deployment "httpd-deploy" successfully rolled out

thor@jump_host ~$ kubectl get pods -n nautilus
		NAME                            READY   STATUS    RESTARTS   AGE
		httpd-deploy-5c68f9f7b7-9sgks   1/1     Running   0          16s
		httpd-deploy-5c68f9f7b7-rnv6l   1/1     Running   0          15s

thor@jump_host ~$ kubectl get deployment -n nautilus -o wide
		NAME           READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS   IMAGES         SELECTOR
		httpd-deploy   2/2     2            2           3m2s   httpd        httpd:2.4.27   app=httpd

thor@jump_host ~$ kubectl rollout history deployment httpd-deploy -n nautilus
		deployment.apps/httpd-deploy 
		REVISION  CHANGE-CAUSE
		2         kubectl set image deployment/httpd-deploy httpd=httpd:2.4.43 --namespace=nautilus --record=true
		3         <none>

thor@jump_host ~$
--------------------------------------------------------------------------------------------------------------------------------
Task 129: 04/Jan/2023

Managing Jinja2 Templates Using Ansible

One of the Nautilus DevOps team members is working on to develop a role for httpd installation and configuration. Work is almost completed, however there is a requirement to add a jinja2 template for index.html file. Additionally, the relevant task needs to be added inside the role. The inventory file ~/ansible/inventory is already present on jump host that can be used. Complete the task as per details mentioned below:

a. Update ~/ansible/playbook.yml playbook to run the httpd role on App Server 1.

b. Create a jinja2 template index.html.j2 under /home/thor/ansible/role/httpd/templates/ directory and add a line This file was created using Ansible on <respective server> (for example This file was created using Ansible on stapp01 in case of App Server 1). Also please make sure not to hard code the server name inside the template. Instead, use inventory_hostname variable to fetch the correct value.

c. Add a task inside /home/thor/ansible/role/httpd/tasks/main.yml to copy this template on App Server 1 under /var/www/html/index.html. Also make sure that /var/www/html/index.html file's permissions are 0655.

d. The user/group owner of /var/www/html/index.html file must be respective sudo user of the server (for example tony in case of stapp01).

Note: Validation will try to run the playbook using command ansible-playbook -i inventory playbook.yml so please make sure the playbook works this way without passing any extra arguments.


1. Check current inventory and playbook

thor@jump_host ~$ cd /home/thor/ansible/

thor@jump_host ~/ansible$ ls -ahl
		total 20K
		drwxr-xr-x 3 thor thor 4.0K Jan  4 14:10 .
		drwxr----- 1 thor thor 4.0K Jan  4 14:10 ..
		-rw-r--r-- 1 thor thor  237 Jan  4 14:10 inventory
		-rw-r--r-- 1 thor thor   73 Jan  4 14:10 playbook.yml
		drwxr-xr-x 3 thor thor 4.0K Jan  4 14:10 role

thor@jump_host ~/ansible$ cat inventory 
		stapp01 ansible_host=172.16.238.10 ansible_user=tony ansible_ssh_pass=Ir0nM@n
		stapp02 ansible_host=172.16.238.11 ansible_user=steve ansible_ssh_pass=Am3ric@
		stapp03 ansible_host=172.16.238.12 ansible_user=banner ansible_ssh_pass=BigGr33n

thor@jump_host ~/ansible$ cat playbook.yml 
		---
		- hosts: 
		  become: yes
		  become_user: root
		  roles:
		    - role/httpd

2. Edit playbook to add host to just App Server 1

thor@jump_host ~/ansible$ vi playbook.yml 

thor@jump_host ~/ansible$ cat playbook.yml 
		---
		- hosts: stapp01 
		  become: yes
		  become_user: root
		  roles:
		    - role/httpd

3. Create Jinja template as per given requirement

thor@jump_host ~/ansible$ vi /home/thor/ansible/role/httpd/templates/index.html.j2

thor@jump_host ~/ansible$ cat /home/thor/ansible/role/httpd/templates/index.html.j2
		This file was created using Ansible on {{ ansible_hostname }}

4. Edit role file as per given requirement to add task to create index.html on App Server

thor@jump_host ~/ansible$ vi /home/thor/ansible/role/httpd/tasks/main.yml

thor@jump_host ~/ansible$ cat /home/thor/ansible/role/httpd/tasks/main.yml
		---
		# tasks file for role/test

		- name: install the latest version of HTTPD
		  yum:
		    name: httpd
		    state: latest

		- name: Start service httpd
		  service:
		    name: httpd
		    state: started

		- name: Use Jinja2 template to generate index.html
		  template:
		    src: /home/thor/ansible/role/httpd/templates/index.html.j2
		    dest: /var/www/html/index.html
		    mode: "0655"
		    owner: "{{ ansible_user }}"
		    group: "{{ ansible_user }}"

5. Run playbook

thor@jump_host ~/ansible$ ansible-playbook -i inventory playbook.yml 

		PLAY [stapp01] ******************************************************************************************************************************

		TASK [Gathering Facts] **********************************************************************************************************************
		ok: [stapp01]

		TASK [role/httpd : install the latest version of HTTPD] *************************************************************************************
		changed: [stapp01]

		TASK [role/httpd : Start service httpd] *****************************************************************************************************
		changed: [stapp01]

		TASK [role/httpd : Use Jinja2 template to generate index.html] ******************************************************************************
		changed: [stapp01]

		PLAY RECAP **********************************************************************************************************************************
		stapp01                    : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

6. Verify and validate task by checking index.html on App server 1

thor@jump_host ~/ansible$ ansible -i inventory stapp01 -a "ls -ahl /var/www/html"
		stapp01 | CHANGED | rc=0 >>
		total 12K
		drwxr-xr-x 2 root root 4.0K Jan  4 14:20 .
		drwxr-xr-x 4 root root 4.0K Jan  4 14:20 ..
		-rw-r-xr-x 1 tony tony   47 Jan  4 14:20 index.html

thor@jump_host ~/ansible$ ansible -i inventory stapp01 -a "cat /var/www/html/index.html"
		stapp01 | CHANGED | rc=0 >>
		This file was created using Ansible on stapp01

thor@jump_host ~/ansible$

---------------------------------------------------------------------------------------------------------------------------------
Task 130: 06/Jan/2023

 Ansible Create Users and Groups


Several new developers and DevOps engineers just joined the xFusionCorp industries. They have been assigned the Nautilus project, and as per the onboarding process we need to create user accounts for new joinees on at least one of the app servers in Stratos DC. We also need to create groups and make new users members of those groups. We need to accomplish this task using Ansible. Below you can find more information about the task.

There is already an inventory file ~/playbooks/inventory on jump host.

On jump host itself there is a list of users in ~/playbooks/data/users.yml file and there are two groups — admins and developers —that have list of different users. Create a playbook ~/playbooks/add_users.yml on jump host to perform the following tasks on app server 2 in Stratos DC.

a. Add all users given in the users.yml file on app server 2.

b. Also add developers and admins groups on the same server.

c. As per the list given in the users.yml file, make each user member of the respective group they are listed under.

d. Make sure home directory for all of the users under developers group is /var/www (not the default i.e /var/www/{USER}). Users under admins group should use the default home directory (i.e /home/devid for user devid).

e. Set password BruCStnMT5 for all of the users under developers group and LQfKeWWxWD for of the users under admins group. Make sure to use the password given in the ~/playbooks/secrets/vault.txt file as Ansible vault password to encrypt the original password strings. You can use ~/playbooks/secrets/vault.txt file as a vault secret file while running the playbook (make necessary changes in ~/playbooks/ansible.cfg file).

f. All users under admins group must be added as sudo users. To do so, simply make them member of the wheel group as well.

Note: Validation will try to run the playbook using command ansible-playbook -i inventory add_users.yml so please make sure playbook works this way, without passing any extra arguments.


1. Check inventory file , users file and password vault file

thor@jump_host ~$ cd /home/thor/playbooks/

thor@jump_host ~/playbooks$ ls -ahl
		total 24K
		drwxr-xr-x 4 thor thor 4.0K Jan  6 07:34 .
		drwxr----- 1 thor thor 4.0K Jan  6 07:34 ..
		-rw-r--r-- 1 thor thor   36 Jan  6 07:34 ansible.cfg
		drwxr-xr-x 2 thor thor 4.0K Jan  6 04:21 data
		-rw-r--r-- 1 thor thor  237 Jan  6 07:34 inventory
		drwxr-xr-x 2 thor thor 4.0K Jan  6 07:34 secrets

thor@jump_host ~/playbooks$ cat inventory 
		stapp01 ansible_host=172.16.238.10 ansible_ssh_pass=Ir0nM@n ansible_user=tony
		stapp02 ansible_host=172.16.238.11 ansible_ssh_pass=Am3ric@ ansible_user=steve
		stapp03 ansible_host=172.16.238.12 ansible_ssh_pass=BigGr33n ansible_user=banner

thor@jump_host ~/playbooks$ cat data/users.yml 
		admins:
		  - rob
		  - david
		  - joy

		developers:
		  - tim
		  - ray
		  - jim
		  - mark

thor@jump_host ~/playbooks$ cat secrets/vault.txt 
		P@ss3or432

2. Add vault file location to ansible config file

thor@jump_host ~/playbooks$ cat ansible.cfg 
		[defaults]
		host_key_checking = False

thor@jump_host ~/playbooks$ vi ansible.cfg 

thor@jump_host ~/playbooks$ cat ansible.cfg 
		[defaults]
		host_key_checking = False
		vault_password_file = /home/thor/playbooks/secrets/vault.txt

3. Create playbook add_users.yml as per given requirements to add users and groups

thor@jump_host ~/playbooks$ vi add_users.yml

thor@jump_host ~/playbooks$ cat add_users.yml
		---                                                                                                              
		- name: Ansbile Add User & Group                                                                       
		  hosts: stapp02                                                                                                
		  become: yes                                                                                                    
		  tasks:                                                                                                         
		  - name: Creating Admin Groups                                                                                  
		    group:                                                                                                       
		     name:                                                                                                       
		      admins                                                                                                     
		     state: present                                                                                              
		  - name: Creating Dev Groups                                                                                    
		    group:                                                                                                       
		     name:                                                                                                       
		      developers                                                                                                 
		     state: present                                                                                              
		  - name: Creating Admins Group Users                                                                            
		    user:                                                                                                        
		     name: "{{ item }}"                                                                                          
		     password: "{{ 'LQfKeWWxWD' | password_hash ('sha512') }}"                                                   
		     groups: admins,wheel
		     state: present                                                                                              
		    loop:                                                                                                        
		    - rob                                                                                                        
		    - joy                                                                                                        
		    - david                                                                                                      
		  - name: Creating Developers Group Users                                                                        
		    user:                                                                                                        
		     name: "{{ item }}"                                                                                          
		     password: "{{ 'BruCStnMT5' | password_hash ('sha512') }}"                                                   
		     home: "/var/www/{{ item }}"                                                                                             
		     group: developers                                                                                           
		     state: present                                                                                              
		    loop:                                                                                                        
		    - tim                                                                                                        
		    - jim                                                                                                        
		    - mark                                                                                                       
		    - ray

4. Check connection by getting current users list on app server 2

thor@jump_host ~/playbooks$ ansible -i inventory stapp02 -a "cat /etc/passwd"
		stapp02 | CHANGED | rc=0 >>
		root:x:0:0:root:/root:/bin/bash
		bin:x:1:1:bin:/bin:/sbin/nologin
		daemon:x:2:2:daemon:/sbin:/sbin/nologin
		adm:x:3:4:adm:/var/adm:/sbin/nologin
		lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin
		sync:x:5:0:sync:/sbin:/bin/sync
		shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown
		halt:x:7:0:halt:/sbin:/sbin/halt
		mail:x:8:12:mail:/var/spool/mail:/sbin/nologin
		operator:x:11:0:operator:/root:/sbin/nologin
		games:x:12:100:games:/usr/games:/sbin/nologin
		ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin
		nobody:x:99:99:Nobody:/:/sbin/nologin
		systemd-network:x:192:192:systemd Network Management:/:/sbin/nologin
		dbus:x:81:81:System message bus:/:/sbin/nologin
		sshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologin
		ansible:x:1000:1000::/home/ansible:/bin/bash
		steve:x:1001:1001::/home/steve:/bin/bash

5. Run playbook to create users/groups on App Server 2

thor@jump_host ~/playbooks$ ansible-playbook -i inventory add_users.yml 

		PLAY [Ansbile Add User & Group] *************************************************************************************************************

		TASK [Gathering Facts] **********************************************************************************************************************
		ok: [stapp02]

		TASK [Creating Admin Groups] ****************************************************************************************************************
		changed: [stapp02]

		TASK [Creating Dev Groups] ******************************************************************************************************************
		changed: [stapp02]

		TASK [Creating Admins Group Users] **********************************************************************************************************
		changed: [stapp02] => (item=rob)
		changed: [stapp02] => (item=joy)
		changed: [stapp02] => (item=david)

		TASK [Creating Developers Group Users] ******************************************************************************************************
		changed: [stapp02] => (item=tim)
		changed: [stapp02] => (item=jim)
		changed: [stapp02] => (item=mark)
		changed: [stapp02] => (item=ray)

		PLAY RECAP **********************************************************************************************************************************
		stapp02                    : ok=5    changed=4    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   


6. Verify and validate task

thor@jump_host ~/playbooks$ ansible -i inventory stapp02 -a "cat /etc/passwd"
		stapp02 | CHANGED | rc=0 >>
		root:x:0:0:root:/root:/bin/bash
		bin:x:1:1:bin:/bin:/sbin/nologin
		daemon:x:2:2:daemon:/sbin:/sbin/nologin
		adm:x:3:4:adm:/var/adm:/sbin/nologin
		lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin
		sync:x:5:0:sync:/sbin:/bin/sync
		shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown
		halt:x:7:0:halt:/sbin:/sbin/halt
		mail:x:8:12:mail:/var/spool/mail:/sbin/nologin
		operator:x:11:0:operator:/root:/sbin/nologin
		games:x:12:100:games:/usr/games:/sbin/nologin
		ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin
		nobody:x:99:99:Nobody:/:/sbin/nologin
		systemd-network:x:192:192:systemd Network Management:/:/sbin/nologin
		dbus:x:81:81:System message bus:/:/sbin/nologin
		sshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologin
		ansible:x:1000:1000::/home/ansible:/bin/bash
		steve:x:1001:1001::/home/steve:/bin/bash
		rob:x:1002:1004::/home/rob:/bin/bash
		joy:x:1003:1005::/home/joy:/bin/bash
		david:x:1004:1006::/home/david:/bin/bash
		tim:x:1005:1003::/var/www/tim:/bin/bash
		jim:x:1006:1003::/var/www/jim:/bin/bash
		mark:x:1007:1003::/var/www/mark:/bin/bash
		ray:x:1008:1003::/var/www/ray:/bin/bash

thor@jump_host ~/playbooks$ ssh rob@stapp02
		The authenticity of host 'stapp02 (172.16.238.11)' can't be established.
		ECDSA key fingerprint is SHA256:RfzwP0l5UhLkHTrsf3Aiiib98V9zGk8SLR64bTwIwBg.
		ECDSA key fingerprint is MD5:c3:cb:fd:91:90:a5:b8:56:e6:28:80:85:23:c6:aa:85.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'stapp02' (ECDSA) to the list of known hosts.
		rob@stapp02's password: 

[rob@stapp02 ~]$ sudo su -

[root@stapp02 ~]# id
		uid=0(root) gid=0(root) groups=0(root)

[root@stapp02 ~]# id david
		uid=1004(david) gid=1006(david) groups=1006(david),10(wheel),1002(admins)

[root@stapp02 ~]# id joy
		uid=1003(joy) gid=1005(joy) groups=1005(joy),10(wheel),1002(admins)

[root@stapp02 ~]# id mark
		uid=1007(mark) gid=1003(developers) groups=1003(developers)

[root@stapp02 ~]# exit
logout

[rob@stapp02 ~]$ exit
		logout
		Connection to stapp02 closed.
thor@jump_host ~/playbooks$

---------------------------------------------------------------------------------------------------------------------------------
Task 131: 07/Jan/2023

Puppet Add Users

A new teammate has joined the Nautilus application development team, the application development team has asked the DevOps team to create a new user account for the new teammate on application server 3 in Stratos Datacenter. The task needs to be performed using Puppet only. You can find more details below about the task.

Create a Puppet programming file news.pp under /etc/puppetlabs/code/environments/production/manifests directory on master node i.e Jump Server, and using Puppet user resource add a user on all app servers as mentioned below:

    Create a user siva and set its UID to 1644 on Puppet agent nodes 3 i.e App Servers 3.

Notes: :- Please make sure to run the puppet agent test using sudo on agent nodes, otherwise you can face certificate issues. In that case you will have to clean the certificates first and then you will be able to run the puppet agent test.

:- Before clicking on the Check button please make sure to verify puppet server and puppet agent services are up and running on the respective servers, also please make sure to run puppet agent test to apply/test the changes manually first.

:- Please note that once lab is loaded, the puppet server service should start automatically on puppet master server, however it can take upto 2-3 minutes to start.


1. Login to root user on jump host

thor@jump_host ~$ sudo su -

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for thor: 

2. Create puppet programming file news.pp on given directory location as per give nrequirements

root@jump_host ~# cd /etc/puppetlabs/code/environments/production/manifests

root@jump_host /etc/puppetlabs/code/environments/production/manifests# vi news.pp

root@jump_host /etc/puppetlabs/code/environments/production/manifests# cat news.pp
		class user_create {
		  user { 
		   'siva':
		   ensure   => present,
		   uid => 1644,
		  }
		}

		node 'stapp03.stratos.xfusioncorp.com' {
		  include user_create
		}

3. Parse and validate news.pp file

root@jump_host /etc/puppetlabs/code/environments/production/manifests# puppet parser validate news.pp

4. Login to App Server 3 and sitch to root user

root@jump_host /etc/puppetlabs/code/environments/production/manifests# ssh banner@stapp03
		The authenticity of host 'stapp03 (172.16.238.12)' can't be established.
		ECDSA key fingerprint is SHA256:FRLWcj5X07yxcJggr/l4cU9nSphy2E6WFO+cW4WgnqQ.
		ECDSA key fingerprint is MD5:47:a3:ee:4c:19:d1:8c:47:d7:0f:0e:43:a0:6f:45:89.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'stapp03,172.16.238.12' (ECDSA) to the list of known hosts.
		banner@stapp03's password: 

[banner@stapp03 ~]$ sudo su -

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for banner: 

5. Run puppet agent to pull the changes from master server

[root@stapp03 ~]# puppet agent -tv
		Info: Using configured environment 'production'
		Info: Retrieving pluginfacts
		Info: Retrieving plugin
		Info: Retrieving locales
		Info: Caching catalog for stapp03.stratos.xfusioncorp.com
		Info: Applying configuration version '1673086141'
		Notice: /Stage[main]/User_create/User[siva]/ensure: created
		Notice: Applied catalog in 0.05 seconds

6. Validate the task by checking user created

[root@stapp03 ~]# cat /etc/passwd|grep siva
		siva:x:1644:1644::/home/siva:/bin/bash

[root@stapp03 ~]# id siva
		uid=1644(siva) gid=1644(siva) groups=1644(siva)

---------------------------------------------------------------------------------------------------------------------------------
Task 132: 08/Jan/2023

Ansible Copy Module


There is data on jump host that needs to be copied on all application servers in Stratos DC. Nautilus DevOps team want to perform this task using Ansible. Perform the task as per details mentioned below:

a. On jump host create an inventory file /home/thor/ansible/inventory and add all application servers as managed nodes.

b. On jump host create a playbook /home/thor/ansible/playbook.yml to copy /usr/src/devops/index.html file to all application servers at location /opt/devops.

Note: Validation will try to run the playbook using command ansible-playbook -i inventory playbook.yml so please make sure the playbook works this way without passing any extra arguments.


1. Go to mentioned folder and create inventory file with credentials for all app servers 

thor@jump_host ~$ cd /home/thor/ansible/

thor@jump_host ~/ansible$ ls -ahl
		total 8.0K
		drwxr-xr-x 2 thor thor 4.0K Jan  8 14:04 .
		drwxr----- 1 thor thor 4.0K Jan  8 14:04 ..

thor@jump_host ~/ansible$ vi /home/thor/ansible/inventory

thor@jump_host ~/ansible$ cat /home/thor/ansible/inventory
		stapp01 ansible_host=172.16.238.10 ansible_ssh_pass=Ir0nM@n  ansible_user=tony

		stapp02 ansible_host=172.16.238.11 ansible_ssh_pass=Am3ric@  ansible_user=steve

		stapp03 ansible_host=172.16.238.12 ansible_ssh_pass=BigGr33n  ansible_user=banner

2. Verify the inventory file by running command via ansible

thor@jump_host ~/ansible$ ansible -i inventory all -a "ls -ahl /opt/devops"
		stapp03 | CHANGED | rc=0 >>
		total 8.0K
		drwxr-xr-x 2 root root 4.0K Jan  8 14:05 .
		drwxr-xr-x 1 root root 4.0K Jan  8 14:05 ..
		stapp02 | CHANGED | rc=0 >>
		total 8.0K
		drwxr-xr-x 2 root root 4.0K Jan  8 14:05 .
		drwxr-xr-x 1 root root 4.0K Jan  8 14:05 ..
		stapp01 | CHANGED | rc=0 >>
		total 8.0K
		drwxr-xr-x 2 root root 4.0K Jan  8 14:05 .
		drwxr-xr-x 1 root root 4.0K Jan  8 14:05 ..

3. Create Playbook.yml to copy mentioned file 

thor@jump_host ~/ansible$ vi playbook.yml

thor@jump_host ~/ansible$ cat playbook.yml
		- name: Ansible copy

		  hosts: all

		  become: yes

		  tasks:

		    - name: copy index.html to sysops folder

		      copy: src=/usr/src/devops/index.html dest=/opt/devops

4. Run the playbook

thor@jump_host ~/ansible$ ansible-playbook -i inventory playbook.yml 

		PLAY [Ansible copy] *************************************************************************************************************************

		TASK [Gathering Facts] **********************************************************************************************************************
		ok: [stapp03]
		ok: [stapp01]
		ok: [stapp02]

		TASK [copy index.html to sysops folder] *****************************************************************************************************
		changed: [stapp02]
		changed: [stapp03]
		changed: [stapp01]

		PLAY RECAP **********************************************************************************************************************************
		stapp01                    : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
		stapp02                    : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
		stapp03                    : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

5. Validate the task by checking destination folder on each server by ansible

thor@jump_host ~/ansible$ ansible -i inventory all -a "ls -ahl /opt/devops"
		stapp02 | CHANGED | rc=0 >>
		total 12K
		drwxr-xr-x 2 root root 4.0K Jan  8 14:08 .
		drwxr-xr-x 1 root root 4.0K Jan  8 14:05 ..
		-rw-r--r-- 1 root root   35 Jan  8 14:08 index.html
		stapp03 | CHANGED | rc=0 >>
		total 12K
		drwxr-xr-x 2 root root 4.0K Jan  8 14:08 .
		drwxr-xr-x 1 root root 4.0K Jan  8 14:05 ..
		-rw-r--r-- 1 root root   35 Jan  8 14:08 index.html
		stapp01 | CHANGED | rc=0 >>
		total 12K
		drwxr-xr-x 2 root root 4.0K Jan  8 14:08 .
		drwxr-xr-x 1 root root 4.0K Jan  8 14:05 ..
		-rw-r--r-- 1 root root   35 Jan  8 14:08 index.html

thor@jump_host ~/ansible$

---------------------------------------------------------------------------------------------------------------------------------
Task 133: 10/Jan/2023

Ansible Unarchive Module

One of the DevOps team members has created an ZIP archive on jump host in Stratos DC that needs to be extracted and copied over to all app servers in Stratos DC itself. Because this is a routine task, the Nautilus DevOps team has suggested automating it. We can use Ansible since we have been using it for other automation tasks. Below you can find more details about the task:

We have an inventory file under /home/thor/ansible directory on jump host, which should have all the app servers added already.

There is a ZIP archive /usr/src/itadmin/nautilus.zip on jump host.

Create a playbook.yml under /home/thor/ansible/ directory on jump host itself to perform the below given tasks.

    Unzip /usr/src/itadmin/nautilus.zip archive in /opt/itadmin/ location on all app servers.

    Make sure the extracted data must has the respective sudo user as their user and group owner, i.e tony for app server 1, steve for app server 2, banner for app server 3.

    The extracted data permissions must be 0755

Note: Validation will try to run the playbook using command ansible-playbook -i inventory playbook.yml so please make sure playbook works this way, without passing any extra arguments.

1. Check ansible configuration and connection to all app servers

thor@jump_host ~$ cd /home/thor/ansible/

thor@jump_host ~/ansible$ ls -ahl
		total 16K
		drwxr-xr-x 2 thor thor 4.0K Jan 10 04:42 .
		drwxr----- 1 thor thor 4.0K Jan 10 04:42 ..
		-rw-r--r-- 1 thor thor   36 Jan 10 04:42 ansible.cfg
		-rw-r--r-- 1 thor thor  237 Jan 10 04:42 inventory

thor@jump_host ~/ansible$ cat inventory 
		stapp01 ansible_host=172.16.238.10 ansible_ssh_pass=Ir0nM@n ansible_user=tony
		stapp02 ansible_host=172.16.238.11 ansible_ssh_pass=Am3ric@ ansible_user=steve
		stapp03 ansible_host=172.16.238.12 ansible_ssh_pass=BigGr33n ansible_user=banner


thor@jump_host ~/ansible$ ansible -i inventory all -a "ls -ahl /opt/itadmin/"
		stapp02 | CHANGED | rc=0 >>
		total 8.0K
		drwxr-xr-x 2 root root 4.0K Jan 10 04:43 .
		drwxr-xr-x 1 root root 4.0K Jan 10 04:43 ..
		stapp03 | CHANGED | rc=0 >>
		total 8.0K
		drwxr-xr-x 2 root root 4.0K Jan 10 04:43 .
		drwxr-xr-x 1 root root 4.0K Jan 10 04:43 ..
		stapp01 | CHANGED | rc=0 >>
		total 8.0K
		drwxr-xr-x 2 root root 4.0K Jan 10 04:43 .
		drwxr-xr-x 1 root root 4.0K Jan 10 04:43 ..

2. Create playbook as per given requirement

thor@jump_host ~/ansible$ vi playbook.yml

thor@jump_host ~/ansible$ cat playbook.yml
		- name: Extract archive

		  hosts: stapp01, stapp02, stapp03

		  become: yes

		  tasks:

		    - name: Extract the archive and set the owner/permissions

		      unarchive:

		        src: /usr/src/itadmin/nautilus.zip

		        dest: /opt/itadmin/

		        owner: "{{ ansible_user }}"

		        group: "{{ ansible_user }}"

		        mode: "0755"

3. Run ansible playbook

thor@jump_host ~/ansible$ ansible-playbook -i inventory playbook.yml 

		PLAY [Extract archive] **********************************************************************************************************************

		TASK [Gathering Facts] **********************************************************************************************************************
		ok: [stapp03]
		ok: [stapp01]
		ok: [stapp02]

		TASK [Extract the archive and set the owner/permissions] ************************************************************************************
		changed: [stapp02]
		changed: [stapp01]
		changed: [stapp03]

		PLAY RECAP **********************************************************************************************************************************
		stapp01                    : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
		stapp02                    : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
		stapp03                    : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

4. Validate task by checking unarchived file on all app servers

thor@jump_host ~/ansible$ ansible -i inventory all -a "ls -ahl /opt/itadmin/"
		stapp01 | CHANGED | rc=0 >>
		total 12K
		drwxr-xr-x 3 root root 4.0K Jan 10 04:46 .
		drwxr-xr-x 1 root root 4.0K Jan 10 04:43 ..
		drwxr-xr-x 2 tony tony 4.0K Jan 10 04:43 unarchive
		stapp02 | CHANGED | rc=0 >>
		total 12K
		drwxr-xr-x 3 root  root  4.0K Jan 10 04:46 .
		drwxr-xr-x 1 root  root  4.0K Jan 10 04:43 ..
		drwxr-xr-x 2 steve steve 4.0K Jan 10 04:43 unarchive
		stapp03 | CHANGED | rc=0 >>
		total 12K
		drwxr-xr-x 3 root   root   4.0K Jan 10 04:46 .
		drwxr-xr-x 1 root   root   4.0K Jan 10 04:43 ..
		drwxr-xr-x 2 banner banner 4.0K Jan 10 04:43 unarchive
thor@jump_host ~/ansible$

---------------------------------------------------------------------------------------------------------------------------------
Task 134: 11/Jan/2023

Persistent Volumes in Kubernetes HTTPD (Task Repeated)

The Nautilus DevOps team is working on a Kubernetes template to deploy a web application on the cluster. There are some requirements to create/use persistent volumes to store the application code, and the template needs to be designed accordingly. Please find more details below:

    Create a PersistentVolume named as pv-xfusion. Configure the spec as storage class should be manual, set capacity to 5Gi, set access mode to ReadWriteOnce, volume type should be hostPath and set path to /mnt/finance (this directory is already created, you might not be able to access it directly, so you need not to worry about it).

    Create a PersistentVolumeClaim named as pvc-xfusion. Configure the spec as storage class should be manual, request 1Gi of the storage, set access mode to ReadWriteOnce.

    Create a pod named as pod-xfusion, mount the persistent volume you created with claim name pvc-xfusion at document root of the web server, the container within the pod should be named as container-xfusion using image httpd with latest tag only (remember to mention the tag i.e httpd:latest).

    Create a node port type service named web-xfusion using node port 30008 to expose the web server running within the pod.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

1. Check kubectl utility for currently running services and pods

thor@jump_host ~$ kubectl get all
		NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
		service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   36m

thor@jump_host ~$ kubectl get pv
		No resources found

thor@jump_host ~$ kubectl get pvc
		No resources found in default namespace.

2. Create YAML file as per requirements 

thor@jump_host ~$ vi /tmp/pvc_httpd.yml

thor@jump_host ~$ cat /tmp/pvc_httpd.yml
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-xfusion
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  storageClassName: manual
  hostPath:
    path: /mnt/finance
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-xfusion
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: manual
  resources:
    requests:
      storage: 1Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-xfusion
  labels:
     app: httpd
spec:
  volumes:
    - name: storage-datacenter
      persistentVolumeClaim:
        claimName: pvc-xfusion
  containers:
    - name: container-xfusion
      image: httpd:latest
      ports:
        - containerPort: 80
      volumeMounts:
        - name: storage-datacenter
          mountPath:  /usr/local/apache2/htdocs/
---                                                                                                           
apiVersion: v1                                                                                                
kind: Service                                                                                                 
metadata:                                                                                                     
  name: web-xfusion                                                                                         
spec:                                                                                                         
   type: NodePort                                                                                             
   selector:                                                                                                  
     app: httpd                                                                                     
   ports:                                                                                                     
     - port: 80                                                                                               
       targetPort: 80                                                                                         
       nodePort: 30008
 
3. Create the deployment , pods and persistent volumes

thor@jump_host ~$ kubectl create -f /tmp/pvc_httpd.yml 
		persistentvolume/pv-xfusion created
		persistentvolumeclaim/pvc-xfusion created
		pod/pod-xfusion created
		service/web-xfusion created

4. Wait for pods and services to running status

thor@jump_host ~$ kubectl get all
		NAME              READY   STATUS    RESTARTS   AGE
		pod/pod-xfusion   1/1     Running   0          16s

		NAME                  TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
		service/kubernetes    ClusterIP   10.96.0.1      <none>        443/TCP        38m
		service/web-xfusion   NodePort    10.96.61.213   <none>        80:30008/TCP   16s

thor@jump_host ~$ kubectl get all
		NAME              READY   STATUS    RESTARTS   AGE
		pod/pod-xfusion   1/1     Running   0          21s

		NAME                  TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
		service/kubernetes    ClusterIP   10.96.0.1      <none>        443/TCP        38m
		service/web-xfusion   NodePort    10.96.61.213   <none>        80:30008/TCP   21s

5. Validate Psersistent Volume Claim and Peristent volume 

thor@jump_host ~$ kubectl get pvc
		NAME          STATUS   VOLUME       CAPACITY   ACCESS MODES   STORAGECLASS   AGE
		pvc-xfusion   Bound    pv-xfusion   5Gi        RWO            manual         31s

thor@jump_host ~$ kubectl get pv
		NAME         CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                 STORAGECLASS   REASON   AGE
		pv-xfusion   5Gi        RWO            Retain           Bound    default/pvc-xfusion   manual                  34s

6. Validate task by View Website on right top corner

---------------------------------------------------------------------------------------------------------------------------------
Task 135: 13/Jan/2023

Deploy Apache Web Server on Kubernetes CLuster (Task repeated)

There is an application that needs to be deployed on Kubernetes cluster under Apache web server. The Nautilus application development team has asked the DevOps team to deploy it. We need to develop a template as per requirements mentioned below:

    Create a namespace named as httpd-namespace-devops.

    Create a deployment named as httpd-deployment-devops under newly created namespace. For the deployment use httpd image with latest tag only and remember to mention the tag i.e httpd:latest, and make sure replica counts are 2.

    Create a service named as httpd-service-devops under same namespace to expose the deployment, nodePort should be 30004.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.


1. Check current kubernetes namespace

thor@jump_host ~$ kubectl get namespace
		NAME                 STATUS   AGE
		default              Active   81m
		kube-node-lease      Active   81m
		kube-public          Active   81m
		kube-system          Active   81m
		local-path-storage   Active   81m

2. Create required namespace

thor@jump_host ~$ kubectl create namespace httpd-namespace-devops
		namespace/httpd-namespace-devops created

thor@jump_host ~$ kubectl get namespace
		NAME                     STATUS   AGE
		default                  Active   82m
		httpd-namespace-devops   Active   18s
		kube-node-lease          Active   82m
		kube-public              Active   82m
		kube-system              Active   82m
		local-path-storage       Active   82m

3. Check running pods and services for newly created namespace

thor@jump_host ~$ kubectl get pods -n  httpd-namespace-devops
		No resources found in httpd-namespace-devops namespace.

thor@jump_host ~$ kubectl get service -n  httpd-namespace-devops
		No resources found in httpd-namespace-devops namespace.

4. Create YAML file as pre requirement with configuration for deployment, service and pods

thor@jump_host ~$ vi /tmp/httpd.yaml

thor@jump_host ~$ cat /tmp/httpd.yaml
		---
		apiVersion: v1
		kind: Service
		metadata:
		  name: httpd-service-devops
		  namespace: httpd-namespace-devops
		spec:
		  type: NodePort
		  selector:
		    app: httpd_app_devops
		  ports:
		    - port: 80
		      targetPort: 80
		      nodePort: 30004
		---
		apiVersion: apps/v1
		kind: Deployment
		metadata:
		  name: httpd-deployment-devops
		  namespace: httpd-namespace-devops
		  labels:
		    app: httpd_app_devops
		spec:
		  replicas: 2
		  selector:
		    matchLabels:
		      app: httpd_app_devops
		  template:
		    metadata:
		      labels:
		        app: httpd_app_devops
		    spec:
		      containers:
		        - name: httpd-container-devops
		          image: httpd:latest
		          ports:
		            - containerPort: 80

5. Run the YAML file configuration

thor@jump_host ~$ kubectl create -f /tmp/httpd.yaml
		service/httpd-service-devops created
		deployment.apps/httpd-deployment-devops created

6. Verify service, deployment and pods created and wait for them to get to running status 

thor@jump_host ~$ kubectl get service -n  httpd-namespace-devops
		NAME                   TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)        AGE
		httpd-service-devops   NodePort   10.96.58.40   <none>        80:30004/TCP   5s

thor@jump_host ~$ kubectl get pods -n  httpd-namespace-devops
		NAME                                      READY   STATUS    RESTARTS   AGE
		httpd-deployment-devops-867b499f4-99kpp   1/1     Running   0          13s
		httpd-deployment-devops-867b499f4-dgnfz   1/1     Running   0          13s
thor@jump_host ~$

7. Validate the task by checking website on top right corner

---------------------------------------------------------------------------------------------------------------------------------
Task 136: 14/Jan/2023

Puppet Multi-Packages Installation (Task Repeated)

Some new changes need to be made on some of the app servers in Stratos Datacenter. There are some packages that need to be installed on the app server 3. We want to install these packages using puppet only.

    Puppet master is already installed on Jump Server.

    Create a puppet programming file beta.pp under /etc/puppetlabs/code/environments/production/manifests on master node i.e on Jump Server and perform below mentioned tasks using the same.

    Define a class multi_package_node for agent node 3 i.e app server 3. Install net-tools and unzip packages on the agent node 3.

Notes: :- Please make sure to run the puppet agent test using sudo on agent nodes, otherwise you can face certificate issues. In that case you will have to clean the certificates first and then you will be able to run the puppet agent test.

:- Before clicking on the Check button please make sure to verify puppet server and puppet agent services are up and running on the respective servers, also please make sure to run puppet agent test to apply/test the changes manually first.

1. 1. Switch to root user on jump host to create puppet programming file

thor@jump_host ~$ sudo su - 

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for thor: 

2. Go to mentione folder location and create blog.pp  file with given requirements

root@jump_host ~# cd /etc/puppetlabs/code/environments/production/manifests

root@jump_host /etc/puppetlabs/code/environments/production/manifests# ls -ahl
		total 8.0K
		drwxr-xr-x 1 puppet puppet 4.0K Jul 13  2021 .
		drwxr-xr-x 1 puppet puppet 4.0K Aug  9  2021 ..

root@jump_host /etc/puppetlabs/code/environments/production/manifests# vi beta.pp

root@jump_host /etc/puppetlabs/code/environments/production/manifests# cat beta.pp
		class multi_package_node {
		$multi_package = [ 'net-tools', 'unzip']
		    package { $multi_package: ensure => 'installed' }
		}

		node 'stapp03.stratos.xfusioncorp.com' {
		  include multi_package_node
		}

3. Validate the puppet file
 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# puppet parser validate beta.pp 

4. Login to App Server 3 and switch to root user 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# ssh banner@stapp03
		The authenticity of host 'stapp03 (172.16.238.12)' can't be established.
		ECDSA key fingerprint is SHA256:no5E3rkS5jFnKkqTqKOdwYwHJz71GjgolocMxJ+KsNM.
		ECDSA key fingerprint is MD5:02:15:a7:52:b8:38:e0:78:e8:4a:3a:c7:d3:85:57:a5.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'stapp03,172.16.238.12' (ECDSA) to the list of known hosts.
		banner@stapp03's password: 

[banner@stapp03 ~]$ sudo su -

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for banner: 

5.. Check if given packages are already  installed 

[root@stapp03 ~]# rpm -qa|grep -e net-tools -e unzip

6. Run the puppet agent to pull configuration from puppet server

[root@stapp03 ~]# puppet agent -tv
Notice: Run of Puppet configuration client already in progress; skipping  (/opt/puppetlabs/puppet/cache/state/agent_catalog_run.lock exists)

7. Validate the task by checking required packages installed
[root@stapp03 ~]# rpm -qa|grep -e net-tools -e unzip
unzip-6.0-24.el7_9.x86_64
net-tools-2.0-0.25.20131004git.el7.x86_64
[root@stapp03 ~]# 

---------------------------------------------------------------------------------------------------------------------------------
Task 137: 16/Jan/2023

Puppet Setup SSH Keys (Task Repeated)

The Puppet master and Puppet agent nodes have been set up by the Nautilus DevOps team to perform some testing. In Stratos DC all app servers have been configured as Puppet agent nodes. They want to setup a password less SSH connection between Puppet master and Puppet agent nodes and this task needs to be done using Puppet itself. Below are details about the task:

Create a Puppet programming file beta.pp under /etc/puppetlabs/code/environments/production/manifests directory on the Puppet master node i.e on Jump Server. Define a class ssh_node1 for agent node 1 i.e App Server 1, ssh_node2 for agent node 2 i.e App Server 2, ssh_node3 for agent node3 i.e App Server 3. You will need to generate a new ssh key for thor user on Jump Server, that needs to be added on all App Servers.

Configure a password less SSH connection from puppet master i.e jump host to all App Servers. However, please make sure the key is added to the authorized_keys file of each app's sudo user (i.e tony for App Server 1).

Notes: :- Before clicking on the Check button please make sure to verify puppet server and puppet agent services are up and running on the respective servers, also please make sure to run puppet agent test to apply/test the changes manually first.

:- Please note that once lab is loaded, the puppet server service should start automatically on puppet master server, however it can take upto 2-3 minutes to start.



Kindly Check Task 83 for solution

---------------------------------------------------------------------------------------------------------------------------------
Task 138: 18/Jan/2023

Resolve Git Merge Conflicts (Task Repeated)

Sarah and Max were working on writting some stories which they have pushed to the repository. Max has recently added some new changes and is trying to push them to the repository but he is facing some issues. Below you can find more details:

SSH into storage server using user max and password Max_pass123. Under /home/max you will find the story-blog repository. Try to push the changes to the origin repo and fix the issues. The story-index.txt must have titles for all 4 stories. Additionally, there is a typo in The Lion and the Mooose line where Mooose should be Mouse.

Click on the Gitea UI button on the top bar. You should be able to access the Gitea page. You can login to Gitea server from UI using username sarah and password Sarah_pass123 or username max and password Max_pass123.

Note: For these kind of scenarios requiring changes to be done in a web UI, please take screenshots so that you can share it with us for review in case your task is marked incomplete. You may also consider using a screen recording software such as loom.com to record and share your work.


Kindly Check Task 91 for solution

---------------------------------------------------------------------------------------------------------------------------------
Task 139: 19/Jan/2023

Ansible Unarchive Module (Task Repeated)

One of the DevOps team members has created an ZIP archive on jump host in Stratos DC that needs to be extracted and copied over to all app servers in Stratos DC itself. Because this is a routine task, the Nautilus DevOps team has suggested automating it. We can use Ansible since we have been using it for other automation tasks. Below you can find more details about the task:

We have an inventory file under /home/thor/ansible directory on jump host, which should have all the app servers added already.

There is a ZIP archive /usr/src/sysops/datacenter.zip on jump host.

Create a playbook.yml under /home/thor/ansible/ directory on jump host itself to perform the below given tasks.

    Unzip /usr/src/sysops/datacenter.zip archive in /opt/sysops/ location on all app servers.

    Make sure the extracted data must has the respective sudo user as their user and group owner, i.e tony for app server 1, steve for app server 2, banner for app server 3.

    The extracted data permissions must be 0644

Note: Validation will try to run the playbook using command ansible-playbook -i inventory playbook.yml so please make sure playbook works this way, without passing any extra arguments.

playbook.yml

- name: Extract archive
  hosts: stapp01, stapp02, stapp03
  become: yes
  tasks:
    - name: Extract the archive and set the owner/permissions
      unarchive:
        src: /usr/src/sysops/datacenter.zip
        dest: /opt/sysops/
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        mode: "0644"


Kindly Check Task 123 for solution

---------------------------------------------------------------------------------------------------------------------------------
Task 140: 21/Jan/2023

Pull Docker Image (Task Repeated)

Nautilus project developers are planning to start testing on a new project. As per their meeting with the DevOps team, they want to test containerized environment application features. As per details shared with DevOps team, we need to accomplish the following task:

a. Pull busybox:musl image on App Server 2 in Stratos DC and re-tag (create new tag) this image as busybox:local

Kindly Check Task 112 for solution
---------------------------------------------------------------------------------------------------------------------------------
Task 141: 22/Jan/2023

Run a Docker Container (Task Repeated)

Nautilus DevOps team is testing some applications deployment on some of the application servers. They need to deploy a nginx container on Application Server 2. Please complete the task as per details given below:

    On Application Server 2 create a container named nginx_2 using image nginx with alpine tag and make sure container is in running state.


Kindly Check Task 102 for solution

---------------------------------------------------------------------------------------------------------------------------------
Task 142: 23/Jan/2023

Puppet Create Symlinks (Task Repeated)


Some directory structure in the Stratos Datacenter needs to be changed, there is a directory that needs to be linked to the default Apache document root. We need to accomplish this task using Puppet, as per the instructions given below:

Create a puppet programming file apps.pp under /etc/puppetlabs/code/environments/production/manifests directory on puppet master node i.e on Jump Server. Within that define a class symlink and perform below mentioned tasks:

    Create a symbolic link through puppet programming code. The source path should be /opt/itadmin and destination path should be /var/www/html on Puppet agents 3 i.e on App Servers 3.

    Create a blank file media.txt under /opt/itadmin directory on puppet agent 3 nodes i.e on App Servers 3.

Notes: :- Please make sure to run the puppet agent test using sudo on agent nodes, otherwise you can face certificate issues. In that case you will have to clean the certificates first and then you will be able to run the puppet agent test.

:- Before clicking on the Check button please make sure to verify puppet server and puppet agent services are up and running on the respective servers, also please make sure to run puppet agent test to apply/test the changes manually first.

:- Please note that once lab is loaded, the puppet server service should start automatically on puppet master server, however it can take upto 2-3 minutes to start.

apps.pp

class symlink {
  # First create a symlink to /var/www/html
  file { '/opt/itadmin':
    ensure => 'link',
    target => '/var/www/html',
  }
   # Now create media.txt under /opt/itadmin
  file { '/opt/itadmin/media.txt':
    ensure => 'present',
  }
}

node 'stapp03.stratos.xfusioncorp.com' {
  include symlink
}

Kindly Check Task 53 for solution

---------------------------------------------------------------------------------------------------------------------------------
Task 143: 25/Jan/2023

Deploy Guest Book App on Kubernetes (Task Repeated)

The Nautilus Application development team has finished development of one of the applications and it is ready for deployment. It is a guestbook application that will be used to manage entries for guests/visitors. As per discussion with the DevOps team, they have finalized the infrastructure that will be deployed on Kubernetes cluster. Below you can find more details about it.

BACK-END TIER

    Create a deployment named redis-master for Redis master.

    a.) Replicas count should be 1.

    b.) Container name should be master-redis-devops and it should use image redis.

    c.) Request resources as CPU should be 100m and Memory should be 100Mi.

    d.) Container port should be redis default port i.e 6379.

    Create a service named redis-master for Redis master. Port and targetPort should be Redis default port i.e 6379.

    Create another deployment named redis-slave for Redis slave.

    a.) Replicas count should be 2.

    b.) Container name should be slave-redis-devops and it should use gcr.io/google_samples/gb-redisslave:v3 image.

    c.) Requests resources as CPU should be 100m and Memory should be 100Mi.

    d.) Define an environment variable named GET_HOSTS_FROM and its value should be dns.

    e.) Container port should be Redis default port i.e 6379.

    Create another service named redis-slave. It should use Redis default port i.e 6379.

FRONT END TIER

    Create a deployment named frontend.

    a.) Replicas count should be 3.

    b.) Container name should be php-redis-devops and it should use gcr.io/google-samples/gb-frontend:v4 image.

    c.) Request resources as CPU should be 100m and Memory should be 100Mi.

    d.) Define an environment variable named as GET_HOSTS_FROM and its value should be dns.

    e.) Container port should be 80.

    Create a service named frontend. Its type should be NodePort, port should be 80 and its nodePort should be 30009.

Finally, you can check the guestbook app by clicking on + button in the top left corner and Select port to view on Host 1 then enter your nodePort.

You can use any labels as per your choice.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.


Kindly Check Task 51 for solution

---------------------------------------------------------------------------------------------------------------------------------
Task 144 Onwards : Kodekloud Cheatsheat DevOps Architect Task Commands.txt

---------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------
Task 145:

---------------------------------------------------------------------------------------------------------------------------------
Task 146:

---------------------------------------------------------------------------------------------------------------------------------
Task 147:

---------------------------------------------------------------------------------------------------------------------------------
Task 148:

---------------------------------------------------------------------------------------------------------------------------------
Task 149:

---------------------------------------------------------------------------------------------------------------------------------
Task 150:

---------------------------------------------------------------------------------------------------------------------------------
